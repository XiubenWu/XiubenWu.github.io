<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>0-1背包</title>
    <url>/2021/08/28/0-1%E8%83%8C%E5%8C%85/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>0-1背包问题：给定n种物品和一背包。每种物品有着重量和价值两个属性，背包的有总容量限制。问应该如何选择装入背包中的物品，使得装入背包中物品的总价值最大？背包问题有很多种，0-1背包是最为基础的一种，其核心思想均为用动态规划来解决。</p>
<hr />
<h1 id="解法">解法</h1>
<p>设有以下几种物品：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">物品(i)</th>
<th>重量(<span class="math inline">\(w_i\)</span>)</th>
<th>价值(<span class="math inline">\(v_i\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>物品0</strong></td>
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品1</strong></td>
<td>2</td>
<td>3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品2</strong></td>
<td>3</td>
<td>4</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品3</strong></td>
<td>4</td>
<td>5</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品4</strong></td>
<td>5</td>
<td>6</td>
</tr>
</tbody>
</table>
<p>背包总容量(<span class="math inline">\(w_a\)</span>​)为8，如何选择物品才能让背包的能够装的价值最大？</p>
<h2 id="通常解法">通常解法</h2>
<p>我们用<span class="math inline">\(dp[i][j]\)</span>​​​​​来表示仅从前<strong>i</strong>个物品里面选择且总容量为<strong>j</strong>时的物品的最大价值。动态转移方程可以写成如下形式：</p>
<p><span class="math inline">\(dp[i][j] = max(dp[i-1][j-w[i]]+v[i],dp[i-1][j])\)</span>​</p>
<p>每遍历到一个物品的时候，这个物品有装或者不装两种选择，如果装这个物品，则装之后的最大价值为当前物品的价值加上不装之前(容量为<span class="math inline">\(j-w[i]\)</span>​​​)​​的最大价值，如果不装，则最大价值为前i-1个物品的最大价值，这两者取最大值即可。示例源码如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>))  <span class="comment"># 每个物品重量</span></span><br><span class="line">v = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">7</span>))  <span class="comment"># 每个物品价值</span></span><br><span class="line"></span><br><span class="line">w_a = <span class="number">8</span>  <span class="comment"># 背包容量</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span>] * (w_a + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]  <span class="comment"># 初始化动态规划数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(w_a + <span class="number">1</span>):  <span class="comment"># 初始化边界条件</span></span><br><span class="line">    <span class="keyword">if</span> i &gt;= w[<span class="number">0</span>]:</span><br><span class="line">        dp[<span class="number">0</span>][i] = v[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):  <span class="comment"># 状态转移</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w_a + <span class="number">1</span>):</span><br><span class="line">        dp[i][j] = dp[i - <span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">if</span> j - w[i] &gt;= <span class="number">0</span>:</span><br><span class="line">            dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j - w[i]] + v[i], dp[i - <span class="number">1</span>][j])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终结果</span></span><br><span class="line">print(dp[-<span class="number">1</span>][w_a])</span><br><span class="line"></span><br><span class="line"><span class="number">11</span></span><br><span class="line">选择物品<span class="number">134</span>或<span class="number">125</span>的组合可以获得最大价值。</span><br></pre></td></tr></table></figure>
<ul>
<li>dp转移情况:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0, 2, 2, 2, 2, 2, 2, 2, 2]</span><br><span class="line">[0, 2, 3, 5, 5, 5, 5, 5, 5]</span><br><span class="line">[0, 2, 3, 5, 6, 7, 9, 9, 9]</span><br><span class="line">[0, 2, 3, 5, 6, 7, 9, 10, 11]</span><br><span class="line">[0, 2, 3, 5, 6, 7, 9, 10, 11]</span><br></pre></td></tr></table></figure>
<h2 id="空间优化">空间优化</h2>
<p>在第二层遍历时<span class="math inline">\(j\)</span>的状态由<span class="math inline">\(j\)</span>之前的状态来决定，而<span class="math inline">\(j\)</span>​不会影响到它之前的状态，因此可以用一维数组存储变量，然后倒序更改此数组，将空间复杂度优化到<span class="math inline">\(O(w_a)\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>))  <span class="comment"># 每个物品重量</span></span><br><span class="line">v = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">7</span>))  <span class="comment"># 每个物品价值</span></span><br><span class="line"></span><br><span class="line">w_a = <span class="number">8</span>  <span class="comment"># 背包容量</span></span><br><span class="line">n = <span class="built_in">len</span>(w)</span><br><span class="line"></span><br><span class="line">dp = [<span class="number">0</span>] * (w_a + <span class="number">1</span>)  <span class="comment"># 初始化动态规划数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(w_a + <span class="number">1</span>):  <span class="comment"># 初始化边界条件</span></span><br><span class="line">    <span class="keyword">if</span> i &gt;= w[<span class="number">0</span>]:</span><br><span class="line">        dp[i] = v[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):  <span class="comment"># 状态转移</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(w_a, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> j - w[i] &gt;= <span class="number">0</span>:</span><br><span class="line">            dp[j] = <span class="built_in">max</span>(dp[j - w[i]] + v[i], dp[j])</span><br><span class="line"><span class="comment"># 最终结果</span></span><br><span class="line">print(dp[-<span class="number">1</span>])</span><br><span class="line"><span class="number">11</span></span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="end">END</h1>
<p><a href="https://www.xiubenwu.top/categories/笔记/算法/背包问题/">其他背包问题</a></p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>背包问题</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Boyer–Moore majority vote algorithm</title>
    <url>/2021/07/13/Boyer%E2%80%93Moore-majority-vote-algorithm/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><strong>博耶-摩尔多数投票算法</strong>(Boyer–Moore majority vote algorithm)，是用来求取众数的一种常数空间复杂度的算法。所谓众数，就是数组中数量超过总数组长度一般的元素，也被称为多数元素、主要元素。</p>
<hr />
<h1 id="思想">思想</h1>
<p>两次扫描，第一次得到一个候选元素，第二次判断候选元素是否是我们所需的众数。</p>
<h2 id="第一次扫描">第一次扫描</h2>
<p>指定一个元素为候选元素<code>candidate</code>（可为任意数），再选定一个变量<code>count</code>来进行计数,然后依次扫描，对于数组中每一个元素，首先判断count是否为0，若为0，则把<code>candidate</code>设置为当前元素。之后判断<code>candidate</code>是否与当前元素相等，若相等则<code>count+=1</code>，否则<code>count-=1</code>。</p>
<h2 id="第二次扫描">第二次扫描</h2>
<p>第二次扫描统计可能是多数元素出现的次数，从而断定它是不是多数元素。由于最后的候选元素可能不是多数元素，因此需要进行第二次扫描判定。如<code>[1,5,3,4,2,2,2]</code>最后存留候选元素为2，<code>count</code>值为2，但是其不是多数元素。</p>
<hr />
<h1 id="end">End</h1>
<p>此算法需要两次遍历，亚线性空间算法无法通过一次遍历就得出输入中是否存在多数元素。如<strong>Graham Cormode</strong>所言：<a href="https://dl.acm.org/doi/10.1145/1562764.1562789">no algorithm can correctly distinguish the cases when an item is just above or just below the threshold in a single pass without using a large amount of space</a></p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>特色算法</tag>
      </tags>
  </entry>
  <entry>
    <title>CSS选择器</title>
    <url>/2021/09/13/CSS%E9%80%89%E6%8B%A9%E5%99%A8/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><strong>CSS 选择器</strong>规定了 CSS 规则会被应用到哪些元素上。js中的<strong>querySelector</strong>也可以通过CSS选择器来查询html中的元素。</p>
<hr />
<h1 id="基本选择器">基本选择器</h1>
<table>
<thead>
<tr class="header">
<th>选择器</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong><code>*</code></strong></td>
<td>通用元素选择器，匹配任何元素</td>
</tr>
<tr class="even">
<td><strong>elementname</strong></td>
<td>标签选择器，匹配所有使用elementname标签的元素</td>
</tr>
<tr class="odd">
<td><strong>.classname</strong></td>
<td>class选择器，匹配所有class属性中包含classname的元素</td>
</tr>
<tr class="even">
<td><strong>#idname</strong></td>
<td>id选择器，匹配所有id属性等于idname的元素</td>
</tr>
<tr class="odd">
<td><strong>[attr]</strong></td>
<td>属性选择器选择含有attr属性的元素，属性选择器有多种匹配表达式</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="分组选择器">分组选择器</h2>
<ul>
<li><p>选择器列表</p>
<p><code>,</code> 是将不同的选择器组合在一起的方法，它选择所有能被列表中的任意一个选择器选中的节点。 <strong>语法</strong>：<code>A, B</code> <strong>示例</strong>：<code>div, span</code> 会同时匹配 <code>div</code>元素和 <code>span</code>元素。</p>
<hr /></li>
</ul>
<h2 id="组合器">组合器</h2>
<ul>
<li><p>后代组合器</p>
<p><code></code>（空格）组合器选择前一个元素的后代节点。 <strong>语法：</strong><code>A B</code> <strong>例子：</strong><code>div span</code> 匹配所有位于任意<code>div</code>元素之内的后代<code>span</code>元素。</p></li>
<li><p>直接子代组合器</p>
<p><code>&gt;</code> 组合器选择前一个元素的直接子代的节点。 <strong>语法</strong>：<code>A &gt; B</code> <strong>例子</strong>：<code>ul &gt; li</code> 匹配在<code>ul</code>的直系后代<code>li</code>元素。</p></li>
<li><p>一般兄弟组合器</p>
<p><code>~</code> 组合器选择兄弟元素，也就是说，后一个节点在前一个节点后面的任意位置，并且共享同一个父节点。 <strong>语法</strong>：<code>A ~ B</code> <strong>例子</strong>：<code>p ~ span</code> 匹配同一父元素下，<code>&lt;p&gt;</code>元素后的所有 <code>&lt;span&gt;</code> 元素。</p></li>
<li><p>紧邻兄弟组合器</p>
<p><code>+</code> 组合器选择相邻元素，即后一个元素紧跟在前一个之后，并且共享同一个父节点。 <strong>语法：</strong><code>A + B</code></p></li>
<li><p>列组合器</p>
<p><code>||</code> 组合器选择属于某个表格行的节点。 <strong>语法：</strong> <code>A || B</code></p>
<hr /></li>
</ul>
<h1 id="伪选择器">伪选择器</h1>
<p>伪类</p>
<p><code>:</code> 伪选择器支持按照未被包含在文档树中的状态信息来选择元素。 <strong>例子：</strong><code>a:visited</code> 匹配所有曾被访问过的 <code>&lt;a&gt;</code>元素。</p>
<p><code>a:hover</code>匹配鼠标悬停在<code>&lt;a&gt;</code>元素上时的样式。</p>
<p>伪元素</p>
<p><code>::</code> 伪选择器用于表示无法用 HTML 语义表达的实体。 <strong>例子：</strong><code>p::first-line</code> 匹配所有<code>&lt;p&gt;</code>元素的第一行。</p>
<hr />
<h1 id="速查表">速查表</h1>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">选择器</th>
<th style="text-align: left;">例子</th>
<th style="text-align: left;">例子描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="https://www.w3school.com.cn/cssref/selector_class.asp">.<em>class</em></a></td>
<td style="text-align: left;">.intro</td>
<td style="text-align: left;">选择 class=&quot;intro&quot; 的所有元素。</td>
</tr>
<tr class="even">
<td style="text-align: left;">.<em>class1</em>.<em>class2</em></td>
<td style="text-align: left;">.name1.name2</td>
<td style="text-align: left;">选择 class 属性中同时有 name1 和 name2 的所有元素。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">.<em>class1</em> .<em>class2</em></td>
<td style="text-align: left;">.name1 .name2</td>
<td style="text-align: left;">选择作为类名 name1 元素后代的所有类名 name2 元素。</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://www.w3school.com.cn/cssref/selector_id.asp">#<em>id</em></a></td>
<td style="text-align: left;">#firstname</td>
<td style="text-align: left;">选择 id=&quot;firstname&quot; 的元素。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="https://www.w3school.com.cn/cssref/selector_all.asp">*</a></td>
<td style="text-align: left;">*</td>
<td style="text-align: left;">选择所有元素。</td>
</tr>
</tbody>
</table>
| <a href="https://www.w3school.com.cn/cssref/selector_element.asp"><em>element</em></a> | p | 选择所有
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_element_class.asp"><em>element</em>.<em>class</em></a> | p.intro | 选择 class=&quot;intro&quot; 的所有
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_element_comma.asp"><em>element</em>,<em>element</em></a> | div, p | 选择所有
<div>
元素和所有
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_element_element.asp"><em>element</em> <em>element</em></a> | div p | 选择
<div>
元素内的所有
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_element_gt.asp"><em>element</em>&gt;<em>element</em></a> | div &gt; p | 选择父元素是
<div>
的所有
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_element_plus.asp"><em>element</em>+<em>element</em></a> | div + p | 选择紧跟
<div>
元素的首个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_gen_sibling.asp"><em>element1</em>~<em>element2</em></a> | p ~ ul | 选择前面有
<p>
元素的每个
<ul>
元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attribute.asp"><em>attribute</em>]</a> | [target] | 选择带有 target 属性的所有元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attribute_value.asp"><em>attribute</em>=<em>value</em>]</a> | [target=_blank] | 选择带有 target=&quot;_blank&quot; 属性的所有元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attribute_value_contain.asp"><em>attribute</em>~=<em>value</em>]</a> | [title~=flower] | 选择 title 属性包含单词 &quot;flower&quot; 的所有元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attribute_value_start.asp"><em>attribute</em>|=<em>value</em>]</a> | [lang|=en] | 选择 lang 属性值以 &quot;en&quot; 开头的所有元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attr_begin.asp"><em>attribute</em>^=<em>value</em>]</a> | a[href^=&quot;https&quot;] | 选择其 src 属性值以 &quot;https&quot; 开头的每个 <a> 元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attr_end.asp"><em>attribute</em>$=<em>value</em>]</a> | a[href$=&quot;.pdf&quot;] | 选择其 src 属性以 &quot;.pdf&quot; 结尾的所有 <a> 元素。 | | [<a href="https://www.w3school.com.cn/cssref/selector_attr_contain.asp">*attribute**=<em>value</em>]</a> | a[href*=&quot;w3schools&quot;] | 选择其 href 属性值中包含 &quot;abc&quot; 子串的每个 <a> 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_active.asp">:active</a> | a:active | 选择活动链接。 | | <a href="https://www.w3school.com.cn/cssref/selector_after.asp">::after</a> | p::after | 在每个
<p>
的内容之后插入内容。 | | <a href="https://www.w3school.com.cn/cssref/selector_before.asp">::before</a> | p::before | 在每个
<p>
的内容之前插入内容。 | | <a href="https://www.w3school.com.cn/cssref/selector_checked.asp">:checked</a> | input:checked | 选择每个被选中的 <input> 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_default.asp">:default</a> | input:default | 选择默认的 <input> 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_disabled.asp">:disabled</a> | input:disabled | 选择每个被禁用的 <input> 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_empty.asp">:empty</a> | p:empty | 选择没有子元素的每个
<p>
元素（包括文本节点）。 | | <a href="https://www.w3school.com.cn/cssref/selector_enabled.asp">:enabled</a> | input:enabled | 选择每个启用的 <input> 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_first-child.asp">:first-child</a> | p:first-child | 选择属于父元素的第一个子元素的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_first-letter.asp">::first-letter</a> | p::first-letter | 选择每个
<p>
元素的首字母。 | | <a href="https://www.w3school.com.cn/cssref/selector_first-line.asp">::first-line</a> | p::first-line | 选择每个
<p>
元素的首行。 | | <a href="https://www.w3school.com.cn/cssref/selector_first-of-type.asp">:first-of-type</a> | p:first-of-type | 选择属于其父元素的首个
<p>
元素的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_focus.asp">:focus</a> | input:focus | 选择获得焦点的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_fullscreen.asp">:fullscreen</a> | :fullscreen | 选择处于全屏模式的元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_hover.asp">:hover</a> | a:hover | 选择鼠标指针位于其上的链接。 | | <a href="https://www.w3school.com.cn/cssref/selector_in-range.asp">:in-range</a> | input:in-range | 选择其值在指定范围内的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_indeterminate.asp">:indeterminate</a> | input:indeterminate | 选择处于不确定状态的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_invalid.asp">:invalid</a> | input:invalid | 选择具有无效值的所有 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_lang.asp">:lang(<em>language</em>)</a> | p:lang(it) | 选择 lang 属性等于 &quot;it&quot;（意大利）的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_last-child.asp">:last-child</a> | p:last-child | 选择属于其父元素最后一个子元素每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_last-of-type.asp">:last-of-type</a> | p:last-of-type | 选择属于其父元素的最后
<p>
元素的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_link.asp">:link</a> | a:link | 选择所有未访问过的链接。 | | <a href="https://www.w3school.com.cn/cssref/selector_not.asp">:not(<em>selector</em>)</a> | :not(p) | 选择非
<p>
元素的每个元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_nth-child.asp">:nth-child(<em>n</em>)</a> | p:nth-child(2) | 选择属于其父元素的第二个子元素的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_nth-last-child.asp">:nth-last-child(<em>n</em>)</a> | p:nth-last-child(2) | 同上，从最后一个子元素开始计数。 | | <a href="https://www.w3school.com.cn/cssref/selector_nth-of-type.asp">:nth-of-type(<em>n</em>)</a> | p:nth-of-type(2) | 选择属于其父元素第二个
<p>
元素的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_nth-last-of-type.asp">:nth-last-of-type(<em>n</em>)</a> | p:nth-last-of-type(2) | 同上，但是从最后一个子元素开始计数。 | | <a href="https://www.w3school.com.cn/cssref/selector_only-of-type.asp">:only-of-type</a> | p:only-of-type | 选择属于其父元素唯一的
<p>
元素的每个
<p>
元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_only-child.asp">:only-child</a> | p:only-child | 选择属于其父元素的唯一子元素的每个
<p>
<p>元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_optional.asp">:optional</a> | input:optional | 选择不带 &quot;required&quot; 属性的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_out-of-range.asp">:out-of-range</a> | input:out-of-range | 选择值超出指定范围的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_placeholder.asp">::placeholder</a> | input::placeholder | 选择已规定 &quot;placeholder&quot; 属性的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_read-only.asp">:read-only</a> | input:read-only | 选择已规定 &quot;readonly&quot; 属性的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_read-write.asp">:read-write</a> | input:read-write | 选择未规定 &quot;readonly&quot; 属性的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_required.asp">:required</a> | input:required | 选择已规定 &quot;required&quot; 属性的 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_root.asp">:root</a> | :root | 选择文档的根元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_selection.asp">::selection</a> | ::selection | 选择用户已选取的元素部分。 | | <a href="https://www.w3school.com.cn/cssref/selector_target.asp">:target</a> | #news:target | 选择当前活动的 #news 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_valid.asp">:valid</a> | input:valid | 选择带有有效值的所有 input 元素。 | | <a href="https://www.w3school.com.cn/cssref/selector_visited.asp">:visited</a> | a:visited | 选择所有已访问的链接。 |</p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>Conda 虚拟环境</title>
    <url>/2021/08/25/Conda-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>如果在一台电脑上, 想开发多个不同的项目, 需要用到同一个包的不同版本, 如果使用上面的命令, 在同一个目录下安装或者更新, 新版本会覆盖以前的版本, 其它的项目就无法运行了。 解决方案 : 虚拟环境的作用 : 虚拟环境可以搭建独立的python运行环境, 使得单个项目的运行环境与其它项目互不影响。</p>
<hr />
<h1 id="使用">使用</h1>
<p>基本命令:</p>
<p>查看当前存在哪些虚拟环境:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda env list 或</span><br><span class="line">conda info -e </span><br></pre></td></tr></table></figure>
<p>检查更新当前conda:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda update conda </span><br></pre></td></tr></table></figure>
<p>创建虚拟环境:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create -n your_env_name python&#x3D;x.x</span><br></pre></td></tr></table></figure>
<p>激活虚拟环境:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">activate env_name (Windows cmd)</span><br><span class="line">conda activate env_name (Anaconda Powershell)</span><br></pre></td></tr></table></figure>
<p>关闭虚拟环境：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda deactivate (Windows cmd)</span><br><span class="line">conda deactivate (Anaconda Powershell)</span><br></pre></td></tr></table></figure>
<p>删除虚拟环境:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda remove -n env_name --all</span><br></pre></td></tr></table></figure>
<p>在虚拟环境中安装某个包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install -n your_env_name [package]</span><br></pre></td></tr></table></figure>
<p>删除虚拟环境中的某个包:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda remove --name env_name  package_name</span><br></pre></td></tr></table></figure>
<h2 id="指定路径">指定路径</h2>
<p>conda默认的虚拟环境安装路径在cconda根目录下的<strong>evns</strong>文件夹中。要将虚拟环境安装到指定的路径，则使用如下命令:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda create --prefix&#x3D;D:\folder\**\envName python&#x3D;x.x</span><br></pre></td></tr></table></figure>
<p>这种方式安装的虚拟环境的名称为其全路径**D:*<em></em>,激活环境时:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">activate D:\folder\**\envName (Windows cmd)</span><br><span class="line">conda activate D:\folder\**\envName (Anaconda Powershell)</span><br></pre></td></tr></table></figure>
<p>删除环境:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda remove --prefix&#x3D;D:\folder\**\envName --all</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Deepin安装</title>
    <url>/2021/09/18/Deepin%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h1 id="软件下载">软件下载</h1>
<p><a href="https://www.deepin.org/zh/download/">Deepin</a>官网下载ios镜像文件。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210918Deepin1.png" alt="image-20210918205728743" /><figcaption>image-20210918205728743</figcaption>
</figure>
<h1 id="启动u盘制作">启动U盘制作</h1>
<p>下载的镜像文件中有制作启动盘的程序，直接制作即可。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210918Deepin2.png" alt="image-20210918210018693" /><figcaption>image-20210918210018693</figcaption>
</figure>
<h1 id="设置分区">设置分区</h1>
<h2 id="分区类型">分区类型</h2>
<p>需要在磁盘上新建一个分区来安装Deepin，可以使用传统的Legacy+MBR和UEFI+GPT来安装系统，建议使用UEFI+GPT格式安装。</p>
<h2 id="分区大小">分区大小</h2>
<p>至少三个分区，建议<strong>保留空白卷到UEFI安装时手动设置</strong>，也可预先分配好。</p>
<ul>
<li>efi分区大小至少为300Mb</li>
<li>/根目录挂载自定义大小</li>
<li>swap交换分区当内存小时给内存两倍左右，内存大时给等同于内存的大小</li>
</ul>
<h1 id="root权限">root权限</h1>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210918Deepin3.png" alt="a~1" /><figcaption>a~1</figcaption>
</figure>
<p>新装系统没有root用户,需要先设置root:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo passwd root</span><br><span class="line">&gt;&gt;&gt;输入当前用户密码</span><br><span class="line">&gt;&gt;&gt;输入root用户密码</span><br><span class="line">&gt;&gt;&gt;在此输入root用户密码</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Elementary OS</title>
    <url>/2021/09/20/Elementary-OS/</url>
    <content><![CDATA[<h1 id="下载">下载</h1>
<p>前往<a href="https://elementary.io/">Elementary官网</a>下载ISO镜像，将价格改为0之后可以直接下载：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210920ElementaryOS.png" alt="image-20210920140851567" /><figcaption>image-20210920140851567</figcaption>
</figure>
<p>启动U盘制作器官方推荐<a href="https://www.balena.io/etcher/">Etcher</a>，可以自行选择<a href="https://rufus.ie/zh/">rufus</a>等。</p>
<p>使用Etcher制作U盘镜像：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210920Elementary2.png" alt="image-20210920141845398" /><figcaption>image-20210920141845398</figcaption>
</figure>
<h1 id="root">root</h1>
<p>安装完成后设置root用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo passwd root</span><br><span class="line">&gt;&gt;&gt;输入当前用户密码</span><br><span class="line">&gt;&gt;&gt;输入root用户密码</span><br><span class="line">&gt;&gt;&gt;在此输入root用户密码</span><br></pre></td></tr></table></figure>
<h1 id="设置">设置</h1>
<h2 id="源">源</h2>
<p>安装Pantheon Tweaks.这是 elementary OS 的必备应用。它提供了一些无法通过系统原生设置程序修改的额外的设置和配置选项，请打开终端并运行以下命令以安装 Pantheon Tweaks。注意：先前版本的 Tweak 工具叫做 elementary Tweaks，从 Odin 版本开始更名为 Pantheon Tweaks。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt upgrade</span><br><span class="line"></span><br><span class="line">sudo apt install software-properties-common</span><br><span class="line">sudo add-apt-repository -y ppa:philip.scott&#x2F;pantheon-tweaks</span><br><span class="line">sudo apt install -y pantheon-tweaks</span><br></pre></td></tr></table></figure>
<p>换阿里的源,编辑/etc/apt/source.list文件，内容换成以下文本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse </span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main multiverse restricted universe</span><br></pre></td></tr></table></figure>
<p>加入一些其他的源：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install software-properties-common</span><br><span class="line">sudo add-apt-repository ppa:kdenlive&#x2F;kdenlive-stable</span><br><span class="line">sudo add-apt-repository ppa:philip.scott&#x2F;elementary-tweaks</span><br><span class="line">cd &#x2F;etc&#x2F;xdg&#x2F;autostart&#x2F;</span><br><span class="line">sudo mv io.elementary.appcenter-daemon.desktop io.elementary.appcenter-daemon.desktop.bak</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install git</span><br></pre></td></tr></table></figure>
<ul>
<li>系统监视器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt install gnome-system-monitor</span><br></pre></td></tr></table></figure>
<ul>
<li>安装gdebi，方便安装deb包。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install gdebi</span><br></pre></td></tr></table></figure>
<h2 id="美化">美化</h2>
<p>安装tweaks，下载图标包和主题包：</p>
<p><a href="https://github.com/keeferrourke/la-capitaine-icon-theme">图标包</a>，放置在<code>/usr/share/icons/</code>下</p>
<p><a href="https://github.com/yarik-vv/OS-X-buttons">主题包</a>，放置在<code>/usr/share/themes/</code>下</p>
<p>注销后更换生效。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo搭建</title>
    <url>/2021/02/04/Hexo%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h1 id="hexo-a-easy-fast-simplepowerful-framework"><strong>Hexo, a easy, fast, simple&amp;powerful framework</strong></h1>
<p>HEXO搭建记录 <span id="more"></span></p>
<h1 id="准备工作">准备工作</h1>
<ul>
<li>Git</li>
<li>Node.js</li>
<li>Github</li>
<li>建立本地仓库，与远程仓库绑定，SSH密匙。</li>
</ul>
<h1 id="搭建步骤">搭建步骤</h1>
<ol type="1">
<li>使用npm命令安装Hexo，输入（参数-g全局安装）：<br />
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli </span><br></pre></td></tr></table></figure></li>
<li>安装完成以后，需要初始化一下项目，执行下列命令：<br />
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo init</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure></li>
<li>安装hexo themes，npm安装相关依赖。</li>
<li>建立页面、清除缓存、生成页面、本地预览、部署到网页。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new</span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure> <strong>Note:</strong> 仓库建立两个分支，master存放代码文件，pages用于部署页面。hexo _config文件中注意更改部署页面。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https:&#x2F;&#x2F;github.com&#x2F;name&#x2F;name.github.io.git</span><br><span class="line">  branch: pages</span><br></pre></td></tr></table></figure> 部署到时若出现网页与本地不一致，删除生成文件(public, .deploy_git等)再重新生成部署。</li>
</ol>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>IP详解</title>
    <url>/2021/08/22/IP%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><strong>IP(Internet Protocol)</strong>的作用是在复杂的网络环境中将数据包发送给最终目的主机。其与MAC(数据链路路)的区别如下：IP的作用是主机之间通信用的，而MAC的作用则是实现「直连」的两个设备之间通信,而IP则负责在「没有直连」的两个网络之间进行通信传输。</p>
<hr />
<h1 id="ip地址基础">IP地址基础</h1>
<p>在TCP/IP 网络通信时，为了保证能正常通信，每个设备都需要配置正确的IP地址，否则无法实现正常的通信。</p>
<h2 id="ipv4">IPv4</h2>
<p>IP地址（IPv4地址)由 32位正整数来表示，IP地址在计算机是以二进制的方式处理的。而人类为了方便记忆采用了点分十进制的标记方式，也就是将32位IP地址以每8位为组，共分为4组，每组以「.」隔开，再将每组转换成十进制。</p>
<h2 id="ipv6">IPv6</h2>
<p>IPv4的地址是32位的，大约可以提供42亿个地址，但是早在2011年IPv4地址就已经被分配完了。IPv6的地址是128位的，这可分配的地址数量是大的惊人。</p>
<ul>
<li>IPv4地址长度共32位，是以每8位作为一组，并用点分十进制的表示方式。</li>
<li>IPv6地址长度是128位，是以每16位作为一组，每组用冒号:隔开。</li>
</ul>
<h2 id="广播地址">广播地址</h2>
<p>广播地址用于在同一个链路中相互连接的主机之间发送数据包。</p>
<p>当主机号全为1时，就表示该网络的广播地址。例如把<strong>172.20.0.0/16</strong>用二进制表示如下: 10101100.00010100.00000000.00000000 将这个地址的主机部分全部改为1，则形成广播地址: 10101100.00010100.11111111.11111111 再将这个地址用十进制表示，则为<strong>172.20.255.255</strong> 。 广播地址可以分为本地广播和直接广播两种：</p>
<ul>
<li><strong>在本网络内广播的叫做本地广播</strong>。例如网络地址为192.168.0.0/24的情况下，广播地址是 192.168.0.255。因为这个广播地址的IP包会被路由器屏蔽，所以不会到达192.168,0.0/24以外的其他链路上。</li>
<li><strong>在不同网络之间的广播叫做直接广播</strong>。例如网络地址为192.168.0.0/24的主机向192.168.1.255/24的目标地址发送IP包。收到这个包的路由器，将数据转发给192.168.1.0/24，从而使得所有192.168.1.1~192.168.1.254的主机都能收到这个包(由于直接广播有一定的安全问题，多数情况下会在路由器上设置为不转发。</li>
</ul>
<h2 id="ip地址的分类">IP地址的分类</h2>
<p>互联网诞生之初，IP地址显得很充裕，于是计算机科学家们设计了分类地址。IP地址分类成了5种类型，分别是A类、B类、C类、D类、E类。其中对于A、B、C类主要分为两个部分,分别是<strong>网络号和主机号</strong>。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210822IP1.png" alt="image-20210822234439207" /><figcaption>image-20210822234439207</figcaption>
</figure>
<p>最大主机个数，就是要看主机号的位数，ABC三类的最大主机数如下：</p>
<table>
<thead>
<tr class="header">
<th>类别</th>
<th>IP地址</th>
<th>最大主机数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0.0.0.0~127.255.255.255</td>
<td>16777214</td>
</tr>
<tr class="even">
<td>B</td>
<td>128.0.0.0~191.255.255.255</td>
<td>65534</td>
</tr>
<tr class="odd">
<td>C</td>
<td>192.0.0.0~223.255.255.255</td>
<td>254</td>
</tr>
</tbody>
</table>
<p>在IP地址中，有两个IP是特殊的，分别是主机号<strong>全为1和全为0地址</strong>，所以占用两个IP，最大主机数要减二：</p>
<ul>
<li>主机号全为1指定某个网络下的所有主机，用于广播</li>
<li>主机号全为0指定某个网络</li>
</ul>
<p>而D类和E类地址是没有主机号的，所以不可用于主机 IP，D类常被用于<strong>多播</strong>（多播⽤用于将包发送给特定组内的所有主机），<strong>E类是预留的分类，暂时未使用。</strong></p>
<h3 id="ip分类的优点">IP分类的优点</h3>
<p>不管是路由器还是主机解析到一个IP地址时候，我们判断其IP地址的首位是否为0，为0则为A类地址，那么就能很快的找出网络地址和主机地址。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210822IP2.png" alt="image-20210822235735548" /><figcaption>image-20210822235735548</figcaption>
</figure>
<h3 id="ip分类的缺点">IP分类的缺点</h3>
<p><strong>缺点一：</strong> 同一网络下没有地址层次，比如一个公司里用了B类地址，但是可能需要根据生产环境、测试环境、开发环境来划分地址层次，而这种IP分类是没有地址层次划分的功能，所以这就缺少地址的灵活性。 <strong>缺点二：</strong> A、B、C类有个尴尬处境，就是不能很好的与现实网络匹配。</p>
<ul>
<li>C类地址能包含的最大主机数量实在太少了，只有254个，估计一个网吧都不够用。</li>
<li>而B类地址能包含的最大主机数量又太多了,6万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。</li>
</ul>
<p>这两个缺点，都可以用<strong>CIDR(Classless Inter-Domain Routing)无分类地址</strong>解决。</p>
<h3 id="无分类地址cidr">无分类地址CIDR</h3>
<p>由于IP分类存在许多缺点，所以后面提出了无分类地址的方案，即CIDR 。这种方式不再有分类地址的概念，32比特的IP地址被划分为两部分，前面是网络号，后面是主机号。表示形式<strong>a.b.c.d/x</strong> ，其中 /x表示前×位属于网络号，x的范围是0～32，这就使得IP地址更加具有灵活性。比如10.100.122.2/24，这种地址表示形式就是 CIDR，/24表示前24位是网络号，剩余的8位是主机号。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210822IP3.png" alt="image-20210823000245250" /><figcaption>image-20210823000245250</figcaption>
</figure>
<h2 id="子网掩码">子网掩码</h2>
<p>子网掩码也是一种划分网络号和主机号的方式。掩码可用于掩盖掉主机号，剩下的就是网络号。（<strong>将子网掩码和IP地址按位计算AND，就可得到网络号</strong>）</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210823IP4.png" alt="image-20210823223237596" /><figcaption>image-20210823223237596</figcaption>
</figure>
<p>子网掩码除了划分主机和网络地址外，还可以划分子网。</p>
<h2 id="公有和私有地址">公有和私有地址</h2>
<p>在A、B、C分类地址，实际上有分公有IP地址和私有IP地址。平时我们办公室、家里、学校用的IP地址，一般都是私有IP地址。因为这些地址允许组织内部的IT人员自己管理、自己分配，而且可以重复。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210823IP5.png" alt="image-20210823223819995" /><figcaption>image-20210823223819995</figcaption>
</figure>
<h1 id="ip地址和路由控制">IP地址和路由控制</h1>
<p>IP地址的网络地址这一部分是用于进行路由控制。路由控制表中记录着网络地址与下一步应该发送至路由器的地址。在主机和路由器上都会有各自的路由器控制表。在发送IP包时，首先要确定IP包首部中的目标地址，再从路由控制表中找到与该地址具有相同网络地址的记录，根据该记录将IP包转发给相应的下一个路由器。如果路由控制表中存在多条相同网络地址的记录，就选择相同位数最多的网络地址，也就是最长匹配。</p>
<p>计算机使用一个特殊的IP地址127.0.0.1作为环回地址。与该地址具有相同意义的是一个叫做localhost的主机名。使用这个IP或主机名时，数据包不会流向网络。</p>
<h1 id="ip相关">IP相关</h1>
<h2 id="dns解析">DNS解析</h2>
<p>我们在上网的时候，通常使用的方式是域名，而不是IP地址，因为域名方便人类记忆。那么实现这一技术的就是 DNS域名解析，DNS可以将域名网址自动转换为具体的IP地址。在域名中,越靠右的位置表示其层级越高。</p>
<h3 id="dns解析工作流程">DNS解析工作流程</h3>
<p>浏览器首先看一下自己的缓存里有没有，如果没有就向操作系统的缓存要，还没有就检查本机域名解析文件 hosts ，如果还是没有，就会DNS 服务器进行查询，查询的过程如下:</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210823IP6.png" alt="image-20210823224853359" /><figcaption>image-20210823224853359</figcaption>
</figure>
<h2 id="arp协议">ARP协议</h2>
<p>在传输一个IP数据报的时候，确定了源IP地址和目标IP地址后，就会通过主机「路由表」确定IP数据包下一跳。然而，网络层的下一层是数据链路层，所以我们还要知道「下一跳」的MAC地址。由于主机的路由表中可以找到下一跳的IP地址，所以可以通过ARP协议，求得下一跳的MAC地址。简单地说，ARP是借助ARP请求与 ARP响应两种类型的包确定 MAC地址的。</p>
<ul>
<li><p>主机会通过广播发送ARP请求，这个包中包含了想要知道的MAC地址的主机IP地址。</p></li>
<li><p>当同个链路中的所有设备收到ARP请求时，会去拆开ARP请求包里的内容，如果 ARP请求包中的目标IP地址与自己的IP地址一致，那么这个设备就将自己的MAC地址塞入ARP响应包返回给主机。</p></li>
</ul>
<p>操作系统通常会把第一次通过ARP获取的MAC地址缓存起来，以便下次直接从缓存中找到对应IP地址的 MAC地址。不过，MAC地址的缓存是有一定期限的，超过这个期限，缓存的内容将被清除。</p>
<p>ARP协议是已知IP地址求MAC地址，那 <strong>RARP</strong> 协议正好相反，它是已知MAC地址求IP地址。例如将打印机服务器等小型嵌入式设备接入到网络时就经常会用得到。</p>
<p>通常这需要架设一台RARP服务器，在这个服务器上注册设备的MAC地址及其IP地址。然后再将这个设备接入到网络，接着:</p>
<ul>
<li><p>该设备会发送一条「我的MAC地址是XXXX，请告诉我，我的IP地址应该是什么」的请求信息。</p></li>
<li><p>RARP服务器接到这个消息后返回「MAC地址为XXXX的设备，IP地址为XXXX」的信息给这个设备。</p></li>
</ul>
<p>最后，设备就根据从RARP 服务器所收到的应答信息设置自己的IP地址。</p>
<h2 id="dhcp">DHCP</h2>
<p>DHCP 在生活中我们是很常见的了，我们的电脑通常都是通过DHCP 动态获取IP地址，大大省去了配IP信息繁琐的过程。 DHCP客户端进程监听的是68端口号，DHCP服务端进程监听的是67端口号。</p>
<ul>
<li>客户端首先发起<strong>DHCP 发现报文(DHCP DISCOVER)</strong>的IP数据报，由于客户端没有IP地址,也不知道DHCP服务器的地址，所以使用的是UDP广播通信，其使用的广播目的地址是 255.255.255.255(端口67)并且使用0.0.0.0(端口68)作为源IP地址。DHCP 客户端将该IP数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。</li>
<li>DHCP服务器收到DHCP发现报文时，用 DHCP提供报文(DHCP OFFER）向客户端做出响应。该报文仍然使用IP广播地址255.255.255.255，该报文信息携带服务器提供可租约的Il地址、子网掩码、默认网关、DNS 服务器以及IP地址租用期。</li>
<li>客户端收到一个或多个服务器的DHCP提供报文后，从中选择一个服务器，并向选中的服务器发送DHCP请求报文(DHCP REQUEST)进行响应，回显配置的参数。</li>
<li>最后，服务端用 DHCP ACK报文对DHCP请求报文进行响应，应答所要求的参数。</li>
</ul>
<p>一旦客户端收到DHCP ACK后，交互便完成了，并且客户端能够在租用期内使用DHCP 服务器分配的IP地址。 如果租约的 DHCP IP地址快期后，客户端会向服务器发送 DHCP请求报文:</p>
<ul>
<li>服务器如果同意继续租用，则用DHCP ACK报文进行应答，客户端就会延长租期。</li>
<li>服务器如果不同意继续租用，则用DHCP NACK报文，客户端就要停止使用租约的IP地址。</li>
</ul>
<p>DHCP交互中，全程都是使用UDP广播通信。</p>
<p>为了解决服务器和客户端不在同一个局域网内无法广播这一问题，就出现了DHCP 中继代理。有了DHCP中继代理以后，对不同网段的IP地址分配也可以由一个DHCP服务器统一进行管理。</p>
<h2 id="nat">NAT</h2>
<p>IPv4的地址是非常紧缺的，在前面我们也提到可以通过无分类地址来减缓IPv4地址耗尽的速度，但是互联网的用户增速是非常惊人的，所以IPv4地址依然有被耗尽的危险。于是，<strong>提出了一种网络地址转换NAT的方法</strong>，再次缓解了IPv4地址耗尽的问题。</p>
<p>由于绝大多数的网络应用都是使用传输层协议<strong>TCP或UDP</strong>来传输数据的。因此,可以把P地址＋端口号一起进行转换。这样，就用一个全球IP地址就可以了，这种转换技术就叫<strong>网络地址与端口转换NAPT</strong>。</p>
<p>图中有两个客户端192.168.1.10和192.168.1.11同时与服务器183.232.231.172进行通信，并且这两个客户端的本地端口都是1025。此时，两个私有IP地址都转换IP地址为公有地址120.229.176.121，但是以不同的端口号作为区分。 于是，生成一个NAPT路由器的转换表，就可以正确地转换地址跟端口的组合，令客户端A、B能同时与服务器之间进行通信。这种转换表在NAT路由器上自动生成。例如，在TCP的情况下，建立TCP连接首次握手时的SYN包一经发出，就会生成这个表。而后又随着收到关闭连接时发出FIN包的确认应答从表中被删除。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210823IP7.png" alt="image-20210823231020092" /><figcaption>image-20210823231020092</figcaption>
</figure>
<h3 id="nat的缺点">NAT的缺点</h3>
<p>由于NAT/NAPT都依赖于自己的转换表，因此会有以下的问题:</p>
<ul>
<li>外部无法主动与NAT内部服务器建立连接，因为 NAPT转换表没有转换记录。</li>
<li><p>转换表的生成与转换操作都会产生性能开销。</p></li>
<li><p>通信过程中，如果NAT路由器重启了，所有的 TCP连接都将被重置。</p></li>
</ul>
<h4 id="解决方式">解决方式</h4>
<p><strong>第一种就是改用IPv6</strong> IPv6可用范围非常大，以至于每台设备都可以配置一个公有IP地址，就不搞那么多花里胡哨的地址转换了，但是IPv6普及速度还需要一些时间。 <strong>第二种NAT 穿透技术</strong> NAT穿透技术拥有这样的功能，它能够让网络应用程序主动发现自己位于NAT设备之后，并且会主动获得NAT设备的公有P，并为自己建立端口映射条目，注意这些都是NAT设备后的应用程序自动完成的。也就是说，在NAT穿透技术中，NAT设备后的应用程序处于主动地位，它已经明确地知道NAT设备要修改它外发的数据包，于是它主动配合NAT 设备的操作，主动地建立好映射，这样就不像以前由NAT设备来建立映射了。说人话，就是客户端主动从NAT设备获取公有IP地址，然后自己建立端映射条目，然后用这个条目对外通信，就不需要NAT设备来进行转换了。</p>
<h2 id="icmp">ICMP</h2>
<p>ICMP(Internet Control Message Protocol)是互联网控制报文协议。</p>
<p><strong>ICMP</strong>主要的功能包括:确认IP包是否成功送达目标地址、报告发送过程中I包被废弃的原因和改善网络设置等。</p>
<p>在IP通信中如果某个IP包因为某种原因未能达到目标地址，那么这个具体的原因将由ICMP 负责通知。</p>
<p>ICMP大致可以分为两大类:</p>
<ul>
<li>一类是用于诊断的查询消息，也就是「查询报文类型」</li>
<li>另一类是通知出错原因的错误消息，也就是「差错报文类型」</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210823IP8.png" alt="image-20210823231945110" /><figcaption>image-20210823231945110</figcaption>
</figure>
<h2 id="igmp">IGMP</h2>
<p>在前面我们知道了组播地址，也就是D类地址，既然是组播，那就说明是只有一组的主机能收到数据包，不在一组的主机不能收到数据包，怎么管理是否是在一组呢?那么，就需要IGMP协议了。<strong>ICMP跟IGMP是一点关系都没有的</strong>。</p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>kmp解析</title>
    <url>/2021/12/11/KMP%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>KMP(Knuth-Morris-Pratt)算法是一种字符串匹配算法，由D.E.Knuth，J.H.Morris和V.R.Pratt提出的，因此人们称它为克努特—莫里斯—普拉特操作（简称KMP算法），正是用它的三个发明者的名字缩写来命名的。KMP算法的核心是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。具体实现就是通过一个next数组实现，数组本身包含了模式串的局部匹配信息。KMP算法的时间复杂度为<span class="math inline">\(O(m+n)\)</span>.</p>
<h1 id="原理">原理</h1>
<p>KMP算法的核心有以下几点：</p>
<ul>
<li>模式字符串(用来匹配的字符串)的next数组；</li>
<li>用于求取next数组的最长公共(相等)前后缀；</li>
<li>匹配失败后的回溯.</li>
</ul>
<h2 id="最长公共前后缀">最长公共前后缀</h2>
<ul>
<li>前缀：不包含最后一个字符；</li>
<li>后缀：不包含第一个字符；</li>
</ul>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aabaacaab</span><br><span class="line">最长公共前后缀为aab，长度为3</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line">前缀和后缀均为&quot;&quot;(空),即没有前后缀，公共前后缀长度为0</span><br></pre></td></tr></table></figure>
<p>求解最长公共前后缀，可以使用动态规划，dp[i]表示子串s[0:i+1]的最长公共前缀(如下所示)。dp数组存放当前[0,i](左闭右闭)个字符的最长公共前后缀的长度，求dp[i]的时候，我们已经知道dp[i-1]，即[0,i-1]子串中最长公共前后缀的长度，设dp[i-1]=x，s[0,i-1]的公共前缀为,s[0:x]，s[x]为最长公共前缀的后面一位，比较s[x]与s[i],若它们相等则s[i]的最长公共前后缀为dp[x]+1=dp[i-1]+1，若s[x]与s[i]不相等，则需要找次长公共前后缀(相当于将前缀移动到后缀的地方)。 <span class="math display">\[
dp[i](LoopNext(index))=\left\{
\begin{aligned}
&amp;dp[index-1]+1&amp;,&amp;s[i]==s[dp[index-1]]\\
&amp;0&amp;,&amp;other
\end{aligned}
\right.
\]</span></p>
<p>index是记录匹配是跳转情况的临时指针。以i=6为例(最后一个字符b),<code>s[dp[6-1]]=s[3]=a!=s[6]=b</code>，因此选择跳转到次最长的公共前后缀，index=dp[6-1]=3，重复操作，<code>s[dp[3-1]]=s[1]=b=s[i]=b</code>，出现相等情况，跳出循环，其它情况为临时的指针递减至0也未找到相等的字符串，此时也跳出循环。<code>dp[6] = dp[3-1]+1=2</code>.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">i &#x3D; 0123456</span><br><span class="line">s &#x3D; abaabab</span><br><span class="line">dp&#x3D; 0011232</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 动规求解最长公共前后缀</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_cal</span>(<span class="params">s</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(s) <span class="comment"># 字符串长度</span></span><br><span class="line">    dp = [<span class="number">0</span>] * n <span class="comment"># 动规数组，KMP中的next数组，也有其它叫法...</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        index = i <span class="comment"># 临时指针</span></span><br><span class="line">        <span class="keyword">while</span> index &gt; <span class="number">0</span> <span class="keyword">and</span> s[dp[index - <span class="number">1</span>]] != s[i]: <span class="comment"># 直到找到匹配字符或指针归0</span></span><br><span class="line">            index = dp[index - <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> s[dp[index - <span class="number">1</span>]] == s[i]: <span class="comment"># 有匹配项</span></span><br><span class="line">            dp[i] = dp[index - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 没有任何可以匹配的</span></span><br><span class="line">            dp[i] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dp</span><br></pre></td></tr></table></figure>
<h2 id="字符串匹配">字符串匹配</h2>
<p>至此，已经使用动态规划得到模式串的next数组，接下来的匹配过程也很简单。模式串初始化一个索引指针，表示这个指针之前的字符均匹配，匹配串作一次主循环，对于匹配串的每一个字符，使用模式串指针所指的字符进行比较，若相等两个字符串的指针都加一，若不等，则利用next数组将模式串的指针进行回溯(<strong>等价于将最长公共前缀放到最长公共后缀的地方</strong>)，这个地方的实现方法有多种，例如<code>移位数等于已匹配的字符串长度减去最长公共前后缀长度</code>等，有些next数组从-1开始等，但核心思想均为前缀移到后缀的地方。若移动之后还是不像等则使用next数组继续移动前缀到后缀处，直到相等或模式串指针到0(next数组性质决定其为递减，一直循环终将至0)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMP</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search_string</span>(<span class="params">self, s, target</span>):</span></span><br><span class="line">        next_arr = self.__next_arr(target) <span class="comment"># 建立next数组</span></span><br><span class="line">        n_s = <span class="built_in">len</span>(s) <span class="comment"># 匹配串长度</span></span><br><span class="line">        n_t = <span class="built_in">len</span>(target) <span class="comment"># 模式串长度</span></span><br><span class="line">        index_t = <span class="number">0</span> <span class="comment"># 模式串索引指针</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_s):</span><br><span class="line">            <span class="keyword">if</span> s[i] == target[index_t]: <span class="comment"># 字符相等，两者均后移</span></span><br><span class="line">                index_t += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 字符不相等</span></span><br><span class="line">                <span class="keyword">while</span> index_t != <span class="number">0</span> <span class="keyword">and</span> s[i] != target[index_t]: <span class="comment"># 使用next数组循环匹配</span></span><br><span class="line">                    index_t = next_arr[index_t - <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> index_t == n_t: <span class="comment"># 模式串匹配完成</span></span><br><span class="line">                print(i - n_t + <span class="number">1</span>) <span class="comment"># 输出匹配串的起始索引</span></span><br><span class="line">                index_t = next_arr[index_t - <span class="number">1</span>] <span class="comment"># 继续匹配下一个</span></span><br><span class="line">        <span class="keyword">return</span> next_arr</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next_arr</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            index = i</span><br><span class="line">            <span class="keyword">while</span> index &gt; <span class="number">0</span> <span class="keyword">and</span> s[dp[index - <span class="number">1</span>]] != s[i]:</span><br><span class="line">                index = dp[index - <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> s[dp[index - <span class="number">1</span>]] == s[i]:</span><br><span class="line">                dp[i] = dp[index - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> dp</span><br></pre></td></tr></table></figure>
<h1 id="实现">实现</h1>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> s <span class="keyword">string</span></span><br><span class="line">	s = <span class="string">&quot;CEBDAEEAACEBDAE&quot;</span> <span class="comment">//匹配串</span></span><br><span class="line">	target := <span class="string">&quot;EBDAE&quot;</span> <span class="comment">// 模式串</span></span><br><span class="line">	ans := kmp(s, target)</span><br><span class="line">	fmt.Println(ans)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">calNext</span><span class="params">(s <span class="keyword">string</span>)</span> []<span class="title">int</span></span> &#123;</span><br><span class="line">	n := <span class="built_in">len</span>(s)</span><br><span class="line">	<span class="keyword">var</span> index <span class="keyword">int</span></span><br><span class="line">	dp := <span class="built_in">make</span>([]<span class="keyword">int</span>, n)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt; n; i++ &#123;</span><br><span class="line">		index = i</span><br><span class="line">		<span class="keyword">for</span> index &gt; <span class="number">0</span> &amp;&amp; s[dp[index<span class="number">-1</span>]] != s[i] &#123;</span><br><span class="line">			index = dp[index<span class="number">-1</span>]</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> index == <span class="number">0</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> s[<span class="number">0</span>] == s[i] &#123;</span><br><span class="line">				dp[i] = <span class="number">1</span></span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				dp[i] = <span class="number">0</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			dp[i] = dp[index<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> dp</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">kmp</span><span class="params">(s, target <span class="keyword">string</span>)</span> []<span class="title">int</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> index <span class="keyword">int</span></span><br><span class="line">	n_s, n_t := <span class="built_in">len</span>(s), <span class="built_in">len</span>(target)</span><br><span class="line">	next_arr := calNext(target)</span><br><span class="line">	<span class="keyword">var</span> ans []<span class="keyword">int</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; n_s; i++ &#123;</span><br><span class="line">		<span class="keyword">if</span> s[i] == target[index] &#123;</span><br><span class="line">			index++</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">for</span> index &gt; <span class="number">0</span> &amp;&amp; s[i] != target[next_arr[index<span class="number">-1</span>]] &#123;</span><br><span class="line">				index = next_arr[index<span class="number">-1</span>]</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> n_t == index &#123;</span><br><span class="line">			ans = <span class="built_in">append</span>(ans, i-n_t+<span class="number">1</span>)</span><br><span class="line">			index = next_arr[index<span class="number">-1</span>]</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> ans</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>NPS搭建</title>
    <url>/2021/06/27/NPS%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><strong>NPS</strong>是一款轻量级、高性能、功能强大的<strong>内网穿透</strong>代理服务器。目前支持<strong>tcp、udp流量转发</strong>，可支持任何<strong>tcp、udp</strong>上层协议（访问内网网站、本地支付接口调试、ssh访问、远程桌面，内网dns解析等等……），此外还<strong>支持内网http代理、内网socks5代理</strong>、<strong>p2p等</strong>，并带有功能强大的web管理端。</p>
<hr />
<h1 id="准备工作">准备工作</h1>
<ul>
<li>带有公网IP的服务器(此处以阿里云为例)</li>
<li>本地服务器</li>
<li>NPS服务端和客户端</li>
</ul>
<hr />
<h1 id="步骤">步骤</h1>
<p>前往<a href="https://github.com/ehang-io/nps" class="uri">https://github.com/ehang-io/nps</a>可查看NPS相关信息。在公网服务器和本地服务器<strong>分别</strong>下载安装服务端和客户端(<a href="https://github.com/ehang-io/nps/releases">根据所用系统自行选择</a>)。</p>
<h2 id="服务端">服务端</h2>
<p><strong>在公网IP服务器下载server安装包并配置</strong></p>
<p>将nps的压缩包解压到自定义安装位置，其中的conf文件夹中nps.conf配置文件可以配置相关参数，主要修改用于配置nps的web端的用户名和密码。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#web</span><br><span class="line">web_host&#x3D;a.o.com</span><br><span class="line">web_username&#x3D;admin  &lt;-可不修改</span><br><span class="line">web_password&#x3D;123    &lt;-建议修改，提高安全性</span><br><span class="line">web_port &#x3D; 8080</span><br><span class="line">web_ip&#x3D;0.0.0.0</span><br><span class="line">web_base_url&#x3D;</span><br><span class="line">web_open_ssl&#x3D;false</span><br><span class="line">web_cert_file&#x3D;conf&#x2F;server.pem</span><br><span class="line">web_key_file&#x3D;conf&#x2F;server.key</span><br><span class="line"># if web under proxy use sub path. like http:&#x2F;&#x2F;host&#x2F;nps need this.</span><br><span class="line">#web_base_url&#x3D;&#x2F;nps</span><br></pre></td></tr></table></figure>
<p>使用管理员运行cmd命令行。路径变换到nps的<strong>安装目录</strong>，执行<code>nps install</code>，此时会在<code>C:\Program Files</code>文件夹下产生新的nps配置文件，之后的nps配置直接配置此处文件生效。使用<code>nps start</code>启用服务端，<code>nps stop</code>停止服务，<code>nps restart</code>重启服务。</p>
<p>服务端启用后进入<code>ip:web_port</code>(默认端口为8080)进行web端部署，输入用户名及密码(同配置文件)登录。</p>
<p><strong>nps</strong> web端登录界面：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627npslogin.png" alt="image-20210627222122777" /><figcaption>image-20210627222122777</figcaption>
</figure>
<p><strong>nps</strong> web配置界面：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627npsWeb.png" alt="image-20210627222254529" /><figcaption>image-20210627222254529</figcaption>
</figure>
<p><strong>新增一个客户端</strong>：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-3.png" alt="image-20210627222504249" /><figcaption>image-20210627222504249</figcaption>
</figure>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-4.png" alt="image-20210627222749706" /><figcaption>image-20210627222749706</figcaption>
</figure>
<p>留意客户端id及唯一验证密匙，之后会用到：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-key.png" alt="image-20210627222956319" /><figcaption>image-20210627222956319</figcaption>
</figure>
<h2 id="客户端">客户端</h2>
<p><strong>在本地服务器下载client安装包并配置</strong></p>
<p><strong>npc</strong>的和<strong>nps</strong>的命令使用方式相似。npc直接在其安装目录下的conf文件夹中的npc.conf文件中进行配置，删除掉冗余信息，只保留常规配置中的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[common]</span><br><span class="line">server_addr&#x3D;ip:8024    &lt;-填入公网ip即客户端连接端口(默认8024)</span><br><span class="line">conn_type&#x3D;tcp</span><br><span class="line">vkey&#x3D;3raa67c9m18yflr4  &lt;-填入唯一验证密匙</span><br><span class="line">auto_reconnection&#x3D;true</span><br><span class="line">max_conn&#x3D;1000</span><br><span class="line">flow_limit&#x3D;1000</span><br><span class="line">rate_limit&#x3D;1000</span><br><span class="line">basic_username&#x3D;11</span><br><span class="line">basic_password&#x3D;3</span><br><span class="line">web_username&#x3D;user</span><br><span class="line">web_password&#x3D;1234</span><br><span class="line">crypt&#x3D;true</span><br><span class="line">compress&#x3D;true</span><br></pre></td></tr></table></figure>
<p>之后直接运行npc.exe即可连接至服务端。也可以命令行使用start命令运行。之后可以看到服务端的已连接上：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-5.png" alt="image-20210627224109081" /><figcaption>image-20210627224109081</figcaption>
</figure>
<h2 id="添加隧道">添加隧道</h2>
<p>此时公网ip服务器已和本地连接，可以添加端口映射：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-6.png" alt="image-20210627224408860" /><figcaption>image-20210627224408860</figcaption>
</figure>
<p>配置端口后，浏览器访问<code>公网ip:服务端端口</code>后可以映射至<code>内网地址:端口</code></p>
<h2 id="云服务器配置">云服务器配置</h2>
<p>有的云服务器没有开放响应端口，此时诸多功能限制，如客户端与服务端无法连接等。以阿里云为开放所有端口。</p>
<p>在云服务器ECS中创建新的安全组：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-7.png" alt="image-20210627224917076" /><figcaption>image-20210627224917076</figcaption>
</figure>
<p>可选择打开所需的端口，此处设置全部打开，设置完成后创建安全组。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-8.png" alt="image-20210627225139193" /><figcaption>image-20210627225139193</figcaption>
</figure>
<p>最后将刚才创建的安全组加入实例中：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210627nps-9.png" alt="image-20210627225332450" /><figcaption>image-20210627225332450</figcaption>
</figure>
<p>端口打开完成。 END</p>
<hr />
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>OSError:[WinError 1455]</title>
    <url>/2021/08/28/OSError-WinError-1455/</url>
    <content><![CDATA[<hr />
<p>使用pytorch并调用GPU进行训练时，出现错误<code>ImportError: DLL load failed: 页面文件太小，无法完成操作</code>，而使用CPU进行训练却没有问题。网上给出的解决办法有如下：</p>
<ul>
<li>重启pycharm(鸡肋的办法)</li>
<li>把num_works设置为0，调用了多进程读取数据造成的内存不足，可以考虑🤔</li>
<li>修改虚拟内存大小(终极办法)</li>
</ul>
<p>亲测第三个终极办法比较有效：</p>
<p>将自动管理所有驱动器分页文件大小取消勾选：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210828error1.png" alt="image-20210828151406390" /><figcaption>image-20210828151406390</figcaption>
</figure>
<p>找到python的安装目录(虚拟环境目录)所在的盘符，自定义大小&gt;设置初始值和最大值&gt;点击设置确认，看情况重启一下电脑生效，大功告成。调小batchsize也可减小空间的占用。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210828error2.png" alt="image-20210828151545223" /><figcaption>image-20210828151545223</figcaption>
</figure>
<hr />
<h1 id="注意">注意</h1>
<p>划分虚拟内存将占用相应的磁盘空间，酌情划分。</p>
]]></content>
      <categories>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>PaddleDetect运用</title>
    <url>/2021/08/26/PaddleDetect%E8%BF%90%E7%94%A8/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>PaddleDetection模块化地实现了多种主流目标检测算法，提供了丰富的数据增强策略、网络模块组件（如骨干网络）、损失函数等，并集成了模型压缩和跨平台高性能部署能力。</p>
<h2 id="环境要求">环境要求</h2>
<ul>
<li>PaddlePaddle 2.1</li>
<li>OS 64位操作系统</li>
<li>Python 3(3.5.1+/3.6/3.7/3.8/3.9)，64位版本</li>
<li>pip/pip3(9.0.1+)，64位版本</li>
<li>CUDA &gt;= 10.1</li>
<li>cuDNN &gt;= 7.6</li>
</ul>
<hr />
<h1 id="安装">安装</h1>
<h2 id="安装paddlepaddle"><a href="https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/windows-pip.html">安装PaddlePaddle</a></h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CUDA10.1</span><br><span class="line">python -m pip install paddlepaddle-gpu&#x3D;&#x3D;2.1.0.post101 -f https:&#x2F;&#x2F;paddlepaddle.org.cn&#x2F;whl&#x2F;mkl&#x2F;stable.html</span><br><span class="line"></span><br><span class="line"># CPU</span><br><span class="line">python -m pip install paddlepaddle -i https:&#x2F;&#x2F;mirror.baidu.com&#x2F;pypi&#x2F;simple</span><br></pre></td></tr></table></figure>
<h2 id="验证paddle是否安装成功">验证Paddle是否安装成功</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在您的Python解释器中确认PaddlePaddle安装成功</span><br><span class="line">&gt;&gt;&gt; import paddle</span><br><span class="line">&gt;&gt;&gt; paddle.utils.run_check()</span><br><span class="line"></span><br><span class="line"># 确认PaddlePaddle版本</span><br><span class="line">python -c &quot;import paddle; print(paddle.__version__)&quot;</span><br></pre></td></tr></table></figure>
<h2 id="安装paddledetection">安装PaddleDetection</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 克隆PaddleDetection仓库</span><br><span class="line">cd &lt;path&#x2F;to&#x2F;clone&#x2F;PaddleDetection&gt;</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;PaddlePaddle&#x2F;PaddleDetection.git</span><br><span class="line"></span><br><span class="line"># 编译安装paddledet</span><br><span class="line">cd PaddleDetection</span><br><span class="line">python setup.py install</span><br><span class="line"></span><br><span class="line"># 安装其他依赖</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p><code>pycocotools</code>依赖可能安装失败，可采用第三方实现版本，自己编译:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install git+https:&#x2F;&#x2F;github.com&#x2F;philferriere&#x2F;cocoapi.git#subdirectory&#x3D;PythonAPI</span><br></pre></td></tr></table></figure>
<p>此过程可能报错提示<code>error: Microsoft Visual C++ 14.0 or greater is required. Get it with .......</code>，此时需要安装<code>Microsoft Visual C++ build tool</code>，推荐使用离线版安装，否则会出现下载安装包速度缓慢、安装包损坏等问题。</p>
<p>解决所用问题后安装完所有依赖，进行测试:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python ppdet&#x2F;modeling&#x2F;tests&#x2F;test_architectures.py</span><br></pre></td></tr></table></figure>
<p>出现如下的类似信息则说明安装完成:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">W0826 18:48:00.794705  6988 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.4, Runtime API Version: 11.2</span><br><span class="line">W0826 18:48:00.816314  6988 device_context.cc:422] device: 0, cuDNN Version: 8.2.</span><br><span class="line">.....</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">Ran 5 tests in 2.461s</span><br><span class="line"></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="数据集制作">数据集制作</h1>
<p>目录结构：</p>
<ul>
<li>dataset/
<ul>
<li>mydataset/
<ul>
<li>annotation/ 存放xml格式的标注文件</li>
<li>images/ 存放原始图片文件</li>
<li>ImageSets/ 数据集索引
<ul>
<li>label_list.txt 存放类别标签</li>
<li>train.txt 训练集数据名称</li>
<li>val.txt 验证集数据名称</li>
</ul></li>
<li>train.txt 最终的数据集索引格式(create_list.py制作)</li>
<li>val.txt 最终的数据集索引格式(create_list.py制作)</li>
<li>create_list.py 从ImageSets/Main/下的训练验证集制作数据索引</li>
</ul></li>
<li>get_list.py 按比例划分训练集验证集等</li>
</ul></li>
</ul>
<p><strong>get_list.py示例：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">train_precent = <span class="number">0.9</span></span><br><span class="line">path = <span class="string">&#x27;./fruits&#x27;</span>  <span class="comment"># root path</span></span><br><span class="line">xml = path + <span class="string">&quot;/Annotations&quot;</span></span><br><span class="line">save = path + <span class="string">&quot;/Main&quot;</span></span><br><span class="line">total_xml = os.listdir(xml)</span><br><span class="line"></span><br><span class="line">num = <span class="built_in">len</span>(total_xml)</span><br><span class="line">tr = <span class="built_in">int</span>(num * train_precent)</span><br><span class="line">index = <span class="built_in">list</span>(<span class="built_in">range</span>(num))</span><br><span class="line">random.shuffle(index)</span><br><span class="line"></span><br><span class="line">ftrain = <span class="built_in">open</span>(path + <span class="string">&quot;/ImageSets/train.txt&quot;</span>, <span class="string">&quot;w&quot;</span>)</span><br><span class="line">fval = <span class="built_in">open</span>(path + <span class="string">&quot;/ImageSets/val.txt&quot;</span>, <span class="string">&quot;w&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">    name = total_xml[index[i]][:-<span class="number">4</span>] + <span class="string">&quot;\n&quot;</span></span><br><span class="line">    <span class="keyword">if</span> i &lt; tr:</span><br><span class="line">        ftrain.write(name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fval.write(name)</span><br><span class="line"></span><br><span class="line">ftrain.close()</span><br><span class="line">fval.close()</span><br></pre></td></tr></table></figure>
<p><strong>create_list.py示例：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dir</span>(<span class="params">devkit_dir, <span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> osp.join(devkit_dir, <span class="built_in">type</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">walk_dir</span>(<span class="params">devkit_dir</span>):</span></span><br><span class="line">    filelist_dir = get_dir(devkit_dir, <span class="string">&#x27;ImageSets/&#x27;</span>)</span><br><span class="line">    annotation_dir = get_dir(devkit_dir, <span class="string">&#x27;Annotations&#x27;</span>)</span><br><span class="line">    img_dir = get_dir(devkit_dir, <span class="string">&#x27;JPEGImages&#x27;</span>)</span><br><span class="line">    trainval_list = []</span><br><span class="line">    test_list = []</span><br><span class="line">    added = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, _, files <span class="keyword">in</span> os.walk(filelist_dir):</span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> files:</span><br><span class="line">            img_ann_list = []</span><br><span class="line">            <span class="keyword">if</span> re.match(<span class="string">&#x27;train\.txt&#x27;</span>, fname):</span><br><span class="line">                img_ann_list = trainval_list</span><br><span class="line">            <span class="keyword">elif</span> re.match(<span class="string">&#x27;val\.txt&#x27;</span>, fname):</span><br><span class="line">                img_ann_list = test_list</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            fpath = osp.join(filelist_dir, fname)</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(fpath):</span><br><span class="line">                name_prefix = line.strip().split()[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> name_prefix <span class="keyword">in</span> added:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                added.add(name_prefix)</span><br><span class="line">                ann_path = osp.join(annotation_dir, name_prefix + <span class="string">&#x27;.xml&#x27;</span>)</span><br><span class="line">                img_path = osp.join(img_dir, name_prefix + <span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line">                <span class="comment"># ann_path = osp.join(name_prefix + &#x27;.xml&#x27;)</span></span><br><span class="line">                <span class="comment"># img_path = osp.join(name_prefix + &#x27;.jpg&#x27;)</span></span><br><span class="line">                <span class="keyword">assert</span> os.path.isfile(ann_path), <span class="string">&#x27;file %s not found.&#x27;</span> % ann_path</span><br><span class="line">                <span class="keyword">assert</span> os.path.isfile(img_path), <span class="string">&#x27;file %s not found.&#x27;</span> % img_path</span><br><span class="line">                img_ann_list.append((img_path, ann_path))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trainval_list, test_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_filelist</span>(<span class="params">devkit_dir, output_dir</span>):</span></span><br><span class="line">    trainval_list = []</span><br><span class="line">    test_list = []</span><br><span class="line">    trainval, test = walk_dir(devkit_dir)</span><br><span class="line">    trainval_list.extend(trainval)</span><br><span class="line">    test_list.extend(test)</span><br><span class="line">    random.shuffle(trainval_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(osp.join(output_dir, <span class="string">&#x27;train.txt&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> ftrainval:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> trainval_list:</span><br><span class="line">            ftrainval.write(item[<span class="number">0</span>] + <span class="string">&#x27; &#x27;</span> + item[<span class="number">1</span>] + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(osp.join(output_dir, <span class="string">&#x27;val.txt&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> ftest:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> test_list:</span><br><span class="line">            ftest.write(item[<span class="number">0</span>] + <span class="string">&#x27; &#x27;</span> + item[<span class="number">1</span>] + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    prepare_filelist(devkit_dir=<span class="string">&#x27;./&#x27;</span>, output_dir=<span class="string">&#x27;./&#x27;</span>)</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="配置文件">配置文件：</h1>
<p>在configs文件夹中选择模型，根据模型中的配置文件做相应更改：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ppyolo_mbv3_small_coco.yaml</span></span><br><span class="line"><span class="attr">_BASE_:</span> [</span><br><span class="line">  <span class="string">&#x27;../datasets/voc.yml&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;../runtime.yml&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;./_base_/ppyolo_mbv3_small.yml&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;./_base_/optimizer_1x.yml&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;./_base_/ppyolo_reader.yml&#x27;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>如上所示，要调用此模型，需要对其中的五个配置文件进行配置。</p>
<p><strong>数据集配置:</strong></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># voc.yaml</span></span><br><span class="line"><span class="attr">metric:</span> <span class="string">VOC</span></span><br><span class="line"><span class="attr">map_type:</span> <span class="string">11point</span></span><br><span class="line"><span class="attr">num_classes:</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">TrainDataset:</span></span><br><span class="line">  <span class="type">!VOCDataSet</span></span><br><span class="line">    <span class="attr">dataset_dir:</span> <span class="string">dataset/pipes</span> <span class="comment"># dataset/voc</span></span><br><span class="line">    <span class="attr">anno_path:</span> <span class="string">train.txt</span></span><br><span class="line">    <span class="attr">label_list:</span> <span class="string">ImageSets/Main/label_list.txt</span></span><br><span class="line">    <span class="attr">data_fields:</span> [<span class="string">&#x27;image&#x27;</span>, <span class="string">&#x27;gt_bbox&#x27;</span>, <span class="string">&#x27;gt_class&#x27;</span>, <span class="string">&#x27;difficult&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">EvalDataset:</span></span><br><span class="line">  <span class="type">!VOCDataSet</span></span><br><span class="line">    <span class="attr">dataset_dir:</span> <span class="string">dataset/pipes</span></span><br><span class="line">    <span class="attr">anno_path:</span> <span class="string">val.txt</span></span><br><span class="line">    <span class="attr">label_list:</span> <span class="string">ImageSets/Main/label_list.txt</span></span><br><span class="line">    <span class="attr">data_fields:</span> [<span class="string">&#x27;image&#x27;</span>, <span class="string">&#x27;gt_bbox&#x27;</span>, <span class="string">&#x27;gt_class&#x27;</span>, <span class="string">&#x27;difficult&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">TestDataset:</span></span><br><span class="line">  <span class="type">!ImageFolder</span></span><br><span class="line">    <span class="attr">anno_path:</span> <span class="string">dataset/voc/label_list.txt</span></span><br></pre></td></tr></table></figure>
<p>其他配置文件自定义修改.......</p>
<hr />
<h1 id="训练模型">训练模型</h1>
<p>开始训练：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python tools&#x2F;train.py -c configs&#x2F;ppyolo&#x2F;ppyolo_mbv3_small_coco.yml # 单卡</span><br><span class="line">python -m paddle.distributed.launch --gpus 0,1,2,3,4,5,6,7 tools&#x2F;train.py -c configs&#x2F;yolov3&#x2F;yolov3_mobilenet_v1_roadsign.yml # 多卡</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="预测">预测</h1>
<p>推断图片：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python tools&#x2F;infer.py -c configs&#x2F;yolov3&#x2F;yolov3_mobilenet_v1_roadsign.yml --infer_img&#x3D;demo&#x2F;000000570688.jpg -o weights&#x3D;https:&#x2F;&#x2F;paddledet.bj.bcebos.com&#x2F;models&#x2F;yolov3_mobilenet_v1_roadsign.pdparams</span><br></pre></td></tr></table></figure>
<p>设置预测参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES&#x3D;0 #windows和Mac下不需要执行该命令</span><br><span class="line">python tools&#x2F;infer.py -c configs&#x2F;yolov3&#x2F;yolov3_mobilenet_v1_roadsign.yml \</span><br><span class="line">                    --infer_img&#x3D;demo&#x2F;road554.png \</span><br><span class="line">                    --output_dir&#x3D;infer_output&#x2F; \</span><br><span class="line">                    --draw_threshold&#x3D;0.5 \</span><br><span class="line">                    -o weights&#x3D;output&#x2F;yolov3_mobilenet_v1_roadsign&#x2F;model_final \</span><br><span class="line">                    --use_vdl&#x3D;Ture</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="参数解释">参数解释</h1>
<table>
<colgroup>
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 28%" />
<col style="width: 24%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="header">
<th>FLAG</th>
<th>支持脚本</th>
<th>用途</th>
<th>默认值</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>-c</td>
<td>ALL</td>
<td>指定配置文件</td>
<td>None</td>
<td><strong>必选</strong>，例如-c configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.yml</td>
</tr>
<tr class="even">
<td>-o</td>
<td>ALL</td>
<td>设置或更改配置文件里的参数内容</td>
<td>None</td>
<td>相较于<code>-c</code>设置的配置文件有更高优先级，例如：<code>-o use_gpu=False</code></td>
</tr>
<tr class="odd">
<td>--eval</td>
<td>train</td>
<td>是否边训练边测试</td>
<td>False</td>
<td>如需指定，直接<code>--eval</code>即可</td>
</tr>
<tr class="even">
<td>-r/--resume_checkpoint</td>
<td>train</td>
<td>恢复训练加载的权重路径</td>
<td>None</td>
<td>例如：<code>-r output/faster_rcnn_r50_1x_coco/10000</code></td>
</tr>
<tr class="odd">
<td>--slim_config</td>
<td>ALL</td>
<td>模型压缩策略配置文件</td>
<td>None</td>
<td>例如<code>--slim_config configs/slim/prune/yolov3_prune_l1_norm.yml</code></td>
</tr>
<tr class="even">
<td>--use_vdl</td>
<td>train/infer</td>
<td>是否使用<a href="https://github.com/paddlepaddle/visualdl">VisualDL</a>记录数据，进而在VisualDL面板中显示</td>
<td>False</td>
<td>VisualDL需Python&gt;=3.5</td>
</tr>
<tr class="odd">
<td>--vdl_log_dir</td>
<td>train/infer</td>
<td>指定 VisualDL 记录数据的存储路径</td>
<td>train:<code>vdl_log_dir/scalar</code> infer: <code>vdl_log_dir/image</code></td>
<td>VisualDL需Python&gt;=3.5</td>
</tr>
<tr class="even">
<td>--output_eval</td>
<td>eval</td>
<td>评估阶段保存json路径</td>
<td>None</td>
<td>例如 <code>--output_eval=eval_output</code>, 默认为当前路径</td>
</tr>
<tr class="odd">
<td>--json_eval</td>
<td>eval</td>
<td>是否通过已存在的bbox.json或者mask.json进行评估</td>
<td>False</td>
<td>如需指定，直接<code>--json_eval</code>即可， json文件路径在<code>--output_eval</code>中设置</td>
</tr>
<tr class="even">
<td>--classwise</td>
<td>eval</td>
<td>是否评估单类AP和绘制单类PR曲线</td>
<td>False</td>
<td>如需指定，直接<code>--classwise</code>即可</td>
</tr>
<tr class="odd">
<td>--output_dir</td>
<td>infer/export_model</td>
<td>预测后结果或导出模型保存路径</td>
<td><code>./output</code></td>
<td>例如<code>--output_dir=output</code></td>
</tr>
<tr class="even">
<td>--draw_threshold</td>
<td>infer</td>
<td>可视化时分数阈值</td>
<td>0.5</td>
<td>例如<code>--draw_threshold=0.7</code></td>
</tr>
<tr class="odd">
<td>--infer_dir</td>
<td>infer</td>
<td>用于预测的图片文件夹路径</td>
<td>None</td>
<td><code>--infer_img</code>和<code>--infer_dir</code>必须至少设置一个</td>
</tr>
<tr class="even">
<td>--infer_img</td>
<td>infer</td>
<td>用于预测的图片路径</td>
<td>None</td>
<td><code>--infer_img</code>和<code>--infer_dir</code>必须至少设置一个，<code>infer_img</code>具有更高优先级</td>
</tr>
<tr class="odd">
<td>--save_txt</td>
<td>infer</td>
<td>是否在文件夹下将图片的预测结果保存到文本文件中</td>
<td>False</td>
<td>可选</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>PyCharm不抛出异常而意外终止程序处理方式</title>
    <url>/2021/03/20/PyCharm%E4%B8%8D%E6%8A%9B%E5%87%BA%E5%BC%82%E5%B8%B8%E8%80%8C%E6%84%8F%E5%A4%96%E7%BB%88%E6%AD%A2%E7%A8%8B%E5%BA%8F%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="问题">问题</h1>
<p>在编译<span class="math inline">\(python\)</span>时，有时程序会异常终止而不抛出错误信息，而是返回一串没有太大意义的终止信号。对于较简单的程序还能肉眼观察debug，但是对于复杂的工程项目我们就难以定位错误位置。</p>
<h1 id="解决方式">解决方式</h1>
<p>当这种情况出现是可以考虑在终端运行<span class="math inline">\(python\)</span>文件，这样在命令行中会抛出具体的异常信息，方便我们定位错误。</p>
<h2 id="pychram解决">Pychram解决</h2>
<p>当使用<span class="math inline">\(Pychram\)</span>来调试程序时，可以通过： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Run--&gt;Edit Configuration--&gt;Excution--&gt;Emulate terminal in out console</span><br></pre></td></tr></table></figure> 来启动终端模拟而快速的定位Bug.</p>
]]></content>
      <categories>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title>Pycharm之库的导入顺序？</title>
    <url>/2021/03/20/Pycharm%E4%B9%8B%E5%BA%93%E7%9A%84%E5%AF%BC%E5%85%A5%E9%A1%BA%E5%BA%8F%EF%BC%9F/</url>
    <content><![CDATA[<p>使用python时遇到了一个问题:<code>Cannot mix incompatible Qt library (version 0x50907) with this library (version 0x50a01)</code>，通过字面的意思来看这是因为qt版本冲突，网上找了很多解决方法都是删库，这个处理方式着实不方便，后来在一位博主出找到了另一种<a href="https://blog.csdn.net/dream_allday/article/details/95967312">解决方式</a>。<br />
以下是冲突代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br></pre></td></tr></table></figure>
<p>更改后的无报错代码: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br></pre></td></tr></table></figure></p>
<p>对比两版代码的区别可以发现把torch这个库的导入次序排到前面就可以避免报错了， <strong><em>Amazing！</em></strong> 。原来，python的库导入竟然是有讲究的，安排不恰当就有可能报错。<br />
其实遇到这种情况的几率较小，但遇到时何妨不尝试换换导入顺序 <strong>(特别是将torch库提前)</strong> ，总比删库的解决方式更为简单实用些。</p>
]]></content>
      <categories>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch的网络可视化</title>
    <url>/2022/03/10/Pytorch%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<p>有时候想要绘制一个神经网络的网络结构，可以通过代码中层的定义去用第三方软件依次绘制，但是这样费时费力。因此需要一种快速绘制的办法。</p>
<hr />
<h1 id="netron">Netron</h1>
<p>netron可通过所保存的模型将其用网络的方式可视化出来，但是对于pytorch来说，其支持程度还不够，无法绘制各参数间的关系，可以将pytorch模型导出为onnx格式再使用netron来可视化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 自建一个CNN模型并导出为onnx来可视化</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNNModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, out_channels=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNNModel, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">0</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">0</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="number">0</span>)</span><br><span class="line">        )</span><br><span class="line">        self.ful_layer = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">36</span>, <span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">16</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">16</span>, out_channels),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.ful_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNNModel()</span><br><span class="line">x = torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">torch.onnx.export(model, x, <span class="string">&quot;CNN.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>用netron打开后的模型结果：</li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220310pytorchnet1.png" /></p>
<hr />
<h1 id="torchviz">torchviz</h1>
<p>使用netron可以直观的显示出网络的结构，但是需要对模型进行转换，在转换的过程中也许会遇到模块的不兼容等问题，修改麻烦，使用torchviz也可以快速的生成模型结构图。要使用torchviz需要先安装<a href="https://www.graphviz.org/">Graphviz</a>，这个工具在之前的<a href="https://www.xiubenwu.top/2021/12/13/python函数调用关系分析/">文章</a>中也有提及。安装后的使用方式也非常简单，依旧以简单CNN网络为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchviz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNNModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, out_channels=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNNModel, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">0</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">0</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="number">0</span>)</span><br><span class="line">        )</span><br><span class="line">        self.ful_layer = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">36</span>, <span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">16</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">16</span>, out_channels),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.ful_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNNModel()</span><br><span class="line">x = torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">out = model(x)</span><br><span class="line">g = torchviz.make_dot(out)</span><br><span class="line">g.render(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>最终可以生成PDF文件；</li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220310pytorchnet2.png" /></p>
<p>模型结构相较于netron来说更为冗杂。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Re:字符串匹配之美</title>
    <url>/2021/02/12/Re-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E4%B9%8B%E7%BE%8E/</url>
    <content><![CDATA[<p>正则表达式又称正则表示式、正则表示法、规则表达式、常规表示法(Regular Expression，常简写为regex、regexp或RE)</p>
<span id="more"></span>
<h1 id="介绍">介绍</h1>
<p>正则表达式使用单个字符串来描述、匹配一系列匹配某个句法规则的字符串。在很多文本编辑器里，正则表达式通常被用来检索、替换那些匹配某个模式的文本。许多程序设计语言都支持利用正则表达式进行字符串操作。<br />
正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。<br />
利用正则表达式可以从文本中匹配字符串，比如判断是否为一个邮箱，提取文本中的手机号身份证信息等，在网络爬虫处理信息时较为常用。</p>
<h1 id="基础表达式">基础表达式</h1>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><span style="white-space:nowrap;">字符    </span></th>
<th style="text-align: left;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">\</td>
<td style="text-align: left;">将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符“n”。“”匹配一个换行符。串行“\”匹配“”而“(”则匹配“(”。</td>
</tr>
<tr class="even">
<td style="text-align: left;">^</td>
<td style="text-align: left;">匹配输入字符串的<strong>开始</strong>位置。如果设置了RegExp对象的Multiline属性，^也匹配“”或“”之后的位置。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">$</td>
<td style="text-align: left;">匹配输入字符串的<strong>结束</strong>位置。如果设置了RegExp对象的Multiline属性，$也匹配“”或“”之前的位置。</td>
</tr>
<tr class="even">
<td style="text-align: left;">*</td>
<td style="text-align: left;">匹配前面的子表达式<strong>零次</strong>或<strong>多次</strong>。例如，zo<em>能匹配“z”以及“zoo”。</em>等价于{0,}。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">+</td>
<td style="text-align: left;">匹配前面的子表达式<strong>一次</strong>或<strong>多次</strong>。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。</td>
</tr>
<tr class="even">
<td style="text-align: left;">?</td>
<td style="text-align: left;">匹配前面的子表达式<strong>零次</strong>或<strong>一次</strong>。例如，“do(es)?”可以匹配“does”或“does”中的“do”。?等价于{0,1}。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">{n}</td>
<td style="text-align: left;">n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。</td>
</tr>
<tr class="even">
<td style="text-align: left;">{n,m}</td>
<td style="text-align: left;">m和n均为非负整数，其中n&lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">?</td>
<td style="text-align: left;">当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是<strong>非贪婪</strong>的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的<strong>贪婪模式</strong>则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o+”将匹配所有“o”。</td>
</tr>
<tr class="even">
<td style="text-align: left;">.</td>
<td style="text-align: left;">匹配除“”之外的任何单个字符。要匹配包括“”在内的任何字符，请使用像&quot;(. | )&quot;的模式。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">x|y</td>
<td style="text-align: left;">匹配x或y。例如，“z|food”能匹配“z”或“food”。“(z|f)ood”则匹配“zood”或“food”。</td>
</tr>
<tr class="even">
<td style="text-align: left;">[xyz]</td>
<td style="text-align: left;">字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">[^xyz]</td>
<td style="text-align: left;">负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“p”。</td>
</tr>
<tr class="even">
<td style="text-align: left;">[a-z]</td>
<td style="text-align: left;">字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">[^a-z]</td>
<td style="text-align: left;">负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。</td>
</tr>
<tr class="even">
<td style="text-align: left;">\b</td>
<td style="text-align: left;">匹配一个单词边界，也就是指单词和空格间的位置。例如，“er”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配非单词边界。“er\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配由x指明的控制字符。例如，\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配一个数字字符。等价于[0-9]。</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配一个非数字字符。等价于[^0-9]。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配一个换页符。等价于0c和。</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配一个换行符。等价于0a和。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配一个回车符。等价于0d和。</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ 。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配任何非空白字符。等价于[^ 。</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">匹配一个制表符。等价于09和。</td>
</tr>
</tbody>
</table>
 匹配一个垂直制表符。等价于0b和。 | 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。 | 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。 | 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“41”匹配“A”。“041”则等价于“04&amp;1”。正则表达式中可以使用ASCII编码。. | 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\1”匹配两个连续的相同字符。 | 标识一个八进制转义值或一个向后引用。如果，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。 | 标识一个八进制转义值或一个向后引用。如果，则nm为向后引用。如果，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则。 | 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。 | 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Re</tag>
      </tags>
  </entry>
  <entry>
    <title>SCP</title>
    <url>/2021/07/10/SCP/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><strong>scp</strong>是 <strong>secure copy</strong>的缩写, scp是<a href="https://www.linuxprobe.com/">linux系统</a>下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-1  强制scp命令使用协议ssh1</span><br><span class="line"></span><br><span class="line">-2  强制scp命令使用协议ssh2</span><br><span class="line"></span><br><span class="line">-4  强制scp命令只使用IPv4寻址</span><br><span class="line"></span><br><span class="line">-6  强制scp命令只使用IPv6寻址</span><br><span class="line"></span><br><span class="line">-B  使用批处理模式（传输过程中不询问传输口令或短语）</span><br><span class="line"></span><br><span class="line">-C  允许压缩。（将-C标志传递给ssh，从而打开压缩功能）</span><br><span class="line"></span><br><span class="line">-p 保留原文件的修改时间，访问时间和访问权限。</span><br><span class="line"></span><br><span class="line">-q  不显示传输进度条。</span><br><span class="line"></span><br><span class="line">-r  递归复制整个目录。</span><br><span class="line"></span><br><span class="line">-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。</span><br><span class="line"></span><br><span class="line">-c cipher  以cipher将数据传输进行加密，这个选项将直接传递给ssh。</span><br><span class="line"></span><br><span class="line">-F ssh_config  指定一个替代的ssh配置文件，此参数直接传递给ssh。</span><br><span class="line"></span><br><span class="line">-i identity_file  从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。</span><br><span class="line"></span><br><span class="line">-l limit  限定用户所能使用的带宽，以Kbit&#x2F;s为单位。</span><br><span class="line"></span><br><span class="line">-o ssh_option  如果习惯于使用ssh_config(5)中的参数传递方式，</span><br><span class="line"></span><br><span class="line">-P port  注意是大写的P, port是指定数据传输用到的端口号</span><br><span class="line"></span><br><span class="line">-S program  指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="使用示例">使用示例</h1>
<h2 id="上传文件到服务器">上传文件到服务器</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp &#x2F;path&#x2F;local_filename username@servername:&#x2F;path</span><br><span class="line"></span><br><span class="line">scp &#x2F;host&#x2F;test.txt root@192.168.0.101:&#x2F;home&#x2F;test&#x2F;</span><br></pre></td></tr></table></figure>
<p>示例表示将本机host文件夹下的test.txt文件上传到服务器的/home/test文件夹，并使用root用户登录。</p>
<h2 id="从服务器下载文件">从服务器下载文件</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp username@server_address:&#x2F;path&#x2F;filename &#x2F;host&#x2F;local</span><br><span class="line"></span><br><span class="line">scp root@192.168.0.101:&#x2F;home&#x2F;kimi&#x2F;test.txt &#x2F;myfolder</span><br></pre></td></tr></table></figure>
<p>示例表示将服务器上的文件下载到本地的myfolder文件夹(若根目录使用<code>/</code>表示下载到当前盘符的根目录)。</p>
<h2 id="从服务器下载整个目录">从服务器下载整个目录</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r username@server_address:&#x2F;remote_dir &#x2F;local_dir</span><br><span class="line"></span><br><span class="line">scp -r root@192.168.0.101:&#x2F;home &#x2F;local_dir</span><br></pre></td></tr></table></figure>
<p>示例表示将服务器下的文件整个下载到本地目录(-r参数)。</p>
<h2 id="上传目录到服务器">上传目录到服务器</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r &#x2F;tmp&#x2F;local_dir username@servername:remote_dir</span><br><span class="line"></span><br><span class="line">scp -r test root@192.168.0.101:&#x2F;home</span><br></pre></td></tr></table></figure>
<p>将test文件夹上传到服务器home目录。</p>
<h2 id="在两个远程主机间复制文件">在两个远程主机间复制文件</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp root@192.168.1.104:&#x2F;host1&#x2F;xx.txt root@192.168.1.105:&#x2F;host2</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp source target</span><br></pre></td></tr></table></figure>
<p>多文件传输时使用空格隔开。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>server</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>Tools And Extensions</title>
    <url>/2021/02/08/Tools-And-Extensions/</url>
    <content><![CDATA[<h1 id="记录一些一辈子也用不上的工具">记录一些(一辈子也用不上的)工具</h1>
<p>内容持续更新(或许，可能)</p>
<span id="more"></span>
<h2 id="chrome扩展系列">Chrome扩展系列</h2>
<p>新版的Edge浏览器也推出了扩展商店，而且不用vpn来访问。但作为一个Chrome的老用户并没有去尝试，不清楚是否比Chrome更好用。<br />
* <kbd><strong>Tampermonkey</strong></kbd> 是一个强大的脚本管理器，可以从<a href="https://greasyfork.org/zh-CN">greasyfork</a>等脚本网站上下载多种脚本使用，让你的浏览器更强大丝滑。 * <a href="https://github.com/jae-jae/Userscript-Plus">Userscript-Plus</a>是一款与<strong>Tampermonkey</strong>对应的扩展，可以展示当前页面可用的脚本并提供一键安装。 * <kbd><strong>谷歌访问助手</strong></kbd>在Chrome扩展商店中有众多版本，挑一个自己喜欢的（能用的）。仅提供访问谷歌相关的页面，当使用其他代理时要将此扩展禁用，否则会产生冲突。 * <kbd><strong>Video Speed Controller</strong></kbd>可以调整网页视屏播放速度，几乎在任何网页上都有效，即使明面上不支持调速的网站。 * <kbd><strong>Sourcegraph</strong></kbd>让你在Github上更方便的阅览各项目文件。 * <kbd><strong>Screenity</strong></kbd>是一款录屏软件，可监控整个桌面、摄像头或单个窗口程序，无水印，可应急使用。 * <kbd><strong>沙拉查词</strong></kbd>一款翻译扩展，常看英文比较有用。同类的扩展有还有很多其他的。</p>
<h2 id="功能系列">功能系列</h2>
<ul>
<li><kbd><a href="(http://www.internetdownloadmanager.com/)"><strong>IDM</strong></a></kbd> 一款多线程下载工具，速度可观，但某些链接不支持多线程下载，用此会有反作用。<a href="http://www.internetdownloadmanager.com/">官网入口(付费)</a>，另有绿色版不断更新中。</li>
<li><kbd><a href="https://github.com/bannedbook/fanqiang"><strong>科学上网</strong></a></kbd>支持PC、移动端，内置各种代理(网速不咋地，但是免费)，每种代理各地区效率不一样，按顺序多多尝试即可。代理ip不定期更新。</li>
<li><kbd><a href="https://github.com/CopyTranslator/CopyTranslator"><strong>CopyTranslator</strong></a></kbd>自动格式化并翻译(能自动解决复制来的各种乱七八糟的空格换行等)，新版本性能大大不如老版本，希望改善。</li>
<li><kbd><a href="https://iobit-unlocker.en.softonic.com/"><strong>IObit Unlocker</strong></a></kbd> 强制解锁文件程序占用。</li>
<li><kbd><a href="http://www.hostbuf.com/"><strong>FinalShell</strong></a></kbd>比XShell更强大的SSH免费工具。</li>
<li><kbd><a href="https://www.electronjs.org/apps/netron"><strong>NETRON</strong></a></kbd>:网络可视化工具，可从<a href="https://hub.fastgit.org/lutzroeder/netron/releases">github镜像下载</a></li>
<li><kbd><a href="https://www.jam-software.com/treesize_free"><strong>treesize free</strong></a></kbd>:windows磁盘大小快捷查看。</li>
</ul>
<h2 id="娱乐系列">娱乐系列</h2>
<ul>
<li><kbd><a href="https://github.com/AaronFeng753/Waifu2x-Extension-GUI"><strong>Waifu2x</strong></a></kbd>一款基于神经网络的超分辨率工具，可处理图像视频。</li>
<li><kbd><a href="https://github.com/ChrisAnd1998/TaskbarX"><strong>TaskbarX</strong></a></kbd>Windows任务栏美化工具，居中、透明等。</li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>Torch之Resize/Reshape/interpolate</title>
    <url>/2021/03/24/Torch%E4%B9%8BResize-Reshape-interpolate/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<p>在处理数据的时候，经常需要改变数组维度的大小。其中涉及了数组重组、压缩、拉伸等变换方式。官方库提供了诸如<code>resize</code>、<code>reshape</code>等数组维度转换函数，那该如何运用才能实现自己想要的结果呢。此文以python中的Tensor为例来简要分析几种数组变换的区别。</p>
<h1 id="正文">正文</h1>
<p><code>resize</code>、<code>reshape</code>、<code>veiw</code>等是我们在Torch中常用的维度变换函数。</p>
<h2 id="resize_函数">resize_()函数</h2>
<p>此函数直接操作原始张量，即结果返回到原始张量中，按照<code>tensor.resize_()</code>调用即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">运行：</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(a)</span><br><span class="line">a.resize_((<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<p>以上是转换的输入输出总尺寸（变量中存放的总的数据量相同，可视为所有维度总乘积相等）相同的情况，接下来尝试不同时的情况。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">运行：</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(a)</span><br><span class="line">a.resize_((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(a)</span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">运行：</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(a)</span><br><span class="line">a.resize_((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(a)</span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[                <span class="number">1</span>,                 <span class="number">2</span>,                 <span class="number">3</span>],</span><br><span class="line">        [                <span class="number">4</span>,                 <span class="number">5</span>,                 <span class="number">6</span>],</span><br><span class="line">        [<span class="number">32651492442964069</span>, <span class="number">29273822787141743</span>, <span class="number">27303575259512924</span>]])</span><br></pre></td></tr></table></figure> 由以上两个例子可以看出<code>resize</code>采用多减少补的原则，当输出总尺寸小于原总尺寸时，将自动舍弃索引值较大的数据，使用索引值靠前的数据来组成新的数组；当输出总尺寸大于原尺寸时，则自动补充随机的数据。</p>
<h2 id="reshape函数">reshape函数</h2>
<p>对于输入输出总尺寸相同的情况，<code>reshape</code>函数的结果与<code>resize</code>函数的结果相同。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">运行：</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(a)</span><br><span class="line">a = a.reshape((<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">print(a)</span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure> 当总尺寸不相同的时候，<code>reshape</code>函数则无法进行计算，即<code>reshape</code>函数没有裁剪和自动填充功能，只能够进行简单的维度重组。</p>
<h2 id="view函数">view函数</h2>
<p><code>view</code>的功能与reshape相同，只能进行维度重组，不可进行裁剪和扩充，用法也和<code>reshape</code>相似。 <code>view</code>和<code>reshape</code>都可以将某一个维度设置为<code>-1</code>来实现该维度自适应。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">运行：</span><br><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(a)</span><br><span class="line">a = a.view((-<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">print(a)</span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure></p>
<h2 id="interpolate函数">interpolate函数</h2>
<p><code>intepolate</code>是<code>torch.nn.functional</code>库中的一个插值函数。使用插值的方式来进行数组重组，实现的效果类似于图像的拉伸压缩。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def interpolate(input, size&#x3D;None, scale_factor&#x3D;None, mode&#x3D;&#39;nearest&#39;, align_corners&#x3D;None, recompute_scale_factor&#x3D;None):</span><br><span class="line">    r&quot;&quot;&quot;Down&#x2F;up samples the input to either the given :attr:&#96;size&#96; or the given</span><br><span class="line">        :attr:&#96;scale_factor&#96;</span><br><span class="line"></span><br><span class="line">        The algorithm used for interpolation is determined by :attr:&#96;mode&#96;.</span><br><span class="line"></span><br><span class="line">        Currently temporal, spatial and volumetric sampling are supported, i.e.</span><br><span class="line">        expected inputs are 3-D, 4-D or 5-D in shape.</span><br><span class="line"></span><br><span class="line">        The input dimensions are interpreted in the form:</span><br><span class="line">        &#96;mini-batch x channels x [optional depth] x [optional height] x width&#96;.</span><br><span class="line"></span><br><span class="line">        The modes available for resizing are: &#96;nearest&#96;, &#96;linear&#96; (3D-only),</span><br><span class="line">        &#96;bilinear&#96;, &#96;bicubic&#96; (4D-only), &#96;trilinear&#96; (5D-only), &#96;area&#96;</span><br></pre></td></tr></table></figure> 更据原函数中的描述，此函数仅接受3、4、5维度的数据(深度学习专用函数了)，而且可以选择不同的插值方式，运用灵活。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">运行：</span><br><span class="line">a = torch.tensor([[[[<span class="number">1.0</span>, <span class="number">10.0</span>], [<span class="number">10.0</span>, <span class="number">1.0</span>]]]])</span><br><span class="line">print(a)</span><br><span class="line">a = interpolate(a, size=(<span class="number">5</span>, <span class="number">5</span>), mode=<span class="string">&#x27;bicubic&#x27;</span>,align_corners=<span class="literal">True</span>)</span><br><span class="line">print(a)</span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">1.0000</span>,  <span class="number">3.0391</span>,  <span class="number">5.5000</span>,  <span class="number">7.9609</span>, <span class="number">10.0000</span>],</span><br><span class="line">          [ <span class="number">3.0391</span>,  <span class="number">4.1542</span>,  <span class="number">5.5000</span>,  <span class="number">6.8458</span>,  <span class="number">7.9609</span>],</span><br><span class="line">          [ <span class="number">5.5000</span>,  <span class="number">5.5000</span>,  <span class="number">5.5000</span>,  <span class="number">5.5000</span>,  <span class="number">5.5000</span>],</span><br><span class="line">          [ <span class="number">7.9609</span>,  <span class="number">6.8458</span>,  <span class="number">5.5000</span>,  <span class="number">4.1542</span>,  <span class="number">3.0391</span>],</span><br><span class="line">          [<span class="number">10.0000</span>,  <span class="number">7.9609</span>,  <span class="number">5.5000</span>,  <span class="number">3.0391</span>,  <span class="number">1.0000</span>]]]])</span><br></pre></td></tr></table></figure> # End 以上只是torch中的简单示例，不同语言中有些许差异。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>torch</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer简介</title>
    <url>/2021/11/13/Transformer%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h1 id="自注意力机制">自注意力机制</h1>
<p>现在主流的序列模型都是基于复杂的循环神经网络或者是卷积神经网络构造而来的Encoder-Decoder模型，并且就算是目前性能最好的序列模型也都是基于注意力机制下的Encoder-Decoder架构。为什么作者会不停的提及这些传统的Encoder-Decoder模型呢？接着，作者在介绍部分谈到，由于传统的Encoder-Decoder架构在建模过程中，下一个时刻的计算过程会依赖于上一个时刻的输出，而这种固有的属性就限制了传统的Encoder-Decoder模型就不能以并行的方式进行计算。谷歌2017年所发表的一篇论文，名字叫做”Attention is all you need“，在这篇论文中，作者首次提出了一种全新的Transformer架构来解决这一问题。当然，Transformer架构的优点在于它完全摈弃了传统的循环结构，取而代之的是只通过注意力机制来计算模型输入与输出的隐含表示，而这种注意力的名字就是大名鼎鼎的自注意力机制（self-attention）。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211113Transformer1.png" alt="image-20211113200639554" /><figcaption>image-20211113200639554</figcaption>
</figure>
<p>总体来说，<strong>所谓自注意力机制就是通过某种运算来直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示</strong>。最终，Transformer架构就是基于这种的自注意力机制而构建的Encoder-Decoder模型。</p>
<h2 id="selfattention">SelfAttention</h2>
<p>在论文中作者说道，注意力机制可以描述为将query和一系列的key-value对映射到某个输出的过程，而这个输出的向量就是根据query和key计算得到的权重作用于value上的权重和。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span></p>
<ul>
<li>首先，对于输入序列的每个单词，它都有三个向量编码，分别为：Query、Key、Value。这三个向量是用embedding向量与三个矩阵（ <span class="math inline">\(W^Q,W^k,W^V\)</span>​）相乘得到的结果。这三个矩阵的值在BP的过程中会一直进行更新。</li>
<li>第二步计算Self-Attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是用该词语的Q与句子中其他词语的Key做点乘。以下图为例，假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中重视句子其它部分的程度。</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211113Transformer2.png" alt="image-20211113203629321" /><figcaption>image-20211113203629321</figcaption>
</figure>
<ul>
<li>再对每个分数除以 <span class="math inline">\(\sqrt{d}\)</span>​（d是维度），之后做softmax。</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211113Transformer3.png" alt="image-20211113203726025" /><figcaption>image-20211113203726025</figcaption>
</figure>
<ul>
<li>把每个Value向量和softmax得到的值进行相乘，然后对相乘的值进行相加，得到的结果即是一个词语的self-attention embedding值。</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211113Transformer4.png" alt="image-20211113203814342" /><figcaption>image-20211113203814342</figcaption>
</figure>
<p>这样，自注意力的计算就完成了。得到的向量就可以传递给前馈神经网络。</p>
<h2 id="multiheadattention"><strong>MultiHeadAttention</strong></h2>
<p>上面我们也提到了自注意力机制的缺陷就是：<strong>模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置，</strong> 因此作者提出了通过多头注意力机制来解决这一问题。同时，使用多头注意力机制还能够给予注意力层的输出包含有不同子空间中的编码表示信息，从而增强模型的表达能力。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211113Transformer5.png" alt="image-20211113203940863" /><figcaption>image-20211113203940863</figcaption>
</figure>
<h1 id="位置编码">位置编码</h1>
<h2 id="token-embedding"><strong>Token Embedding</strong></h2>
<p>熟悉文本处理的读者可能都知道，在对文本相关的数据进行建模时首先要做的便是对其进行向量化。例如在机器学习中，常见的文本表示方法有one-hot编码、词袋模型以及TF-IDF等。不过在深度学习中，更常见的做法便是将各个词（或者字）通过一个Embedding层映射到低维稠密的向量空间。因此，在Transformer模型中，首先第一步要做的同样是将文本以这样的方式进行向量化表示，并且将其称之为Token Embedding，也就是深度学习中常说的词嵌入（Word Embedding）。</p>
<p>如果是换做之前的网络模型，例如CNN或者RNN，那么对于文本向量化的步骤就到此结束了，因为这些网络结构本身已经具备了捕捉时序特征的能力，不管是CNN中的n-gram形式还是RNN中的时序形式。但是这对仅仅只有自注意力机制的网络结构来说却不行。为什么呢？根据自注意力机制原理的介绍我们知道，自注意力机制在实际运算过程中不过就是几个矩阵来回相乘进行线性变换而已。因此，这就导致即使是打乱各个词的顺序，那么最终计算得到的结果本质上却没有发生任何变换，换句话说仅仅只使用自注意力机制会丢失文本原有的序列信息。</p>
<h2 id="positional-embedding"><strong>Positional Embedding</strong></h2>
<p>论文中提出PE来对序列的位置信息进行编码：</p>
<p><span class="math inline">\(PE_{pos,2i}=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\)</span>​</p>
<p><span class="math inline">\(PE_{pos,2i+1}=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\)</span>​</p>
<p>Positional Embedding可以弥补自注意力机制不能捕捉序列时序信息的缺陷。</p>
<h1 id="transformer网络结构">Transformer网络结构</h1>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211113Transformer1.png" alt="image-20211113200639554" /><figcaption>image-20211113200639554</figcaption>
</figure>
<h2 id="encoder">Encoder</h2>
<p>Encoder部分来说其内部主要由两部分网络所构成：多头注意力机制和两层前馈神经网络。同时，对于这两部分网络来说，都加入了残差连接，并且在残差连接后还进行了层归一化操作。对于第2部分的两层全连接网络来说，其具体计算过程为:</p>
<p><span class="math inline">\(FFN(x)=max(0,xW_1+b_1)W_2+b_2\)</span></p>
<p>即对于第1层网络的输出还运用了Relu激活函数。</p>
<h2 id="decoder"><strong>Decoder</strong></h2>
<p>同Encoder部分一样，论文中也采用了6个完全相同的网络层堆叠而成，不过这里我们依旧只是先看1层时的情况。对于Decoder部分来说，其整体上与Encoder类似，只是多了一个用于与Encoder输出进行交互的多头注意力机制。不同于Encoder部分，在Decoder中一共包含有3个部分的网络结构。最上面的和最下面的部分（暂时忽略Mask）与Encoder相同，只是多了中间这个与Encoder输出（Memory）进行交互的部分，作者称之为“Encoder-Decoder attention”。对于这部分的输入，<strong>Q来自于下面多头注意力机制的输出，K和V均是Encoder部分的输出（Memory）经过线性变换后得到</strong>。而作者之所以这样设计也是在模仿传统Encoder-Decoder网络模型的解码过程。</p>
<p>Transformer的Encoder-Decoder attention中，<strong>K和V均是编码部分的输出Memory经过线性变换后的结果</strong>（此时的Memory中包含了原始输入序列每个位置的编码信息），<strong>而Q是解码部分多头注意力机制输出的隐含向量经过线性变换后的结果</strong>。在Decoder对每一个时刻进行解码时，首先需要做的便是通过Q与 K进行交互（query查询），并计算得到注意力权重矩阵；然后再通过注意力权重与V进行计算得到一个权重向量，该权重向量所表示的含义就是在解码时如何将注意力分配到Memory的各个位置上。</p>
<h1 id="示例">示例</h1>
<h2 id="multiheadattention-1"><strong>MultiHeadAttention</strong></h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMultiheadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, num_heads, dropout=<span class="number">0.</span>, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyMultiheadAttention, self).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param embed_dim:   词嵌入的维度，也就是前面的d_model参数，论文中的默认值为512</span></span><br><span class="line"><span class="string">        :param num_heads:   多头注意力机制中多头的数量，也就是前面的nhead参数， 论文默认值为 8</span></span><br><span class="line"><span class="string">        :param bias:        最后对多头的注意力（组合）输出进行线性变换时，是否使用偏置</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.embed_dim = embed_dim  <span class="comment"># 前面的d_model参数</span></span><br><span class="line">        self.head_dim = embed_dim // num_heads  <span class="comment"># head_dim 指的就是d_k,d_v</span></span><br><span class="line">        self.kdim = self.head_dim</span><br><span class="line">        self.vdim = self.head_dim</span><br><span class="line">        self.num_heads = num_heads  <span class="comment"># 多头个数</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == self.embed_dim, <span class="string">&quot;embed_dim 除以 num_heads必须为整数&quot;</span></span><br><span class="line">        <span class="comment"># 上面的限制条件就是论文中的  d_k = d_v = d_model/n_head 条件</span></span><br><span class="line">        self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))  </span><br><span class="line">        <span class="comment"># embed_dim = kdim * num_heads</span></span><br><span class="line">        <span class="comment"># 这里第二个维度之所以是embed_dim，实际上这里是同时初始化了num_heads个W_q堆叠起来的, 也就是num_heads个头</span></span><br><span class="line">        self.k_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))  </span><br><span class="line">        <span class="comment"># W_k,  embed_dim = kdim * num_heads</span></span><br><span class="line">        self.v_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))  </span><br><span class="line">        <span class="comment"># W_v,  embed_dim = vdim * num_heads</span></span><br><span class="line">        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line">        <span class="comment"># 最后将所有的Z组合起来的时候，也是一次性完成， embed_dim = vdim * num_heads</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_head_attention_forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    query,  <span class="comment"># [tgt_len,batch_size, embed_dim]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    key,  <span class="comment"># [src_len, batch_size, embed_dim]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    value,  <span class="comment"># [src_len, batch_size, embed_dim]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    num_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">    dropout_p,</span></span></span><br><span class="line"><span class="function"><span class="params">    out_proj_weight, <span class="comment"># [embed_dim = vdim * num_heads, embed_dim]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    out_proj_bias,</span></span></span><br><span class="line"><span class="function"><span class="params">    training=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    key_padding_mask=<span class="literal">None</span>,  <span class="comment"># [batch_size,src_len/tgt_len]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    q_proj_weight=<span class="literal">None</span>,  <span class="comment"># [embed_dim,kdim * num_heads]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    k_proj_weight=<span class="literal">None</span>,  <span class="comment"># [embed_dim, kdim * num_heads]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    v_proj_weight=<span class="literal">None</span>,  <span class="comment"># [embed_dim, vdim * num_heads]</span></span></span></span><br><span class="line"><span class="function"><span class="params">    attn_mask=<span class="literal">None</span>,  <span class="comment"># [tgt_len,src_len] </span></span></span></span><br><span class="line"><span class="function"><span class="params">  </span>):</span></span><br><span class="line">    <span class="comment"># 第一阶段：计算得到Q、K、V</span></span><br><span class="line">    q = F.linear(query, q_proj_weight)</span><br><span class="line">    <span class="comment">#  [tgt_len,batch_size,embed_dim] x [embed_dim,kdim * num_heads] </span></span><br><span class="line">    <span class="comment">#  = [tgt_len,batch_size,kdim * num_heads]</span></span><br><span class="line">    k = F.linear(key, k_proj_weight)</span><br><span class="line">    <span class="comment"># [src_len, batch_size,embed_dim] x [embed_dim,kdim * num_heads]</span></span><br><span class="line">    <span class="comment"># = [src_len,batch_size,kdim * num_heads]</span></span><br><span class="line">    v = F.linear(value, v_proj_weight)</span><br><span class="line">    <span class="comment"># [src_len, batch_size,embed_dim] x [embed_dim,vdim * num_heads] </span></span><br><span class="line">    <span class="comment"># = [src_len,batch_size,vdim * num_heads]</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二阶段：缩放，以及attn_mask维度判断</span></span><br><span class="line">    tgt_len, bsz, embed_dim = query.size()  <span class="comment"># [tgt_len,batch_size, embed_dim]</span></span><br><span class="line">    src_len = key.size(<span class="number">0</span>)</span><br><span class="line">    head_dim = embed_dim // num_heads  <span class="comment"># num_heads * head_dim = embed_dim</span></span><br><span class="line">    scaling = <span class="built_in">float</span>(head_dim) ** -<span class="number">0.5</span></span><br><span class="line">    q = q * scaling  <span class="comment"># [query_len,batch_size,kdim * num_heads]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">      <span class="comment"># [tgt_len,src_len] or [num_heads*batch_size,tgt_len, src_len]</span></span><br><span class="line">        <span class="keyword">if</span> attn_mask.dim() == <span class="number">2</span>:</span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">0</span>)  <span class="comment"># [1, tgt_len,src_len] 扩充维度</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">list</span>(attn_mask.size()) != [<span class="number">1</span>, query.size(<span class="number">0</span>), key.size(<span class="number">0</span>)]:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;The size of the 2D attn_mask is not correct.&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> attn_mask.dim() == <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">list</span>(attn_mask.size()) != [bsz * num_heads, query.size(<span class="number">0</span>), key.size(<span class="number">0</span>)]:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;The size of the 3D attn_mask is not correct.&#x27;</span>)</span><br><span class="line">        <span class="comment"># 现在 atten_mask 的维度就变成了3D</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 第三阶段：计算得到注意力权重矩阵</span></span><br><span class="line">    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># [batch_size * num_heads,tgt_len,kdim]</span></span><br><span class="line">    <span class="comment"># 因为前面是num_heads个头一起参与的计算，所以这里要进行一下变形，以便于后面计算。且同时交换了0，1两个维度</span></span><br><span class="line">    k = k.contiguous().view(-<span class="number">1</span>, bsz*num_heads, head_dim).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#[batch_size * num_heads,src_len,kdim]</span></span><br><span class="line">    v = v.contiguous().view(-<span class="number">1</span>, bsz*num_heads, head_dim).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#[batch_size * num_heads,src_len,vdim]</span></span><br><span class="line">    attn_output_weights = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># [batch_size * num_heads,tgt_len,kdim] x [batch_size * num_heads, kdim, src_len]</span></span><br><span class="line">    <span class="comment"># =  [batch_size * num_heads, tgt_len, src_len]  这就num_heads个QK相乘后的注意力矩阵</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第四阶段：进行相关掩码操作</span></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attn_output_weights += attn_mask  <span class="comment"># [batch_size * num_heads, tgt_len, src_len]</span></span><br><span class="line">    <span class="keyword">if</span> key_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)</span><br><span class="line">        <span class="comment"># 变成 [batch_size, num_heads, tgt_len, src_len]的形状</span></span><br><span class="line">        attn_output_weights = attn_output_weights.masked_fill(</span><br><span class="line">            key_padding_mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>), <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)) </span><br><span class="line">         <span class="comment"># 扩展维度，从[batch_size,src_len]变成[batch_size,1,1,src_len]</span></span><br><span class="line">        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len,src_len)  </span><br><span class="line">        <span class="comment"># [batch_size * num_heads, tgt_len, src_len]</span></span><br><span class="line">     attn_output_weights = F.softmax(attn_output_weights, dim=-<span class="number">1</span>)</span><br><span class="line">   <span class="comment"># [batch_size * num_heads, tgt_len, src_len]</span></span><br><span class="line">    attn_output_weights = F.dropout(attn_output_weights, p=dropout_p, training=training)</span><br><span class="line">    attn_output = torch.bmm(attn_output_weights, v)</span><br><span class="line">    <span class="comment"># Z = [batch_size * num_heads, tgt_len, src_len]  x  [batch_size * num_heads,src_len,vdim]</span></span><br><span class="line">    <span class="comment"># = # [batch_size * num_heads,tgt_len,vdim]</span></span><br><span class="line">    <span class="comment"># 这就num_heads个Attention(Q,K,V)结果</span></span><br><span class="line"></span><br><span class="line">    attn_output = attn_output.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous().view(tgt_len, bsz, embed_dim)</span><br><span class="line">    <span class="comment"># 先transpose成 [tgt_len, batch_size* num_heads ,kdim]</span></span><br><span class="line">    <span class="comment"># 再view成 [tgt_len,batch_size,num_heads*kdim]</span></span><br><span class="line">    attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)</span><br><span class="line"></span><br><span class="line">    Z = F.linear(attn_output, out_proj_weight, out_proj_bias)</span><br><span class="line">    <span class="comment"># 这里就是多个z  线性组合成Z  [tgt_len,batch_size,embed_dim]</span></span><br><span class="line">    <span class="keyword">return</span> Z, attn_output_weights.<span class="built_in">sum</span>(dim=<span class="number">1</span>) / num_heads  <span class="comment"># 将num_heads个注意力权重矩阵按对应维度取平均</span></span><br></pre></td></tr></table></figure>
<h1 id="transformer的特点">Transformer的特点</h1>
<ul>
<li>训练时可并行计算</li>
<li>Transformer的特征抽取能力也比RNN系列的模型要好，使用了self-attention和多头机制来让源序列和目标序列自身的embedding表示所蕴含的信息更加丰富。</li>
</ul>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Useful Websites</title>
    <url>/2021/07/15/Useful-Websites/</url>
    <content><![CDATA[<h1 id="section">~~~~~~</h1>
<p><a href="https://archive.ics.uci.edu/ml/index.php" class="uri">https://archive.ics.uci.edu/ml/index.php</a>:机器学习数据集大全</p>
<p><a href="https://archive-beta.ics.uci.edu/" class="uri">https://archive-beta.ics.uci.edu/</a>:机器学习数据集大全(beta版)</p>
<p><a href="http://pythontutor.com/index.html">PythonTutor首页</a>:内存查看</p>
<p><a href="http://pythontutor.com/visualize.html#mode=edit">PythonTutor edit</a>:编辑器</p>
<p><a href="https://www.programcreek.com/python/" class="uri">https://www.programcreek.com/python/</a>:python各种库函数使用查询(样例、项目)</p>
<p><a href="https://colab.research.google.com/">Google Colaborator</a>:免费GPU</p>
<h1 id="资源">资源</h1>
<p><a href="https://www.aigei.com/ui/web/" class="uri">https://www.aigei.com/ui/web/</a>:免费HTML模板资源</p>
<p><a href="https://www.php.cn/" class="uri">https://www.php.cn/</a>：php中文网，特效素材等资源</p>
<p><a href="https://snapcraft.io/">snapcraft</a>：Linux的应用商店</p>
<p><a href="https://mirrors.dtops.cc/ISO/MacOS/">mac镜像</a></p>
<p><a href="http://www.lib4dev.in/" class="uri">http://www.lib4dev.in/</a>：源码搜索</p>
<p><a href="https://github.com/dair-ai/ml-visuals" class="uri">https://github.com/dair-ai/ml-visuals</a>:ML绘图资源</p>
<p><a href="https://github.com/rougier/scientific-visualization-book" class="uri">https://github.com/rougier/scientific-visualization-book</a>：matplotlib科学计算可视化教程</p>
<p><a href="https://readpaper.com/" class="uri">https://readpaper.com/</a>：文献阅读helper</p>
<h1 id="工具">工具</h1>
<p><a href="https://github.com/julrog/nn_vis">神经网络可视化工具：3D、彩色、可定制，还能可视化参数重要性</a></p>
<h1 id="tweak主题">Tweak主题</h1>
<p><a href="https://github.com/EliverLara/Ant">Ant</a>------<a href="https://www.opendesktop.org/p/1099856/">other</a></p>
<p><a href="https://github.com/vinceliuice/vimix-gtk-themes">Vim</a>-----<a href="https://www.gnome-look.org/p/1013698/">other</a></p>
<p><a href="https://gitlab.com/LinxGem33/X-Arc-White">X-Arc-Collection</a>-----<a href="https://gitee.com/ostheme/X-Arc-White/">gitee</a>-----<a href="https://www.gnome-look.org/p/1167049/">other</a></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>XShell 连接远程服务器</title>
    <url>/2021/08/11/XShell-%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>新建会话，名称按需填写，主机名称填写要连接的服务器的ip地址，服务器的端口号默认22即可。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210811XShell1.png" alt="image-20210811142220907" /><figcaption>image-20210811142220907</figcaption>
</figure>
<p>用户身份验证中输入远程主机的用户名的密码，连接即可。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210811XShell2.png" alt="image-20210811142328871" /><figcaption>image-20210811142328871</figcaption>
</figure>
<hr />
<h1 id="其他">其他</h1>
<p><strong>Alt + N：新建会话</strong></p>
<p><strong>Alt + S：简单模式</strong></p>
<p><strong>Alt + R：透明模式</strong></p>
<p><strong>Alt + A：总在最前面</strong></p>
<p><strong>Alt + Enter：全屏</strong></p>
<p><strong>Alt + 1 ：切到第一个会话，2，3，4…类推</strong></p>
<p><strong>Ctrl + Alt + F：新建传输文件</strong></p>
<p><strong>Ctrl + Shift + L：清屏</strong></p>
<p><code>rz</code> 命令可以上传本地文件到服务器。</p>
<p><code>sz - be filename</code>命令可以将服务器文件下载到本地</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>server</category>
      </categories>
      <tags>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV5运用</title>
    <url>/2021/08/26/YOLOV5%E8%BF%90%E7%94%A8/</url>
    <content><![CDATA[<h1 id="源码下载">源码下载</h1>
<p><a href="https://github.com/ultralytics/yolov5">ultralytics团队贡献源码</a>，以及官方<a href="https://docs.ultralytics.com/">帮助文档</a>.</p>
<ul>
<li>下载</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;ultralytics&#x2F;yolov5.git</span><br></pre></td></tr></table></figure>
<ul>
<li>依赖安装</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="检测方式">检测方式</h1>
<p>提供从Pytorch Hub进行检测和本地py文件检测方式。</p>
<ul>
<li>Pytorch Hub检测：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line">model = torch.hub.load(<span class="string">&#x27;ultralytics/yolov5&#x27;</span>, <span class="string">&#x27;yolov5s&#x27;</span>)  <span class="comment"># or yolov5m, yolov5l, yolov5x, custom</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Images</span></span><br><span class="line">img = <span class="string">&#x27;https://ultralytics.com/images/zidane.jpg&#x27;</span>  <span class="comment"># or file, Path, PIL, OpenCV, numpy, list</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">results = model(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Results</span></span><br><span class="line">results.print()  <span class="comment"># or .show(), .save(), .crop(), .pandas(), etc.</span></span><br></pre></td></tr></table></figure>
<ul>
<li>本地检测：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ python detect.py --source 0  # webcam</span><br><span class="line">                            file.jpg  # image </span><br><span class="line">                            file.mp4  # video</span><br><span class="line">                            path&#x2F;  # directory</span><br><span class="line">                            path&#x2F;*.jpg  # glob</span><br><span class="line">                            &#39;https:&#x2F;&#x2F;youtu.be&#x2F;NUsoVlDFqZg&#39;  # YouTube</span><br><span class="line">                            &#39;rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4&#39;  # RTSP, RTMP, HTTP stream</span><br></pre></td></tr></table></figure>
<p>注意调节命令行<strong>argparse</strong>参数。</p>
<hr />
<h1 id="目录结构">目录结构</h1>
<ul>
<li>data/:主要作用为存放各类数据集中的yaml配置文件，<strong>coco128.yaml</strong>示例如下，主要配置其中的path(数据集根目录)、train/val/test(path下的训练集、验证集、测试集，测试集可省略)。根据自己的需要定制nc(目标种类数量)以及names(物体名称)。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># YOLOv5 🚀 by Ultralytics, GPL-3.0 license</span></span><br><span class="line"><span class="comment"># COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017)</span></span><br><span class="line"><span class="comment"># Example usage: python train.py --data coco128.yaml</span></span><br><span class="line"><span class="comment"># parent</span></span><br><span class="line"><span class="comment"># ├── yolov5</span></span><br><span class="line"><span class="comment"># └── datasets</span></span><br><span class="line"><span class="comment">#     └── coco128  ← downloads here</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span></span><br><span class="line"><span class="attr">path:</span> <span class="string">./datasets/coco128</span>  <span class="comment"># dataset root dir</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">images/train2017</span>  <span class="comment"># train images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">val:</span> <span class="string">images/train2017</span>  <span class="comment"># val images (relative to &#x27;path&#x27;) 128 images</span></span><br><span class="line"><span class="attr">test:</span>  <span class="comment"># test images (optional)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Classes</span></span><br><span class="line"><span class="attr">nc:</span> <span class="number">80</span>  <span class="comment"># number of classes</span></span><br><span class="line"><span class="attr">names:</span> [<span class="string">&#x27;person&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;motorcycle&#x27;</span>, <span class="string">&#x27;airplane&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>, <span class="string">&#x27;traffic light&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;fire hydrant&#x27;</span>, <span class="string">&#x27;stop sign&#x27;</span>, <span class="string">&#x27;parking meter&#x27;</span>, <span class="string">&#x27;bench&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;elephant&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>, <span class="string">&#x27;zebra&#x27;</span>, <span class="string">&#x27;giraffe&#x27;</span>, <span class="string">&#x27;backpack&#x27;</span>, <span class="string">&#x27;umbrella&#x27;</span>, <span class="string">&#x27;handbag&#x27;</span>, <span class="string">&#x27;tie&#x27;</span>, <span class="string">&#x27;suitcase&#x27;</span>, <span class="string">&#x27;frisbee&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;skis&#x27;</span>, <span class="string">&#x27;snowboard&#x27;</span>, <span class="string">&#x27;sports ball&#x27;</span>, <span class="string">&#x27;kite&#x27;</span>, <span class="string">&#x27;baseball bat&#x27;</span>, <span class="string">&#x27;baseball glove&#x27;</span>, <span class="string">&#x27;skateboard&#x27;</span>, <span class="string">&#x27;surfboard&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;tennis racket&#x27;</span>, <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;wine glass&#x27;</span>, <span class="string">&#x27;cup&#x27;</span>, <span class="string">&#x27;fork&#x27;</span>, <span class="string">&#x27;knife&#x27;</span>, <span class="string">&#x27;spoon&#x27;</span>, <span class="string">&#x27;bowl&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sandwich&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;broccoli&#x27;</span>, <span class="string">&#x27;carrot&#x27;</span>, <span class="string">&#x27;hot dog&#x27;</span>, <span class="string">&#x27;pizza&#x27;</span>, <span class="string">&#x27;donut&#x27;</span>, <span class="string">&#x27;cake&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;couch&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;bed&#x27;</span>, <span class="string">&#x27;dining table&#x27;</span>, <span class="string">&#x27;toilet&#x27;</span>, <span class="string">&#x27;tv&#x27;</span>, <span class="string">&#x27;laptop&#x27;</span>, <span class="string">&#x27;mouse&#x27;</span>, <span class="string">&#x27;remote&#x27;</span>, <span class="string">&#x27;keyboard&#x27;</span>, <span class="string">&#x27;cell phone&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;microwave&#x27;</span>, <span class="string">&#x27;oven&#x27;</span>, <span class="string">&#x27;toaster&#x27;</span>, <span class="string">&#x27;sink&#x27;</span>, <span class="string">&#x27;refrigerator&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;clock&#x27;</span>, <span class="string">&#x27;vase&#x27;</span>, <span class="string">&#x27;scissors&#x27;</span>, <span class="string">&#x27;teddy bear&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;hair drier&#x27;</span>, <span class="string">&#x27;toothbrush&#x27;</span>]  <span class="comment"># class names</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Download script/URL (optional)</span></span><br><span class="line"><span class="attr">download:</span> <span class="string">https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip</span></span><br></pre></td></tr></table></figure>
<ul>
<li>models/:文件夹中主要存放模型结构配置文件<strong>yolov5s.yaml</strong>如下所示，主要更改其中的nc参数。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># YOLOv5 🚀 by Ultralytics, GPL-3.0 license</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line"><span class="attr">nc:</span> <span class="number">2</span>  <span class="comment"># number of classes</span></span><br><span class="line"><span class="attr">depth_multiple:</span> <span class="number">0.33</span>  <span class="comment"># model depth multiple</span></span><br><span class="line"><span class="attr">width_multiple:</span> <span class="number">0.50</span>  <span class="comment"># layer channel multiple</span></span><br><span class="line"><span class="attr">anchors:</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="number">10</span>,<span class="number">13</span>, <span class="number">16</span>,<span class="number">30</span>, <span class="number">33</span>,<span class="number">23</span>]  <span class="comment"># P3/8</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="number">30</span>,<span class="number">61</span>, <span class="number">62</span>,<span class="number">45</span>, <span class="number">59</span>,<span class="number">119</span>]  <span class="comment"># P4/16</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="number">116</span>,<span class="number">90</span>, <span class="number">156</span>,<span class="number">198</span>, <span class="number">373</span>,<span class="number">326</span>]  <span class="comment"># P5/32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># YOLOv5 backbone</span></span><br><span class="line"><span class="attr">backbone:</span></span><br><span class="line">  <span class="comment"># [from, number, module, args]</span></span><br><span class="line">  [[<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Focus</span>, [<span class="number">64</span>, <span class="number">3</span>]],  <span class="comment"># 0-P1/2</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">128</span>, <span class="number">3</span>, <span class="number">2</span>]],  <span class="comment"># 1-P2/4</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">3</span>, <span class="string">C3</span>, [<span class="number">128</span>]],</span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">256</span>, <span class="number">3</span>, <span class="number">2</span>]],  <span class="comment"># 3-P3/8</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">9</span>, <span class="string">C3</span>, [<span class="number">256</span>]],</span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>]],  <span class="comment"># 5-P4/16</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">9</span>, <span class="string">C3</span>, [<span class="number">512</span>]],</span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">1024</span>, <span class="number">3</span>, <span class="number">2</span>]],  <span class="comment"># 7-P5/32</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">SPP</span>, [<span class="number">1024</span>, [<span class="number">5</span>, <span class="number">9</span>, <span class="number">13</span>]]],</span><br><span class="line">   [<span class="number">-1</span>, <span class="number">3</span>, <span class="string">C3</span>, [<span class="number">1024</span>, <span class="literal">False</span>]],  <span class="comment"># 9</span></span><br><span class="line">  ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># YOLOv5 head</span></span><br><span class="line"><span class="attr">head:</span></span><br><span class="line">  [[<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">nn.Upsample</span>, [<span class="string">None</span>, <span class="number">2</span>, <span class="string">&#x27;nearest&#x27;</span>]],</span><br><span class="line">   [[<span class="number">-1</span>, <span class="number">6</span>], <span class="number">1</span>, <span class="string">Concat</span>, [<span class="number">1</span>]],  <span class="comment"># cat backbone P4</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">3</span>, <span class="string">C3</span>, [<span class="number">512</span>, <span class="literal">False</span>]],  <span class="comment"># 13</span></span><br><span class="line"></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">nn.Upsample</span>, [<span class="string">None</span>, <span class="number">2</span>, <span class="string">&#x27;nearest&#x27;</span>]],</span><br><span class="line">   [[<span class="number">-1</span>, <span class="number">4</span>], <span class="number">1</span>, <span class="string">Concat</span>, [<span class="number">1</span>]],  <span class="comment"># cat backbone P3</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">3</span>, <span class="string">C3</span>, [<span class="number">256</span>, <span class="literal">False</span>]],  <span class="comment"># 17 (P3/8-small)</span></span><br><span class="line"></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">256</span>, <span class="number">3</span>, <span class="number">2</span>]],</span><br><span class="line">   [[<span class="number">-1</span>, <span class="number">14</span>], <span class="number">1</span>, <span class="string">Concat</span>, [<span class="number">1</span>]],  <span class="comment"># cat head P4</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">3</span>, <span class="string">C3</span>, [<span class="number">512</span>, <span class="literal">False</span>]],  <span class="comment"># 20 (P4/16-medium)</span></span><br><span class="line"></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">1</span>, <span class="string">Conv</span>, [<span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>]],</span><br><span class="line">   [[<span class="number">-1</span>, <span class="number">10</span>], <span class="number">1</span>, <span class="string">Concat</span>, [<span class="number">1</span>]],  <span class="comment"># cat head P5</span></span><br><span class="line">   [<span class="number">-1</span>, <span class="number">3</span>, <span class="string">C3</span>, [<span class="number">1024</span>, <span class="literal">False</span>]],  <span class="comment"># 23 (P5/32-large)</span></span><br><span class="line"></span><br><span class="line">   [[<span class="number">17</span>, <span class="number">20</span>, <span class="number">23</span>], <span class="number">1</span>, <span class="string">Detect</span>, [<span class="string">nc</span>, <span class="string">anchors</span>]],  <span class="comment"># Detect(P3, P4, P5)</span></span><br><span class="line">  ]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>utils/：主要的函数源码</li>
</ul>
<h2 id="其他">其他</h2>
<p>运行train.py或者detect.py后将会生成runs文件夹，其中存放训练的权重、过程参数以及检测的结果。以训练次数递增。</p>
<hr />
<h1 id="自建数据集">自建数据集</h1>
<p>数据集结构：</p>
<ul>
<li>myDatasets</li>
<li>annotations</li>
<li>images
<ul>
<li>xxxx.jpg</li>
<li>......</li>
</ul></li>
<li>labels
<ul>
<li>xxxx.txt</li>
<li>......</li>
</ul></li>
</ul>
<p>训练集和测试集隔离：</p>
<ul>
<li>myDatasets
<ul>
<li>annotations</li>
<li>images
<ul>
<li>train
<ul>
<li>xxxx.jpg</li>
<li>......</li>
</ul></li>
<li>val
<ul>
<li>xxxx.jpg</li>
<li>......</li>
</ul></li>
</ul></li>
<li>labels
<ul>
<li>train
<ul>
<li>xxxx.txt</li>
<li>......</li>
</ul></li>
<li>val
<ul>
<li>xxxx.jpg</li>
<li>......</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>annotations中存放源图像的xml标注格式。使用如下脚本将xml格式转换为txt格式，并将结果存放在labels文件夹中:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">xml2txt.py</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">classes = [<span class="string">&quot;pipe&quot;</span>, <span class="string">&quot;bad&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span>(<span class="params">size, box</span>):</span></span><br><span class="line">    dw = <span class="number">1.</span> / (size[<span class="number">0</span>])</span><br><span class="line">    dh = <span class="number">1.</span> / (size[<span class="number">1</span>])</span><br><span class="line">    x = (box[<span class="number">0</span>] + box[<span class="number">1</span>]) / <span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    y = (box[<span class="number">2</span>] + box[<span class="number">3</span>]) / <span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    w = box[<span class="number">1</span>] - box[<span class="number">0</span>]</span><br><span class="line">    h = box[<span class="number">3</span>] - box[<span class="number">2</span>]</span><br><span class="line">    x = x * dw</span><br><span class="line">    w = w * dw</span><br><span class="line">    y = y * dh</span><br><span class="line">    h = h * dh</span><br><span class="line">    <span class="keyword">return</span> x, y, w, h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_annotation</span>(<span class="params">rootpath, xmlname</span>):</span></span><br><span class="line">    xmlpath = rootpath + <span class="string">&#x27;annotation/&#x27;</span></span><br><span class="line">    xmlfile = os.path.join(xmlpath, xmlname)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(xmlfile, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> in_file:</span><br><span class="line">        txtname = xmlname[:-<span class="number">4</span>] + <span class="string">&#x27;.txt&#x27;</span></span><br><span class="line">        txtpath = rootpath + <span class="string">&#x27;labels/&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(txtpath):</span><br><span class="line">            os.makedirs(txtpath)</span><br><span class="line">        txtfile = os.path.join(txtpath, txtname)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(txtfile, <span class="string">&quot;w+&quot;</span>) <span class="keyword">as</span> out_file:</span><br><span class="line">            tree = ET.parse(in_file)</span><br><span class="line">            root = tree.getroot()</span><br><span class="line">            size = root.find(<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">            w = <span class="built_in">int</span>(size.find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">            h = <span class="built_in">int</span>(size.find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line">            out_file.truncate()</span><br><span class="line">            <span class="keyword">for</span> obj <span class="keyword">in</span> root.<span class="built_in">iter</span>(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">                difficult = obj.find(<span class="string">&#x27;difficult&#x27;</span>).text</span><br><span class="line">                cls = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">                <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> <span class="built_in">int</span>(difficult) == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                cls_id = classes.index(cls)</span><br><span class="line">                xmlbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">                b = (<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmin&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmax&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymin&#x27;</span>).text),</span><br><span class="line">                     <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))</span><br><span class="line">                bb = convert((w, h), b)</span><br><span class="line">                out_file.write(<span class="built_in">str</span>(cls_id) + <span class="string">&quot; &quot;</span> + <span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    rootpath = <span class="string">&#x27;./datasets/pipes/&#x27;</span>  <span class="comment"># 根目录</span></span><br><span class="line">    xmlpath = rootpath + <span class="string">&#x27;annotation/&#x27;</span></span><br><span class="line">    <span class="built_in">list</span> = os.listdir(xmlpath)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="built_in">list</span>)):</span><br><span class="line">        path = os.path.join(xmlpath, <span class="built_in">list</span>[i])</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&#x27;.xml&#x27;</span> <span class="keyword">in</span> path) <span class="keyword">or</span> (<span class="string">&#x27;.XML&#x27;</span> <span class="keyword">in</span> path):</span><br><span class="line">            convert_annotation(rootpath, <span class="built_in">list</span>[i])</span><br><span class="line">            print(<span class="string">&#x27;done&#x27;</span>, i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&#x27;not xml file&#x27;</span>, i)</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="训练模型">训练模型:</h1>
<p>根据需要自定义参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python train.py --img 640 --batch 16 --epochs 100 --data .&#x2F;data&#x2F;coco128.yaml --cfg .&#x2F;models&#x2F;yolov5s.yaml --weights yolov5s.pt</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>deepin-wine</title>
    <url>/2021/09/22/deepin-wine/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>Deepin-wine(<a href="https://github.com/wszqkzqk/deepin-wine-ubuntu">站点1</a>，<a href="https://deepin-wine.i-m.dev/">站点2</a>),为linux提供多种国产软件。</p>
<hr />
<h1 id="安装">安装</h1>
<p>克隆仓库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;wszqkzqk&#x2F;deepin-wine-ubuntu.git # 国外</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;gitee.com&#x2F;wszqkzqk&#x2F;deepin-wine-for-ubuntu.git # 国内</span><br></pre></td></tr></table></figure>
<p>运行./install.sh`进行安装。</p>
<ul>
<li>访问镜像仓库寻找软件<code>https://mirrors.aliyun.com/deepin/pool/non-free/d/</code>，下载deb包安装。(推荐)</li>
<li>添加仓库<code>wget -O- https://deepin-wine.i-m.dev/setup.sh | sh</code>，采用<code>sudo apt-get install com.qq.weixin.deepin</code>安装</li>
</ul>
<hr />
<h1 id="卸载">卸载</h1>
<p>卸载与清理按照层次从浅到深可以分为如下四个层级。</p>
<p>如果只是想清除APP账户配置啥的那么请按照<code>1</code>清理；如果你发现程序奔溃之类的，请按照<code>1-2</code>清理；如果需要卸载APP，按照<code>1-2-3</code>清理；如果你想把一切回到最初的起点，执行<code>1-2-3-4</code>清理。</p>
<ol type="1">
<li><p>清理应用运行时目录</p>
<p>例如QQ/TIM会把帐号配置、聊天文件等保存<code>~/Documents/Tencent Files</code>目录下，而微信是<code>~/Documents/WeChat Files</code>，删除这些文件夹以移除帐号配置等数据。</p></li>
<li><p>清理wine容器</p>
<p>deepin-wine应用第一次启动后会在<code>~/.deepinwine/</code>目录下生成一个文件夹（名字各不相同）用于存储wine容器（可以理解我一个“Windows虚拟机”），如果使用出了问题，可以试试删除这个目录下对应的子文件夹。</p></li>
<li><p>卸载软件包</p>
<p>执行<code>sudo apt-get purge --autoremove &lt;包名&gt;</code>命令把你安装过的包给移除。</p></li>
<li><p>移除软件仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rm &#x2F;etc&#x2F;apt&#x2F;preferences.d&#x2F;deepin-wine.i-m.dev.pref \</span><br><span class="line">        &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;deepin-wine.i-m.dev.list \</span><br><span class="line">        &#x2F;etc&#x2F;profile.d&#x2F;deepin-wine.i-m.dev.sh</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>django</title>
    <url>/2022/01/01/django/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<h1 id="操作">操作</h1>
<ul>
<li>安装</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ pip install django</span><br></pre></td></tr></table></figure>
<ul>
<li>创建工程</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ django-admin startproject mysite</span><br></pre></td></tr></table></figure>
<p>这行代码将会在当前目录下创建一个 <code>mysite</code> 目录，大致结构如下：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">mysite/</span><br><span class="line">    manage.py</span><br><span class="line">    mysite/</span><br><span class="line">        __init__.py</span><br><span class="line">        settings.py</span><br><span class="line">        urls.py</span><br><span class="line">        wsgi.py</span><br></pre></td></tr></table></figure>
<p>这些目录和文件的用处是：</p>
<blockquote>
<p>最外层的:file: mysite/ 根目录只是你项目的容器， Django 不关心它的名字，你可以将它重命名为任何你喜欢的名字。 <code>manage.py</code>: 一个让你用各种方式管理 Django 项目的命令行工具。你可以阅读 <a href="https://docs.djangoproject.com/zh-hans/2.1/ref/django-admin/">django-admin and manage.py</a> 获取所有 <code>manage.py</code> 的细节。 里面一层的 <code>mysite/</code> 目录包含你的项目，它是一个纯 Python 包。它的名字就是当你引用它内部任何东西时需要用到的 Python 包名。 (比如 <code>mysite.urls</code>). <code>mysite/settings.py</code>：Django 项目的配置文件。如果你想知道这个文件是如何工作的，请查看 <a href="https://docs.djangoproject.com/zh-hans/2.1/topics/settings/">Django settings</a> 了解细节。 <code>mysite/urls.py</code>：Django 项目的 URL 声明，就像你网站的“目录”。阅读 <a href="https://docs.djangoproject.com/zh-hans/2.1/topics/http/urls/">URL调度器</a> 文档来获取更多关于 URL 的内容。 <code>mysite/wsgi.py</code>：作为你的项目的运行在 WSGI 兼容的Web服务器上的入口。阅读 <a href="https://docs.djangoproject.com/zh-hans/2.1/howto/deployment/wsgi/">如何使用 WSGI 进行部署</a> 了解更多细节。</p>
</blockquote>
<ul>
<li>启动服务</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py runserver</span><br></pre></td></tr></table></figure>
<blockquote>
<p>更换端口 默认情况下，<a href="https://docs.djangoproject.com/zh-hans/2.1/ref/django-admin/#django-admin-runserver"><code>runserver</code></a> 命令会将服务器设置为监听本机内部 IP 的 8000 端口。 如果你想更换服务器的监听端口，请使用命令行参数。举个例子，下面的命令会使服务器监听 8080 端口：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py runserver 8080</span><br></pre></td></tr></table></figure>
<p>如果你想要修改服务器监听的IP，在端口之前输入新的。比如，为了监听所有服务器的公开IP（这你运行 Vagrant 或想要向网络上的其它电脑展示你的成果时很有用），使用： <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py runserver 0:8000</span><br></pre></td></tr></table></figure></p>
</blockquote>
<ul>
<li>创建应用</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py startapp appname</span><br></pre></td></tr></table></figure>
<h1 id="path函数">path函数</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> django.urls <span class="keyword">import</span> path</span><br></pre></td></tr></table></figure>
<p>Django path() 可以接收四个参数，分别是两个必选参数：route、view 和两个可选参数：kwargs、name。</p>
<p>语法格式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">path(route, view, kwargs=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>route: 字符串，表示 URL 规则，与之匹配的 URL 会执行对应的第二个参数 view。</li>
<li>view: 用于执行与正则表达式匹配的 URL 请求。</li>
<li>kwargs: 视图使用的字典类型的参数。</li>
<li>name: 用来反向获取 URL。</li>
</ul>
<p>此外，使用<strong>re_path()</strong>可以实现某些正则表达式的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> django.urls <span class="keyword">import</span> include, re_path</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="数据库操作">数据库操作</h1>
<p>生成迁移文件：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py makemigrations</span><br></pre></td></tr></table></figure>
<p>迁移到数据库:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py migrate</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="跨域问题">跨域问题</h1>
<p>ip、端口、协议其中之一不同即存在跨域问题。本地可以访问，前端axios不可访问，报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Access to XMLHttpRequest at &#39;http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;app0&#x2F;get&#39; from origin &#39;http:&#x2F;&#x2F;localhost:3000&#39; has been blocked by CORS policy: No &#39;Access-Control-Allow-Origin&#39; header is present on the requested resource.</span><br></pre></td></tr></table></figure>
<p>跨域问题可从前端解决也可从后端解决，前端使用代理来处理，后端使用corsheaders中间件。</p>
<p>安装<code>django-cors-headers</code>，settings.py允许跨域设置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置允许主机，&#x27;*&#x27;代表所有主机</span></span><br><span class="line">ALLOWED_HOSTS = [<span class="string">&#x27;*&#x27;</span>, ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 跨域请求--------------------------------------允许</span></span><br><span class="line">CORS_ALLOW_CREDENTIALS = <span class="literal">True</span></span><br><span class="line">CORS_ORIGIN_ALLOW_ALL = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">MIDDLEWARE = [</span><br><span class="line">    <span class="string">&#x27;django.middleware.security.SecurityMiddleware&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.middleware.common.CommonMiddleware&#x27;</span>,</span><br><span class="line">    <span class="comment"># 跨域中间件</span></span><br><span class="line">    <span class="string">&#x27;corsheaders.middleware.CorsMiddleware&#x27;</span>,</span><br><span class="line">    <span class="comment"># &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;,</span></span><br><span class="line">    <span class="string">&#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>让外部机器可以访问：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ python manage.py runserver 0.0.0.0:8000</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>frp内网穿透</title>
    <url>/2021/12/06/frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>frp 是一个专注于内网穿透的高性能的反向代理应用，支持 TCP、UDP、HTTP、HTTPS 等多种协议。</p>
<span id="more"></span>
<p>其可以将内网服务以安全、便捷的方式通过具有公网 IP 节点的中转暴露到公网。内网穿透软件类似的还有<a href="https://www.xiubenwu.top/2021/06/27/NPS%E6%90%AD%E5%BB%BA/">NPS</a>，与NPS实现方式类似，frp拥有服务端的客户端两个部分。客户端在需要穿透的主机上安装，服务端安装在拥有公网IP的云主机上。frp特点如下(以下摘自官网)：</p>
<ol type="1">
<li>frp是一个高性能的反向代理应用，可以帮助您轻松地进行内网穿透，对外网提供服务， 支持tcp, udp, http, https等协议类型，并且web服务支持根据域名进行路由转发。</li>
<li>frp内网穿透主要用于没有公网IP的用户，实现远程桌面、远程控制路由器、 搭建的WEB、FTP、SMB服务器被外网访问、远程查看摄像头、调试一些远程的API（比如微信公众号，企业号的开发）等。</li>
<li>为什么要选择FRP？市面上提供内网穿透服务的公司对免费的用户是有限制的， 本站免费提供无限流量、无限域名绑定、不限制网速、不限制连接数的内网穿透服务。</li>
<li>为什么免费？本人恰好有闲置的VPS服务器， 所以奉献出来给大家免费使用。</li>
</ol>
<p>frp可以部署在自己的云服务器上，对于没有公网IP的用户来说，frp提供免费的云服务器，非常良心。</p>
<p><a href="http://freefrp.wlphp.com/">frp官网</a></p>
<p><a href="https://gofrp.org/docs">使用文档</a></p>
<h1 id="配置">配置</h1>
<h2 id="服务端配置">服务端配置</h2>
<p>服务端的配置文件为<code>frps.ini</code>，压缩包中提供<code>frps_full.ini</code>里面内含所有的基本配置，可自行选用配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">服务端配置说明</span><br><span class="line">[common]</span><br><span class="line">server_addr &#x3D; www.yourdomain.com </span><br><span class="line">#frps服务端地址</span><br><span class="line">server_port &#x3D; 7000</span><br><span class="line">#frps服务端通讯端口，客户端连接到服务端内网穿透传输数据的端口</span><br><span class="line">privilege_token &#x3D; frp888</span><br><span class="line">#特权模式密钥，客户端连接到FRPS服务端的验证密钥</span><br><span class="line">log_file &#x3D; frpc.log</span><br><span class="line">#日志存放路径</span><br><span class="line">log_level &#x3D; info</span><br><span class="line">#日志记录类别,可选：trace, debug, info, warn, error</span><br><span class="line">log_max_days &#x3D; 7</span><br><span class="line">#日志保存天数</span><br><span class="line">login_fail_exit &#x3D; false</span><br><span class="line">#设置为false，frpc连接frps失败后重连，默认为true不重连</span><br><span class="line">protocol &#x3D; kcp</span><br><span class="line">#KCP协议在弱网环境下传输效率提升明显，但是对frps会有一些额外的流量消耗。服务端须先设置kcp_bind_port &#x3D; 7000，www.yourdomain.com服务端已设置支持</span><br><span class="line"></span><br><span class="line">[http_dsm]</span><br><span class="line">#穿透服务名称,不能和其他已建立的相同，使用公共服务器的建议修改成复杂一点的名称，避免与其他人冲突，很多路由器内置frpc的默认服务名称为[web]，很容易很其他人冲突</span><br><span class="line">type &#x3D; http</span><br><span class="line">#穿透协议类型，可选：tcp，udp，http，https，stcp，xtcp，这个设置之前必须自行搞清楚应该是什么</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">#本地监听IP，可以是本机IP，也可以是本地的局域网内某IP，例如你的局域网是互通的，你可以在路由器上安装frpc，然后local_ip填的内网其他机器ip，这样也可以把内网其他机器穿透出去</span><br><span class="line">local_port &#x3D; 5000</span><br><span class="line">#本地监听端口，通常有ssh端口22，远程桌面3389等等</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">#对传输内容进行压缩，可以有效减小 frpc 与 frps 之间的网络流量，加快流量转发速度，但是会额外消耗一些 cpu 资源</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">#将 frpc 与 frps 之间的通信内容加密传输</span><br><span class="line">custom_domains &#x3D; dsm.yourdomain.com</span><br><span class="line">#自定义域名访问穿透服务，一般域名设置了二级域名泛解析以后，这里填*.yourdomain.com即可，*自定义，如果不想用域名或者自行搭建frps没有域名，则穿透协议类型选择tcp，见以下tcp部分详解</span><br><span class="line">#通过app访问的注意，DS file,DS video,DS audio,DS finder里地址栏默认都是5000端口，穿透后地址栏须填写为【穿透域名:80】，DS photo由于本地local_port为80，穿透后也为80的话直接写域名地址即可</span><br><span class="line"></span><br><span class="line">[https_dsm]</span><br><span class="line">type &#x3D; https</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 5001</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">custom_domains &#x3D;  dsm.yourdomain.com</span><br><span class="line">#以上https配置同http，注意开启https（默认5001端口），证书配置在客户端，无证书的注意浏览器访问时添加信任</span><br><span class="line"></span><br><span class="line">[http_transmission]</span><br><span class="line">type &#x3D; http</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 9091</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">custom_domains &#x3D; tr.yourdomain.com</span><br><span class="line">#transmission下载客户端</span><br><span class="line"></span><br><span class="line">[http_rutorrent]</span><br><span class="line">type &#x3D; http</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 80</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">custom_domains &#x3D; rt.yourdomain.com</span><br><span class="line">#rutorrent下载客户端，用Download Station的类似，注意端口</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[http_blog]</span><br><span class="line">type &#x3D; http</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 80</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">custom_domains &#x3D; blog.yourdomain.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[http_plex]</span><br><span class="line">type &#x3D; http</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 32400</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">custom_domains &#x3D; plex.yourdomain.com</span><br><span class="line">#plex视频服务器</span><br><span class="line"></span><br><span class="line">[https_feixun]</span><br><span class="line">privilege_mode &#x3D; true</span><br><span class="line">type &#x3D; http</span><br><span class="line">local_ip &#x3D; 192.168.1.1</span><br><span class="line">#路由器ip</span><br><span class="line">local_port &#x3D; 80</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">authentication_timeout &#x3D; 0</span><br><span class="line">custom_domains &#x3D; feixun.yourdomain.com</span><br><span class="line">#穿透路由器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[tcp_ssh]</span><br><span class="line">#ssh连接</span><br><span class="line">type &#x3D; tcp</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 22</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">remote_port &#x3D; 3463</span><br><span class="line">#远程端口，一般tcp和udp需要设置，不需要设置custom_domain,访问时为【frps服务器地址+远程端口】，没有域名的用这种方式通过【frps服务器地址+远程端口】即可实现访问</span><br><span class="line"></span><br><span class="line">[udp]</span><br><span class="line">type &#x3D; udp</span><br><span class="line">local_ip &#x3D; 192.168.1.2</span><br><span class="line">local_port &#x3D; 53</span><br><span class="line">use_compression &#x3D; true</span><br><span class="line">use_encryption &#x3D; true</span><br><span class="line">remote_port &#x3D; 3453</span><br><span class="line">访问时为【frps服务器地址+远程端口】</span><br></pre></td></tr></table></figure>
<p>服务端配置示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[common]</span><br><span class="line">bind_port &#x3D; 7000 # frp监听端口</span><br><span class="line">dashboard_port &#x3D; 7500 # 监控界面</span><br><span class="line"></span><br><span class="line">dashboard_user &#x3D; admin # 监控界面的登录账号和密码</span><br><span class="line">dashboard_pwd &#x3D; 68652135</span><br><span class="line"></span><br><span class="line">log_file &#x3D; .&#x2F;frps.log</span><br><span class="line">log_max_days &#x3D; 3</span><br></pre></td></tr></table></figure>
<p>配置文件修改好之后使用如下命令启用frps：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;frps -c .&#x2F;frp.ini</span><br></pre></td></tr></table></figure>
<p>此方式将使控制台不能操作，断开shell连接后会终止运行，因此需要将frps在后台挂起运行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup .&#x2F;frps -c .&#x2F;frps.ini &amp;</span><br></pre></td></tr></table></figure>
<h2 id="客户端配置">客户端配置</h2>
<p>客户端的配置文件为<code>frpc.ini</code>，同样的，<code>frpc_full.ini</code>中含有基本的配置说明。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">客户端配置说明</span><br><span class="line">[common]</span><br><span class="line">bind_addr &#x3D; 0.0.0.0</span><br><span class="line">#服务器IP，0.0.0.0为服务器全局所有IP可用，假如你的服务器有多个IP则可以这样做，或者填写为指定其中的一个服务器IP,支持IPV6</span><br><span class="line">bind_port &#x3D; 7000</span><br><span class="line">#通讯端口，用于和客户端内网穿透传输数据的端口，可自定义</span><br><span class="line">bind_udp_port &#x3D; 7001</span><br><span class="line">#UDP通讯端口，用于点对点内网穿透</span><br><span class="line">kcp_bind_port &#x3D; 7000</span><br><span class="line">#用于KCP协议UDP通讯端口，在弱网环境下传输效率提升明显，但是会有一些额外的流量消耗。设置后frpc客户端须设置protocol &#x3D; kcp</span><br><span class="line">vhost_http_port &#x3D; 80</span><br><span class="line">#http监听端口，注意可能和服务器上其他服务用的80冲突，比如centos有些默认有Apache，可自定义</span><br><span class="line">vhost_https_port &#x3D; 443</span><br><span class="line">#https监听端口，可自定义</span><br><span class="line">dashboard_port &#x3D; 7500</span><br><span class="line">#通过浏览器查看 frp 的状态以及代理统计信息展示端口，可自定义</span><br><span class="line">dashboard_user &#x3D; admin</span><br><span class="line">#信息展示面板用户名</span><br><span class="line">dashboard_pwd &#x3D; admin</span><br><span class="line">#信息展示面板密码</span><br><span class="line">log_max_days &#x3D; 7</span><br><span class="line">#最多保存多少天日志</span><br><span class="line">privilege_token &#x3D; frp888</span><br><span class="line">#特权模式认证密钥</span><br><span class="line">privilege_allow_ports &#x3D; 1-65535</span><br><span class="line">#端口白名单，为了防止端口被滥用，可以手动指定允许哪些端口被使用</span><br><span class="line">max_pool_count &#x3D; 100</span><br><span class="line">#每个内网穿透服务限制最大连接池上限，避免大量资源占用，可自定义</span><br><span class="line">authentication_timeout &#x3D; 0</span><br><span class="line">#frpc 所在机器和 frps 所在机器的时间相差不能超过 15 分钟，因为时间戳会被用于加密验证中，防止报文被劫持后被其他人利用,单位为秒，默认值为 900，即 15 分钟。如果修改为 0，则 frps 将不对身份验证报文的时间戳进行超时校验。国外服务器由于时区的不同，时间会相差非常大，这里需要注意同步时间或者设置此值为0</span><br><span class="line">log_file &#x3D; frps.log</span><br><span class="line">log_level &#x3D; info</span><br></pre></td></tr></table></figure>
<p>客户端配置示例(ssh穿透示例)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[common]</span><br><span class="line">server_addr &#x3D; xxx.xxx.xxx.xxx # 服务器公网ip</span><br><span class="line">server_port &#x3D; 7000 # 监听端口</span><br><span class="line"></span><br><span class="line">[ssh]</span><br><span class="line">type &#x3D; tcp</span><br><span class="line">local_ip &#x3D; 127.0.0.1</span><br><span class="line">local_port &#x3D; 22</span><br><span class="line">remote_port &#x3D; 7001</span><br></pre></td></tr></table></figure>
<p>配置文件修改好之后使用如下命令启用frpc：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup .&#x2F;frpc -c frpc.ini &amp;</span><br></pre></td></tr></table></figure>
<h1 id="异常处理">异常处理</h1>
<p>在frpc客户端连接会提示<code>login to server failed: EOF</code>，此时修改配置文件，在common项下添加<code>tls_enable</code>参数为<code>true</code>即可。修改后配置情况：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[common]</span><br><span class="line">tls_enable &#x3D; true</span><br><span class="line">server_addr &#x3D; xxx.xxx.xxx.xxx # 服务器公网ip</span><br><span class="line">server_port &#x3D; 7000 # 监听端口</span><br><span class="line"></span><br><span class="line">[ssh]</span><br><span class="line">type &#x3D; tcp</span><br><span class="line">local_ip &#x3D; 127.0.0.1</span><br><span class="line">local_port &#x3D; 22</span><br><span class="line">remote_port &#x3D; 7001</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Git使用</title>
    <url>/2021/02/05/git%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p><strong>Git常用命令笔记</strong></p>
<span id="more"></span>
<h1 id="git常用命令">git常用命令</h1>
<h2 id="创建库">创建库</h2>
<ul>
<li><strong><em>makedir name</em></strong> 新建目录</li>
<li><strong><em>cd name</em></strong> 进入目录</li>
<li><strong><em>git init</em></strong> 初始化为仓库</li>
<li><strong><em>pwd</em></strong> 当前路径</li>
<li><strong><em>ls</em></strong> 列举当前目录文件
<ul>
<li><strong><em>ls -ah</em></strong> 显示隐藏(.git)</li>
</ul></li>
</ul>
<h2 id="将文件添加进库">将文件添加进库</h2>
<ul>
<li><strong><em>git add filename</em></strong> 将文件添加入仓库</li>
<li><strong><em>git commit -m &quot;commend&quot;</em></strong> 将文件提交至仓库<br />
<em>git add</em> 将<strong>工作区</strong>文件放入<strong>暂缓区</strong>，<em>git commit</em> 将暂缓区文件提交到<strong>分支</strong>。可以添加多个文件后一次性进行提交。</li>
</ul>
<h2 id="版本管理">版本管理</h2>
<ul>
<li><strong><em>git status</em></strong> 查看当前的仓库状态，是否有未更新的改动等。</li>
<li><strong><em>git diff</em></strong> 查看文件的具体改动内容
<ul>
<li><strong><em>git diff HEAD -- filename</em></strong> 查看工作区与最新版本库文件中的区别。</li>
</ul></li>
<li>***git reset --hard HEAD<sup>*** 退回上一版本(HEAD:当前版本；HEAD</sup>上一版本:HEAD^^：前二版本；HEAD~n:前n版本)</li>
<li><strong><em>git log</em></strong> 和 <strong><em>git reflog</em></strong> 查看提交的日志文件， 之后使用 <strong><em>git reset --hard commit_id</em></strong> 跳转到相应的版本。</li>
<li><strong><em>git checkout -- filename</em></strong> 将工作区的修改撤回至暂缓区（若存在）或者版本库，其中的--不可缺少，否则成为切换分支的命令。当添加到暂缓区后想放弃工作区的修改时：<strong><em>git reset HEAD filename</em></strong> 再使用 <strong><em>git checkout -- filename</em></strong>.</li>
<li><strong><em>git rm filename</em></strong> 之后 <strong><em>git commit</em></strong> 删除文件并更新到版本库。</li>
</ul>
<h2 id="远程仓库">远程仓库</h2>
<ul>
<li><strong><em>ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</em></strong> 创建ssh key，加密本地仓库和远程GitHub仓库的传输，无需设置密码。之后<strong>将SSH公匙添加至github仓库Account settings中的SSH keys</strong>.</li>
<li><strong><em>git remote add origin git@github.com:username/repositoryname.git</em></strong> 将本地仓库与远程仓库关联起来。(origin为默认远程仓库名，可修改其他名称。)</li>
<li><strong><em>git remote rm origin</em></strong> 删除Git仓库中的origin信息(关联错误需重新关联时)。</li>
<li><strong><em>git remote -v</em></strong> 查看远程仓库信息。</li>
<li><strong><em>git push -u origin master</em></strong> 本地文件推送至远程仓库，首次使用时需要-u将本地和远程关联，后续命令可简化，以后推送特定文件可直接<strong><em>git push origin master</em></strong>.</li>
<li><strong><em>git clone git@github.com:username/repositoryname.git</em></strong> 从远程仓库克隆到本地。</li>
<li>从本地推送分支，使用 <strong><em>git push origin branch-name</em></strong>，如果推送失败，先用 <strong><em>git pull</em></strong> 抓取远程的新提交。</li>
<li>在本地创建和远程分支对应的分支，使用 <strong><em>git checkout -b branch-name origin/branch-name</em></strong> ，本地和远程分支的名称最好一致。</li>
<li>建立本地分支和远程分支的关联，使用 <strong><em>git branch --set-upstream branch-name origin/branch-name</em></strong> .</li>
</ul>
<h2 id="分支管理">分支管理</h2>
<ul>
<li><strong><em>git checkout -b branchname</em></strong><br />
<strong><em>git switch -c branchname</em></strong> 创建并切换分支，相当于两命令之和：
<ul>
<li><strong><em>git branch branchname</em></strong> 创建分支</li>
<li><strong><em>git checkout branchname</em></strong> 切换分支</li>
<li><strong><em>git switch branchname</em></strong> 新版本使用的更直观的切换分支命令。</li>
</ul></li>
<li><strong><em>git branch</em></strong> 列举所有分支，*前缀标明当前分支。</li>
<li><strong><em>git merge branchname</em></strong> 合并指定分支到当前分支。</li>
<li><strong><em>git branch -d branchname</em></strong> 删除指定分支。
<ul>
<li><strong><em>git branch -D branchname</em></strong> 强行删除未合并的分支。</li>
</ul></li>
<li><strong><em>git stash</em></strong> 隐藏未提交的工作区。</li>
<li><strong><em>git stash list</em></strong> 查看隐藏的工作现场。</li>
<li><strong><em>git stash pop</em></strong> 恢复同时删除隐藏区内容。
<ul>
<li><strong><em>git stash apply</em></strong> 恢复stash.</li>
<li><strong><em>git stash drop</em></strong> 删除stash.</li>
</ul></li>
<li><strong><em>git rebase</em></strong> 整理分支。</li>
</ul>
<h2 id="标签管理">标签管理</h2>
<ul>
<li><strong><em>git tag v1.0</em></strong> 给当前分支打标签。</li>
<li><strong><em>git tag v000 commit_id</em></strong> 给特点版本的commit打标签。</li>
<li><strong><em>git show tagname</em></strong> 查看标签信息。</li>
<li><strong><em>git tag -a tagname -m &quot;blablabla...&quot;</em></strong> 可以指定标签信息。</li>
<li><strong><em>git tag -d tagname</em></strong> 删除标签。</li>
<li><strong><em>git push origin tagname</em></strong> 推送标签到远程。
<ul>
<li><strong><em>git push origin --tags</em></strong> 推送全部标签。</li>
</ul></li>
<li>删除远程仓库标签步骤：
<ul>
<li>先在本地删除：<strong><em>git tag -d tagname</em></strong></li>
<li>再从远程删除：<strong><em>git push origin :refs/tags/tagname</em></strong></li>
</ul></li>
</ul>
<h2 id="其他">其他</h2>
<p><a href="https://www.sourcetreeapp.com/">图形化工具SourceTree官方网站</a></p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>linux安装QQ依赖报错解决</title>
    <url>/2021/10/24/linux%E5%AE%89%E8%A3%85QQ%E4%BE%9D%E8%B5%96%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p>在使用deepwine安装windows下的QQ软件的时候，报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install com.qq.im.deepin</span><br><span class="line">正在读取软件包列表... 完成</span><br><span class="line">正在分析软件包的依赖关系树... 完成</span><br><span class="line">正在读取状态信息... 完成                 </span><br><span class="line">有一些软件包无法被安装。如果您用的是 unstable 发行版，这也许是</span><br><span class="line">因为系统无法达到您要求的状态造成的。该版本中可能会有一些您需要的软件</span><br><span class="line">包尚未被创建或是它们已被从新到(Incoming)目录移出。</span><br><span class="line">下列信息可能会对解决问题有所帮助：</span><br><span class="line"></span><br><span class="line">下列软件包有未满足的依赖关系：</span><br><span class="line"> libgirepository-1.0-1 : 破坏: python-gi (&lt; 3.34.0-4~) 但是 3.30.4-1 正要被安装</span><br><span class="line">E: 无法修正错误，因为您要求某些软件包保持现状，就是它们破坏了软件包间的依赖关系</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211024qq1.png" alt="image-20211024133350448" /><figcaption>image-20211024133350448</figcaption>
</figure>
<h1 id="解决办法">解决办法</h1>
<p><code>libgirepository-1.0-1</code>依赖于<code>libffi7</code>先安装<code>libffi7</code>再安装<code>python-gi</code>，解决所有依赖后再次安装qq:<code>sudo apt install com.qq.im.deepin</code>.</p>
<h2 id="安装libffi7">安装libffi7</h2>
<p>软件包地址:<a href="https://packages.ubuntu.com/zh-cn/focal/libffi7" class="uri">https://packages.ubuntu.com/zh-cn/focal/libffi7</a>，可以在新立得软件包(synaptic)直接搜索安装：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211024qq2.png" alt="image-20211024133908507" /><figcaption>image-20211024133908507</figcaption>
</figure>
<h2 id="安装python-gi">安装python-gi</h2>
<p>到软件包地址<a href="https://packages.ubuntu.com/zh-cn/focal/python-gi" class="uri">https://packages.ubuntu.com/zh-cn/focal/python-gi</a>下载软件包，然后以<code>dpkg</code>命令安装，此时可能会缺少相关依赖，按照<code>apt install -f</code>处理依赖关系继续安装，完成。</p>
<h2 id="完成安装">完成安装</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install com.qq.im.deepin</span><br><span class="line">正在读取软件包列表... 完成</span><br><span class="line">正在分析软件包的依赖关系树... 完成</span><br><span class="line">正在读取状态信息... 完成                 </span><br><span class="line">将会同时安装下列软件：</span><br><span class="line">  deepin-wine-plugin-virtual libffi6 python-dbus python-gobject</span><br><span class="line">  python-gobject-2 python-is-python2</span><br><span class="line">建议安装：</span><br><span class="line">  python-dbus-dbg python-dbus-doc python-gobject-2-dbg</span><br><span class="line">下列【新】软件包将被安装：</span><br><span class="line">  com.qq.im.deepin:i386 deepin-wine-plugin-virtual libffi6 python-dbus</span><br><span class="line">  python-gobject python-gobject-2 python-is-python2</span><br><span class="line">升级了 0 个软件包，新安装了 7 个软件包，要卸载 0 个软件包，有 5 个软件包未被升级。</span><br><span class="line">需要下载 141 MB 的归档。</span><br><span class="line">解压缩后会消耗 161 MB 的额外空间。</span><br><span class="line">您希望继续执行吗？ [Y&#x2F;n] y</span><br><span class="line">获取:1 http:&#x2F;&#x2F;us.archive.ubuntu.com&#x2F;ubuntu hirsute&#x2F;universe amd64 python-is-python2 all 2.7.18-9 [2,976 B]</span><br><span class="line">获取:2 https:&#x2F;&#x2F;deepin-wine.i-m.dev  python-dbus 1.2.8-3 [103 kB]               </span><br><span class="line">获取:3 https:&#x2F;&#x2F;deepin-wine.i-m.dev  libffi6 3.2.1-9 [20.8 kB]</span><br><span class="line">获取:4 https:&#x2F;&#x2F;deepin-wine.i-m.dev  python-gobject-2 2.28.6.1-1+rebuild [281 kB]</span><br><span class="line">获取:5 https:&#x2F;&#x2F;deepin-wine.i-m.dev  python-gobject 3.30.4-1 [20.0 kB]</span><br><span class="line">获取:6 https:&#x2F;&#x2F;deepin-wine.i-m.dev  deepin-wine-plugin-virtual 5.1.13-1 [2,144 B]</span><br><span class="line">获取:7 https:&#x2F;&#x2F;deepin-wine.i-m.dev  com.qq.im.deepin 9.3.2deepin20 [141 MB]</span><br><span class="line">已下载 141 MB，耗时 29秒 (4,814 kB&#x2F;s)                                          </span><br><span class="line">正在选中未选择的软件包 python-is-python2。</span><br><span class="line">(正在读取数据库 ... 系统当前共安装有 223915 个文件和目录。)</span><br><span class="line">准备解压 ...&#x2F;0-python-is-python2_2.7.18-9_all.deb  ...</span><br><span class="line">正在解压 python-is-python2 (2.7.18-9) ...</span><br><span class="line">正在选中未选择的软件包 python-dbus。</span><br><span class="line">准备解压 ...&#x2F;1-python-dbus_1.2.8-3_amd64.deb  ...</span><br><span class="line">正在解压 python-dbus (1.2.8-3) ...</span><br><span class="line">正在选中未选择的软件包 libffi6:amd64。</span><br><span class="line">准备解压 ...&#x2F;2-libffi6_3.2.1-9_amd64.deb  ...</span><br><span class="line">正在解压 libffi6:amd64 (3.2.1-9) ...</span><br><span class="line">正在选中未选择的软件包 python-gobject-2。</span><br><span class="line">准备解压 ...&#x2F;3-python-gobject-2_2.28.6.1-1+rebuild_amd64.deb  ...</span><br><span class="line">正在解压 python-gobject-2 (2.28.6.1-1+rebuild) ...</span><br><span class="line">正在选中未选择的软件包 python-gobject。</span><br><span class="line">准备解压 ...&#x2F;4-python-gobject_3.30.4-1_all.deb  ...</span><br><span class="line">正在解压 python-gobject (3.30.4-1) ...</span><br><span class="line">正在选中未选择的软件包 deepin-wine-plugin-virtual。</span><br><span class="line">准备解压 ...&#x2F;5-deepin-wine-plugin-virtual_5.1.13-1_all.deb  ...</span><br><span class="line">正在解压 deepin-wine-plugin-virtual (5.1.13-1) ...</span><br><span class="line">正在选中未选择的软件包 com.qq.im.deepin:i386。</span><br><span class="line">准备解压 ...&#x2F;6-com.qq.im.deepin_9.3.2deepin20_i386.deb  ...</span><br><span class="line">正在解压 com.qq.im.deepin:i386 (9.3.2deepin20) ...</span><br><span class="line">正在设置 libffi6:amd</span><br></pre></td></tr></table></figure>
<p>注销刷新图标。</p>
]]></content>
      <categories>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>lnmp一键部署</title>
    <url>/2021/08/17/lnmp%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<!-- more -->
<h1 id="简介">简介</h1>
<p><strong>LNMP</strong>一键安装包是一个用Linux Shell编写的可以为CentOS/RHEL/Fedora/Aliyun/Amazon、Debian/Ubuntu/Raspbian/Deepin/Mint Linux VPS或独立主机安装<strong>LNMP(Nginx/MySQL/PHP)、LNMPA(Nginx/MySQL/PHP/Apache)、LAMP(Apache/MySQL/PHP)</strong>生产环境的Shell程序。</p>
<p><a href="https://lnmp.org/">相关网站</a></p>
<h1 id="安装">安装</h1>
<p>使用<code>screen -S lnmp</code>建立新的窗口，若<code>screen</code>命令不存在则安装:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum|apt-egt install screen</span><br></pre></td></tr></table></figure>
<p>下载并安装LNMP一键安装包：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;soft.vpser.net&#x2F;lnmp&#x2F;lnmp1.8.tar.gz -cO lnmp1.8.tar.gz &amp;&amp; tar zxf lnmp1.8.tar.gz &amp;&amp; cd lnmp1.8 &amp;&amp; .&#x2F;install.sh lnmp</span><br></pre></td></tr></table></figure>
<p>如需要安装<strong>LNMPA</strong>或LAMP，将./install.sh 后面的参数lnmp替换为lnmpa或lamp即可。同时也支持单独安装Nginx或数据库，命令为 ./install.sh nginx 或 ./install.sh db。如需更改网站和数据库目录、自定义Nginx参数、PHP参数模块、开启lua等需在运行./install.sh 命令前修改安装包目录下的 lnmp.conf 文件</p>
<p>安装开始时需要选择各类版本，注意查看自己系统所支持的版本。</p>
<hr />
<h1 id="端口冲突">端口冲突</h1>
<p>安装完成后会显示运行状态。如果系统中有其它软件占用端口将导致相应程序启动失败。如NPS默认占用了80和443端口，这将导致nginx不能正常启动，除非关闭NPS.因此可以调整NPS的默认端口：</p>
<p>找到NPS的配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@xxxxxxxxxxxxx ~]# find &#x2F; -name nps.conf</span><br><span class="line">&#x2F;etc&#x2F;nps&#x2F;conf&#x2F;nps.conf</span><br><span class="line">&#x2F;home&#x2F;Programes&#x2F;nps&#x2F;conf&#x2F;nps.conf</span><br></pre></td></tr></table></figure>
<p>打开<code>/etc/nps/conf/nps.conf</code>找到其中的端口配置，将80和443修改为81、444(或其他)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#HTTP(S) proxy port, no startup if empty</span><br><span class="line">http_proxy_ip&#x3D;0.0.0.0</span><br><span class="line">http_proxy_port&#x3D;81  # 80</span><br><span class="line">https_proxy_port&#x3D;444 # 443</span><br><span class="line">https_just_proxy&#x3D;true</span><br></pre></td></tr></table></figure>
<p>重启nps完成修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nps restart</span><br></pre></td></tr></table></figure>
<p>这样访问ip地址调用80端口就可以直接出现lnmp的界面了。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210817lnmp1.png" alt="image-20210817130530024" /><figcaption>image-20210817130530024</figcaption>
</figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>server</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Manjaro</title>
    <url>/2021/12/23/manjaro/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>Manjaro Linux是快速的、用户友好的、面向桌面的、基于Arch Linux的操作系统。它的一些显著特性包括：一份直观的安装程序、自动硬件检测、稳定的滚动式发布模式、对安装多个内核的支持、用于管理图形卡的特别Bash脚本、高度的桌面可配置性。Manjaro Linux提供Xfce桌面作为核心选项，并为高级用户提供一份最小主义的Net版本。用户还可以获得社区支持的GNOME 3/Cinnamon及KDE版本。Manjaro的社区论坛可提供帮助并充满活力，用户受益其中。</p>
<ul>
<li>桌面环境：GNOME, KDE Plasma, Xfce</li>
<li>软件包管理：Flatpak, Pacman, snap</li>
<li>发布模式：Rolling</li>
</ul>
<hr />
<h1 id="init">Init</h1>
<p>Manjaro 装好后，需要运行的第一条命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo pacman -Syy ## 强制更新 package 目录</span><br><span class="line">sudo pacman-mirrors --interactive --country China  # 列出所有国内的镜像源，并提供交互式的界面手动选择镜像源</span><br><span class="line">sudo pacman -Syyu  # 强制更新 package 目录，并尝试更新已安装的所有 packages.</span><br><span class="line">sudo pacman -S yay  # 安装 yay</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo pacman -S base-devel </span><br><span class="line"><span class="comment"># 安装 fakeroot、binutils 等打包基本工具否则安装软件时报错：</span></span><br><span class="line"><span class="comment"># Cannot find the fakeroot binary. ==&gt; 错误： Cannot find the strip binary required for object fil...</span></span><br><span class="line"><span class="comment"># 错误： Cannot find the fakeroot binary. ==&gt; 错误： Cannot find the strip binary required for object file stripping. ==&gt; 错误：Makepkg 无法构建 deepin-wine-wechat.</span></span><br></pre></td></tr></table></figure>
<p>pacman 是 arch/manjaro 的官方包管理器，而刚刚安装的 yay，则是一个能查询 arch linux 的 aur 仓库的第三方包管理器，非常流行。</p>
<p>pacman 的常用命令语法：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pacman -S package_name        <span class="comment"># 安装软件  </span></span><br><span class="line">pacman -S extra/package_name  <span class="comment"># 安装不同仓库中的版本</span></span><br><span class="line">pacman -Syu                   <span class="comment"># 升级整个系统，y是更新数据库，yy是强制更新，u是升级软件</span></span><br><span class="line">pacman -Ss string             <span class="comment"># 在包数据库中查询软件</span></span><br><span class="line">pacman -Si package_name       <span class="comment"># 显示软件的详细信息</span></span><br><span class="line">pacman -Sc                    <span class="comment"># 清除软件缓存，即/var/cache/pacman/pkg目录下的文件</span></span><br><span class="line">pacman -R package_name        <span class="comment"># 删除单个软件</span></span><br><span class="line">pacman -Rs package_name       <span class="comment"># 删除指定软件及其没有被其他已安装软件使用的依赖关系</span></span><br><span class="line">pacman -Qs string             <span class="comment"># 查询已安装的软件包</span></span><br><span class="line">pacman -Qi package_name       <span class="comment"># 查询本地安装包的详细信息</span></span><br><span class="line">pacman -Ql package_name       <span class="comment"># 获取已安装软件所包含的文件的列表</span></span><br><span class="line">pacman -U package.tar.zx      <span class="comment"># 从本地文件安装</span></span><br><span class="line">pactree package_name          <span class="comment"># 显示软件的依赖树</span></span><br></pre></td></tr></table></figure>
<p>yay 的用法和 pacman 完全类似，上述所有 <code>pacman xxx</code> 命令，均可替换成 <code>yay xxx</code> 执行。</p>
<p>此外，还有一条 <code>yay</code> 命令值得记一下：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">yay -c  <span class="comment"># 卸载所有无用的依赖。类比 apt-get autoremove</span></span><br></pre></td></tr></table></figure>
<h2 id="安装deb包">安装deb包</h2>
<p>pacman系的不能直接安装debian系的软件包，需要进行一定转化，安装<kbd>debtap</kbd>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">yay -S debtap</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">yaourt -S debtap</span><br><span class="line"><span class="comment"># or </span></span><br><span class="line">pacaur -S debtap</span><br><span class="line"><span class="comment"># 三种方式选一,但必须先安装三种工具:</span></span><br><span class="line"> sudo pacman -S XXX  <span class="comment"># XXX 可为 yaourt、yay、pacaur </span></span><br></pre></td></tr></table></figure>
<p>更新 debtap</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo debtap -u</span><br></pre></td></tr></table></figure>
<p><strong>deb</strong> 包转为 <strong>tar.xz</strong> 包，在此过程中，会要求输入包装名（packager name）和许可证（License）（可输入比如 GPL 或者不输入）。对生成的 tar.xz 包，使用 pacman 进行安装。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo debtap XXX.deb</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo pacman -U XXX.tar.xz</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="常用软件与配置">常用软件与配置</h2>
<h3 id="添加-archlinux-中文社区仓库">1. 添加 archlinux 中文社区仓库</h3>
<p><a href="https://www.archlinuxcn.org/archlinux-cn-repo-and-mirror/">Arch Linux 中文社区仓库</a> 是由 Arch Linux 中文社区驱动的非官方用户仓库，包含一些额外的软件包以及已有软件的 git 版本等变种。部分软件包的打包脚本来源于 AUR。</p>
<p>一些国内软件，如果直接从 aur 安装，那就会有一个编译过程，有点慢。而 archlinuxcn 有已经编译好的包，可以直接安装。更新速度也很快，推荐使用。</p>
<p>配置方法见 <a href="https://github.com/archlinuxcn/repo">Arch Linux Chinese Community Repository</a>。</p>
<h3 id="安装常用软件">2. 安装常用软件</h3>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo pacman -S google-chrome  firefox         <span class="comment"># 浏览器</span></span><br><span class="line">sudo pacman -S netease-cloud-music     <span class="comment"># 网易云音乐</span></span><br><span class="line">sudo pacman -S noto-fonts-cjk wqy-bitmapfont wqy-microhei wqy-zenhei   <span class="comment"># 中文字体：思源系列、文泉系列</span></span><br><span class="line">sudo pacman -S wps-office ttf-wps-fonts</span><br><span class="line"></span><br><span class="line">sudo pacman -S vim                     <span class="comment"># 命令行编辑器</span></span><br><span class="line">sudo pacman -S git                     <span class="comment"># 版本管理工具</span></span><br><span class="line">sudo pacman -S clang make cmake gdb    <span class="comment"># 编译调试环境</span></span><br><span class="line">sudo pacman -S visual-studio-code-bin  <span class="comment"># 代码编辑器</span></span><br><span class="line"></span><br><span class="line">sudo pacman -S wireshark-qt  mitmproxy         <span class="comment"># 抓包工具</span></span><br><span class="line">sudo pacman -S docker  <span class="comment"># docker 容器</span></span><br></pre></td></tr></table></figure>
<h2 id="输入法安装">输入法安装</h2>
<p>在<kbd>Manjaro Hello</kbd>中的<kbd>Applications</kbd>中安装fcitx，可以避免手动配置.xprofile</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211223majaro1.png" alt="Manjaro Applications" /><figcaption>Manjaro Applications</figcaption>
</figure>
<p>安装搜狗输入法</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">yay -S fcitx-sogoupinyin</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">sudo pacman -S fcitx-sogoupinyin</span><br></pre></td></tr></table></figure>
<p>搜狗经常报错，安装google输入法亦可：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">yay -S fcitx-googlepinyin</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>pip镜像源</title>
    <url>/2021/08/21/pip%E9%95%9C%E5%83%8F%E6%BA%90/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>记录pip安装是镜像源配置文件位置，配置方式等。</p>
<hr />
<h1 id="配置方式">配置方式</h1>
<h2 id="windows">Windows</h2>
<p>通常用-i参数可以暂时指定安装源，全局配置为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip config set global.index-url http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;</span><br><span class="line">&gt;&gt;&gt;Writing to C:\Users\name\AppData\Roaming\pip\pip.ini</span><br></pre></td></tr></table></figure>
<p>查看当前的镜像源配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip config list</span><br></pre></td></tr></table></figure>
<p>在pip.ini中配置而指定多个镜像源：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url &#x3D; http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;</span><br><span class="line">extra-index-url&#x3D;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	http:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.python.org&#x2F;simple</span><br></pre></td></tr></table></figure>
<p>安装时给予镜像源信任(不是必要的):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url &#x3D; http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;</span><br><span class="line">extra-index-url&#x3D;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	http:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.python.org&#x2F;simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host&#x3D;mirrors.aliyun.com</span><br><span class="line">	pypi.tuna.tsinghua.edu.cn</span><br><span class="line">	pypi.mirrors.ustc.edu.cn</span><br><span class="line">	pypi.mirrors.ustc.edu.cn</span><br><span class="line">	pypi.douban.com</span><br></pre></td></tr></table></figure>
<h2 id="linux">Linux</h2>
<p>配置文件位于：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~&#x2F;.pip&#x2F;pip.conf</span><br><span class="line"></span><br><span class="line"> vim ~&#x2F;.pip&#x2F;pip.conf 可对其修改</span><br></pre></td></tr></table></figure>
<p>格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;</span><br><span class="line">extra-index-url&#x3D;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.mirrors.ustc.edu.cn&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;</span><br><span class="line">	https:&#x2F;&#x2F;pypi.python.org&#x2F;simple&#x2F;</span><br><span class="line">[install]</span><br><span class="line">trusted-host&#x3D;mirrors.aliyun.com</span><br><span class="line">	pypi.tuna.tsinghua.edu.cn</span><br><span class="line">	pypi.mirrors.ustc.edu.cn</span><br><span class="line">	pypi.mirrors.ustc.edu.cn</span><br><span class="line">	pypi.douban.com</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python函数调用关系分析</title>
    <url>/2021/12/13/python%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>在Pycharm的专业版中，提供了一个分析项目性能的工具<code>run-&gt;Profile</code>，可以将工程运行时各函数的调用次数和所用时间记录下来，提供静态数据和关系图表两种表达方式。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211213pyviz1.png" /></p>
<p>只需轻轻一点，统计数据和图表关系一览无遗，确实方便。但是此方法局限性很大，一是必须使用pycharm，这样使用其他ide如VS Code的用户就没法使用了；二是它不仅要使用pychram，还得是专业版，专业版是要收费的(破解和学生版除外)，这又是一道坎挡在了我们的路上。那么有其他的方法来进行类似的分析吗？当然是有的。</p>
<hr />
<h1 id="profile">profile</h1>
<p>profile可以分析函数运行时的性能消耗情况，调用方法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> profile</span><br><span class="line">profile.run(<span class="string">&quot;main()&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>main()即为需要分析的入口函数，以字符串形式传入。运行之后可以得到如下的输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">19 function calls in 0.000 seconds</span><br><span class="line"></span><br><span class="line">   Ordered by: standard name</span><br><span class="line"></span><br><span class="line">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 :0(exec)</span><br><span class="line">        4    0.000    0.000    0.000    0.000 :0(len)</span><br><span class="line">        4    0.000    0.000    0.000    0.000 :0(print)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 :0(setprofile)</span><br><span class="line">        2    0.000    0.000    0.000    0.000 :0(sleep)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 &lt;string&gt;:1(&lt;module&gt;)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 profile:0(main())</span><br><span class="line">        0    0.000             0.000          profile:0(profiler)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 test.py:1(next_cal)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 test.py:16(__init__)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 test.py:19(search_string)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 test.py:36(__next_arr)</span><br><span class="line">        1    0.000    0.000    0.000    0.000 test.py:51(main)</span><br></pre></td></tr></table></figure>
<p>虽然函数调用情况可以得到的，但是分析起来比较麻烦，函数之间的调用关系也没有给出。此方式虽然简单但是不够直观。</p>
<hr />
<h1 id="pycallgraph">pycallgraph</h1>
<p>在pycallgraph之前不得不先提及一下<strong><a href="http://www.graphviz.org/">Graphviz</a></strong>，毕竟pycallgraph是基于Graphviz的。Graphviz是开源的图形可视化软件，能够将结构信息表示为抽象图和网络图的方法。至官网下载并安装Graphviz：<a href="http://www.graphviz.org/download/" class="uri">http://www.graphviz.org/download/</a>，同时在将<code>graphviz\bin</code>添加到环境变量中。graphviz中有多种布局器，pycallgraph所使用的便是其中的dot.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dot 默认布局方式，主要用于有向图</span><br><span class="line">neato 基于spring-model(又称force-based)算法</span><br><span class="line">twopi 径向布局</span><br><span class="line">circo 圆环布局</span><br><span class="line">fdp 用于无向图</span><br></pre></td></tr></table></figure>
<p>看看graphviz都能画些什么样的图：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211213pyviz2.png" /></p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211213pyviz3.png" /></p>
<p>安装并配置好graphviz后，需要安装pycallgraph，直接使用pip安装即可:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install pycallgraph</span><br></pre></td></tr></table></figure>
<p>之后可以通过命令行或代码来生成函数调用关系图。</p>
<p>命令行(windows中不识别pycallgraph，应该还要需要配置其环境变量):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pycallgraph graphviz -- .&#x2F;main.py</span><br></pre></td></tr></table></figure>
<p>code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pycallgraph <span class="keyword">import</span> PyCallGraph</span><br><span class="line"><span class="keyword">from</span> pycallgraph.output <span class="keyword">import</span> GraphvizOutput</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> PyCallGraph(output=GraphvizOutput(output_file=<span class="string">&quot;test.png&quot;</span>)):</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>当然可以配置显示或不显示哪些函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pycallgraph <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> pycallgraph <span class="keyword">import</span> PyCallGraph</span><br><span class="line"><span class="keyword">from</span> pycallgraph.output <span class="keyword">import</span> GraphvizOutput</span><br><span class="line">config = Config()</span><br><span class="line"><span class="comment"># 关系图中包括(include)哪些函数名。</span></span><br><span class="line"><span class="comment"># 如果是某一类的函数，例如类gobang，则可以直接写&#x27;gobang.*&#x27;，表示以gobang.开头的所有函数。（利用正则表达式）。</span></span><br><span class="line"><span class="comment"># config.trace_filter = GlobbingFilter(include=[</span></span><br><span class="line"><span class="comment">#     &#x27;draw_chessboard&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;draw_chessman&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;draw_chessboard_with_chessman&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;choose_save&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;choose_turn&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;choose_mode&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;choose_button&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;save_chess&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;load_chess&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;play_chess&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;pop_window&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;tip&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;get_score&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;max_score&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;win&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;key_control&#x27;</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line"><span class="comment"># 该段作用是关系图中不包括(exclude)哪些函数。(正则表达式规则)</span></span><br><span class="line"><span class="comment"># config.trace_filter = GlobbingFilter(exclude=[</span></span><br><span class="line"><span class="comment">#     &#x27;pycallgraph.*&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;*.secret_function&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;FileFinder.*&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;ModuleLockManager.*&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;SourceFilLoader.*&#x27;</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line">graphviz = GraphvizOutput()</span><br><span class="line">graphviz.output_file = <span class="string">&#x27;test.png&#x27;</span></span><br><span class="line"><span class="keyword">with</span> PyCallGraph(output=graphviz, config=config):</span><br><span class="line">	main()</span><br></pre></td></tr></table></figure>
<p>运行后再当前目录下生成test.png文件绘制了函数调用关系(默认文件名是pycallgraph.png)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphvizOutput</span>(<span class="params">Output</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        self.tool = <span class="string">&#x27;dot&#x27;</span></span><br><span class="line">        self.output_file = <span class="string">&#x27;pycallgraph.png&#x27;</span></span><br><span class="line">        self.output_type = <span class="string">&#x27;png&#x27;</span></span><br><span class="line">        self.font_name = <span class="string">&#x27;Verdana&#x27;</span></span><br><span class="line">        self.font_size = <span class="number">7</span></span><br><span class="line">        self.group_font_size = <span class="number">10</span></span><br><span class="line">        self.group_border_color = Color(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line">        Output.__init__(self, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.prepare_graph_attributes()</span><br></pre></td></tr></table></figure>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211213pyviz4.png" /></p>
<p>可见此方式对函数的调用关系展示得较为清晰明了，其缺点是制作的图分辨率不高，略显模糊。下为yolov5的调用关系图。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211213pyviz5.png" /></p>
<h1 id="cprofilesnakeviz">cProfile+snakeviz</h1>
<p>使用<strong>cProfie</strong>可以将函数运行分析结果保存为.prof文件，然后使用其他工具进行可视化，此处以<strong>snakeviz</strong>为例。</p>
<h2 id="cprofile">cProfile</h2>
<p>使用cProfile分析工程，可以使用命令行和代码两种方式。</p>
<ul>
<li>cmd</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m cProfile -o result.prof test.py</span><br></pre></td></tr></table></figure>
<ul>
<li>code</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cProfile</span><br><span class="line">cProfile.run(<span class="string">&quot;main()&quot;</span>, filename=<span class="string">&#x27;result.prof&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>不推荐使用命令行形式，针对，整个文件，会打包过多的无用函数，影响观感。使用code方式针对特定函数分析即可。</p>
<h1 id="snakeviz">snakeviz</h1>
<p>安装snakeviz:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install snakeviz</span><br></pre></td></tr></table></figure>
<p>直接调用cProfile生成的文件，在浏览器中可以打开交互式网页，查看分析结果。缺点：虽然能够从图中获知函数调用顺序，但是函数调用关系依旧不够直观。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211213pyviz6.png" /></p>
]]></content>
      <categories>
        <category>随笔</category>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>随笔</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>snap</title>
    <url>/2021/09/21/snap/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p><a href="https://snapcraft.io/">snapcraft</a>提供了很多linux应用，需要先安装snap（snapd），再使用snap管理这些软件，这些软件都制作时将所有运行环境包含在内，不像apt安装deb等需要解决依赖问题，下载的软件都相对于稳定，但也正是包装了所需的运行环境，造成整个软件臃肿，大小较大，占更多的磁盘空间，这是snap不足的。</p>
<h2 id="安装snap">安装snap</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install snapd</span><br></pre></td></tr></table></figure>
<h2 id="基本命令">基本命令</h2>
<p>在snapcraft商店找到要安装的软件，使用<code>snap install</code>安装：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210921snap1.png" alt="image-20210921120039888" /><figcaption>image-20210921120039888</figcaption>
</figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">snap list # 列出已安装的snap应用</span><br><span class="line">snap run # 运行已安装的应用</span><br><span class="line">snap remove # 卸载已安装的运用</span><br></pre></td></tr></table></figure>
<h1 id="其他">其他</h1>
<p>snap软件一般安装在根目录下<code>/snap/</code>，其桌面启动图标放在<code>/var/lib/snapd/desktop/applications/</code></p>
<h1 id="启动图标问题">启动图标问题</h1>
<p>安装的软件有时候图标不显示在启动菜单中。找到启动菜单的文件位置，以deepin系统为例，在如下位置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;share&#x2F;applications&#x2F;</span><br></pre></td></tr></table></figure>
<p>启动文件以<code>.desktop</code>为后缀，如果此文件夹中没有相应程序的<code>.desktop</code>文件，将相应程序的启动文件复制过来，或者创建一个链接（需要root权限）。比如snap安装的程序的<code>.desktop</code>文件位置位于:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;var&#x2F;lib&#x2F;snapd&#x2F;desktop&#x2F;applications&#x2F;</span><br></pre></td></tr></table></figure>
<p>如果文件已经在<code>/usr/share/applications/</code>中但是还不显示在启动菜单中，以文本方式打开此<code>.desktop</code>文件，将其中的<code>OnlyShowIn=**;**;**;</code>注释掉，比如安装tweak时(中文为&quot;优化&quot;)，不显示在启动菜单，将</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">OnlyShowIn&#x3D;GNOME;Unity;Pahtheon;</span><br></pre></td></tr></table></figure>
<p>注释掉：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># OnlyShowIn&#x3D;GNOME;Unity;Pahtheon;</span><br></pre></td></tr></table></figure>
<p>这样就会在启动菜单显示了。</p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh无法正常连接解决</title>
    <url>/2021/12/22/ssh%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E8%BF%9E%E6%8E%A5%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<p>在一般情况下在安装SSH后即可进行远程连接。当然会出现例外情况。</p>
<span id="more"></span>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install ssh</span><br></pre></td></tr></table></figure>
<p>SSH分客户端openssh-client和openssh-server</p>
<p>如果你只是想登陆别的机器的SSH只需要安装openssh-client，深度操作系统有默认安装，如果没有终端执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install openssh-client</span><br></pre></td></tr></table></figure>
<p>如果要使本机开放SSH服务就需要安装openssh-server，终端执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<p>若安装完成后未能正常连接，则查看是否启动了ssh:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ps -e |grep ssh</span><br></pre></td></tr></table></figure>
<p>如果看到sshd那说明ssh-server已经启动了。</p>
<p>如果没有则可以这样启动：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;etc&#x2F;init.d&#x2F;ssh start </span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service ssh start</span><br></pre></td></tr></table></figure>
<p>ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号，如222。</p>
<p>然后重启SSH服务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;etc&#x2F;init.d&#x2F;ssh stop</span><br><span class="line">sudo &#x2F;etc&#x2F;init.d&#x2F;ssh start</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>tinyproxy代理</title>
    <url>/2022/01/24/tinyproxy%E4%BB%A3%E7%90%86/</url>
    <content><![CDATA[<p>--</p>
<p>使用校园网时github经常抽风上不去，于是考虑使用手中的服务器做一个代理，看了一下tinyproxy比较轻量简单，就决定是你了。</p>
<h1 id="安装">安装</h1>
<p>在Centos7及以前的版本可以使用yum安装，但是在Centos8目前还没有合适的包，于是进行源码安装：</p>
<p>项目地址为<a href="https://github.com/tinyproxy/tinyproxy.github.io" class="uri">https://github.com/tinyproxy/tinyproxy.github.io</a>，先clone下来：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> git://github.com/tinyproxy/tinyproxy</span><br></pre></td></tr></table></figure>
<p>然后编译一下:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> tinyproxy</span><br><span class="line">autoreconf -i</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line"><span class="built_in">cd</span> docs/web</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<p>完成之后，<code>tinyproxy/</code>下的<code>etc/</code>中存放了配置文件<code>tinyproxy.conf</code>，<code>tinyproxy/</code>下的<code>src/</code>中存放了配置文件<code>tinyproxy</code>。需要先对配置文件进行编辑，<code>tinyproxy.conf</code>内容：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment">## tinyproxy.conf -- tinyproxy daemon configuration file</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment">## This example tinyproxy.conf file contains example settings</span></span><br><span class="line"><span class="comment">## with explanations in comments. For decriptions of all</span></span><br><span class="line"><span class="comment">## parameters, see the tinproxy.conf(5) manual page.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># User/Group: This allows you to set the user and group that will be</span></span><br><span class="line"><span class="comment"># used for tinyproxy after the initial binding to the port has been done</span></span><br><span class="line"><span class="comment"># as the root user. Either the user or group name or the UID or GID</span></span><br><span class="line"><span class="comment"># number may be used.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">User</span> nobody</span><br><span class="line">Group nobody</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Port: Specify the port which tinyproxy will listen on.  Please note</span></span><br><span class="line"><span class="comment"># that should you choose to run on a port lower than 1024 you will need</span></span><br><span class="line"><span class="comment"># to start tinyproxy using root.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">Port <span class="number">4399</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Listen: If you have multiple interfaces this allows you to bind to</span></span><br><span class="line"><span class="comment"># only one. If this is commented out, tinyproxy will bind to all</span></span><br><span class="line"><span class="comment"># interfaces present.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#Listen 192.168.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Bind: This allows you to specify which interface will be used for</span></span><br><span class="line"><span class="comment"># outgoing connections.  This is useful for multi-home&#x27;d machines where</span></span><br><span class="line"><span class="comment"># you want all traffic to appear outgoing from one particular interface.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#Bind 192.168.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># BindSame: If enabled, tinyproxy will bind the outgoing connection to the</span></span><br><span class="line"><span class="comment"># ip address of the incoming connection.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#BindSame yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Timeout: The maximum number of seconds of inactivity a connection is</span></span><br><span class="line"><span class="comment"># allowed to have before it is closed by tinyproxy.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">Timeout <span class="number">600</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ErrorFile: Defines the HTML file to send when a given HTTP error</span></span><br><span class="line"><span class="comment"># occurs.  You will probably need to customize the location to your</span></span><br><span class="line"><span class="comment"># particular install.  The usual locations to check are:</span></span><br><span class="line"><span class="comment">#   /usr/local/share/tinyproxy</span></span><br><span class="line"><span class="comment">#   /usr/share/tinyproxy</span></span><br><span class="line"><span class="comment">#   /etc/tinyproxy</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#ErrorFile 404 &quot;/usr/local/share/tinyproxy/404.html&quot;</span></span><br><span class="line"><span class="comment">#ErrorFile 400 &quot;/usr/local/share/tinyproxy/400.html&quot;</span></span><br><span class="line"><span class="comment">#ErrorFile 503 &quot;/usr/local/share/tinyproxy/503.html&quot;</span></span><br><span class="line"><span class="comment">#ErrorFile 403 &quot;/usr/local/share/tinyproxy/403.html&quot;</span></span><br><span class="line"><span class="comment">#ErrorFile 408 &quot;/usr/local/share/tinyproxy/408.html&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># DefaultErrorFile: The HTML file that gets sent if there is no</span></span><br><span class="line"><span class="comment"># HTML file defined with an ErrorFile keyword for the HTTP error</span></span><br><span class="line"><span class="comment"># that has occured.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">DefaultErrorFile <span class="string">&quot;/usr/local/share/tinyproxy/default.html&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># StatHost: This configures the host name or IP address that is treated</span></span><br><span class="line"><span class="comment"># as the stat host: Whenever a request for this host is received,</span></span><br><span class="line"><span class="comment"># Tinyproxy will return an internal statistics page instead of</span></span><br><span class="line"><span class="comment"># forwarding the request to that host.  The default value of StatHost is</span></span><br><span class="line"><span class="comment"># tinyproxy.stats.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#StatHost &quot;tinyproxy.stats&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># StatFile: The HTML file that gets sent when a request is made</span></span><br><span class="line"><span class="comment"># for the stathost.  If this file doesn&#x27;t exist a basic page is</span></span><br><span class="line"><span class="comment"># hardcoded in tinyproxy.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">StatFile <span class="string">&quot;/usr/local/share/tinyproxy/stats.html&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># LogFile: Allows you to specify the location where information should</span></span><br><span class="line"><span class="comment"># be logged to.  If you would prefer to log to syslog, then disable this</span></span><br><span class="line"><span class="comment"># and enable the Syslog directive.  These directives are mutually</span></span><br><span class="line"><span class="comment"># exclusive. If neither Syslog nor LogFile are specified, output goes</span></span><br><span class="line"><span class="comment"># to stdout.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#LogFile &quot;/usr/local/var/log/tinyproxy/tinyproxy.log&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Syslog: Tell tinyproxy to use syslog instead of a logfile.  This</span></span><br><span class="line"><span class="comment"># option must not be enabled if the Logfile directive is being used.</span></span><br><span class="line"><span class="comment"># These two directives are mutually exclusive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#Syslog On</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># LogLevel: Warning</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Set the logging level. Allowed settings are:</span></span><br><span class="line"><span class="comment">#	Critical	(least verbose)</span></span><br><span class="line"><span class="comment">#	Error</span></span><br><span class="line"><span class="comment">#	Warning</span></span><br><span class="line"><span class="comment">#	Notice</span></span><br><span class="line"><span class="comment">#	Connect		(to log connections without Info&#x27;s noise)</span></span><br><span class="line"><span class="comment">#	Info		(most verbose)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The LogLevel logs from the set level and above. For example, if the</span></span><br><span class="line"><span class="comment"># LogLevel was set to Warning, then all log messages from Warning to</span></span><br><span class="line"><span class="comment"># Critical would be output, but Notice and below would be suppressed.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">LogLevel Info</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># PidFile: Write the PID of the main tinyproxy thread to this file so it</span></span><br><span class="line"><span class="comment"># can be used for signalling purposes.</span></span><br><span class="line"><span class="comment"># If not specified, no pidfile will be written.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#PidFile &quot;/usr/local/var/run/tinyproxy/tinyproxy.pid&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># XTinyproxy: Tell Tinyproxy to include the X-Tinyproxy header, which</span></span><br><span class="line"><span class="comment"># contains the client&#x27;s IP address.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#XTinyproxy Yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Upstream:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Turns on upstream proxy support.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The upstream rules allow you to selectively route upstream connections</span></span><br><span class="line"><span class="comment"># based on the host/domain of the site being accessed.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Syntax: upstream type (user:pass@)ip:port (&quot;domain&quot;)</span></span><br><span class="line"><span class="comment"># Or:     upstream none &quot;domain&quot;</span></span><br><span class="line"><span class="comment"># The parts in parens are optional.</span></span><br><span class="line"><span class="comment"># Possible types are http, socks4, socks5, none</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For example:</span></span><br><span class="line"><span class="comment">#  # connection to test domain goes through testproxy</span></span><br><span class="line"><span class="comment">#  upstream http testproxy:8008 &quot;.test.domain.invalid&quot;</span></span><br><span class="line"><span class="comment">#  upstream http testproxy:8008 &quot;.our_testbed.example.com&quot;</span></span><br><span class="line"><span class="comment">#  upstream http testproxy:8008 &quot;192.168.128.0/255.255.254.0&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  # upstream proxy using basic authentication</span></span><br><span class="line"><span class="comment">#  upstream http user:pass@testproxy:8008 &quot;.test.domain.invalid&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  # no upstream proxy for internal websites and unqualified hosts</span></span><br><span class="line"><span class="comment">#  upstream none &quot;.internal.example.com&quot;</span></span><br><span class="line"><span class="comment">#  upstream none &quot;www.example.com&quot;</span></span><br><span class="line"><span class="comment">#  upstream none &quot;10.0.0.0/8&quot;</span></span><br><span class="line"><span class="comment">#  upstream none &quot;192.168.0.0/255.255.254.0&quot;</span></span><br><span class="line"><span class="comment">#  upstream none &quot;.&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  # connection to these boxes go through their DMZ firewalls</span></span><br><span class="line"><span class="comment">#  upstream http cust1_firewall:8008 &quot;testbed_for_cust1&quot;</span></span><br><span class="line"><span class="comment">#  upstream http cust2_firewall:8008 &quot;testbed_for_cust2&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  # default upstream is internet firewall</span></span><br><span class="line"><span class="comment">#  upstream http firewall.internal.example.com:80</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># You may also use SOCKS4/SOCKS5 upstream proxies:</span></span><br><span class="line"><span class="comment">#  upstream socks4 127.0.0.1:9050</span></span><br><span class="line"><span class="comment">#  upstream socks5 socksproxy:1080</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The LAST matching rule wins the route decision.  As you can see, you</span></span><br><span class="line"><span class="comment"># can use a host, or a domain:</span></span><br><span class="line"><span class="comment">#  name     matches host exactly</span></span><br><span class="line"><span class="comment">#  .name    matches any host in domain &quot;name&quot;</span></span><br><span class="line"><span class="comment">#  .        matches any host with no domain (in &#x27;empty&#x27; domain)</span></span><br><span class="line"><span class="comment">#  IP/bits  matches network/mask</span></span><br><span class="line"><span class="comment">#  IP/mask  matches network/mask</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#Upstream http some.remote.proxy:port</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># MaxClients: This is the absolute highest number of threads which will</span></span><br><span class="line"><span class="comment"># be created. In other words, only MaxClients number of clients can be</span></span><br><span class="line"><span class="comment"># connected at the same time.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">MaxClients <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Allow: Customization of authorization controls. If there are any</span></span><br><span class="line"><span class="comment"># access control keywords then the default action is to DENY. Otherwise,</span></span><br><span class="line"><span class="comment"># the default action is ALLOW.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The order of the controls are important. All incoming connections are</span></span><br><span class="line"><span class="comment"># tested against the controls based on order.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Allow 127.0.0.1</span></span><br><span class="line"><span class="comment"># Allow ::1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BasicAuth: HTTP &quot;Basic Authentication&quot; for accessing the proxy.</span></span><br><span class="line"><span class="comment"># If there are any entries specified, access is only granted for authenticated</span></span><br><span class="line"><span class="comment"># users.</span></span><br><span class="line"><span class="comment">#BasicAuth user password</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># AddHeader: Adds the specified headers to outgoing HTTP requests that</span></span><br><span class="line"><span class="comment"># Tinyproxy makes. Note that this option will not work for HTTPS</span></span><br><span class="line"><span class="comment"># traffic, as Tinyproxy has no control over what headers are exchanged.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#AddHeader &quot;X-My-Header&quot; &quot;Powered by Tinyproxy&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ViaProxyName: The &quot;Via&quot; header is required by the HTTP RFC, but using</span></span><br><span class="line"><span class="comment"># the real host name is a security concern.  If the following directive</span></span><br><span class="line"><span class="comment"># is enabled, the string supplied will be used as the host name in the</span></span><br><span class="line"><span class="comment"># Via header; otherwise, the server&#x27;s host name will be used.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">ViaProxyName <span class="string">&quot;tinyproxy&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># DisableViaHeader: When this is set to yes, Tinyproxy does NOT add</span></span><br><span class="line"><span class="comment"># the Via header to the requests. This virtually puts Tinyproxy into</span></span><br><span class="line"><span class="comment"># stealth mode. Note that RFC 2616 requires proxies to set the Via</span></span><br><span class="line"><span class="comment"># header, so by enabling this option, you break compliance.</span></span><br><span class="line"><span class="comment"># Don&#x27;t disable the Via header unless you know what you are doing...</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#DisableViaHeader Yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Filter: This allows you to specify the location of the filter file.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#Filter &quot;/usr/local/etc/tinyproxy/filter&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># FilterURLs: Filter based on URLs rather than domains.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#FilterURLs On</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># FilterExtended: Use POSIX Extended regular expressions rather than</span></span><br><span class="line"><span class="comment"># basic.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#FilterExtended On</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># FilterCaseSensitive: Use case sensitive regular expressions.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#FilterCaseSensitive On</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># FilterDefaultDeny: Change the default policy of the filtering system.</span></span><br><span class="line"><span class="comment"># If this directive is commented out, or is set to &quot;No&quot; then the default</span></span><br><span class="line"><span class="comment"># policy is to allow everything which is not specifically denied by the</span></span><br><span class="line"><span class="comment"># filter file.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># However, by setting this directive to &quot;Yes&quot; the default policy becomes</span></span><br><span class="line"><span class="comment"># to deny everything which is _not_ specifically allowed by the filter</span></span><br><span class="line"><span class="comment"># file.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#FilterDefaultDeny Yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Anonymous: If an Anonymous keyword is present, then anonymous proxying</span></span><br><span class="line"><span class="comment"># is enabled.  The headers listed are allowed through, while all others</span></span><br><span class="line"><span class="comment"># are denied. If no Anonymous keyword is present, then all headers are</span></span><br><span class="line"><span class="comment"># allowed through.  You must include quotes around the headers.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Most sites require cookies to be enabled for them to work correctly, so</span></span><br><span class="line"><span class="comment"># you will need to allow Cookies through if you access those sites.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#Anonymous &quot;Host&quot;</span></span><br><span class="line"><span class="comment">#Anonymous &quot;Authorization&quot;</span></span><br><span class="line"><span class="comment">#Anonymous &quot;Cookie&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ConnectPort: This is a list of ports allowed by tinyproxy when the</span></span><br><span class="line"><span class="comment"># CONNECT method is used.  To disable the CONNECT method altogether, set</span></span><br><span class="line"><span class="comment"># the value to 0.  If no ConnectPort line is found, all ports are</span></span><br><span class="line"><span class="comment"># allowed.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The following two ports are used by SSL.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#ConnectPort 443</span></span><br><span class="line"><span class="comment">#ConnectPort 563</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Configure one or more ReversePath directives to enable reverse proxy</span></span><br><span class="line"><span class="comment"># support. With reverse proxying it&#x27;s possible to make a number of</span></span><br><span class="line"><span class="comment"># sites appear as if they were part of a single site.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If you uncomment the following two directives and run tinyproxy</span></span><br><span class="line"><span class="comment"># on your own computer at port 8888, you can access Google using</span></span><br><span class="line"><span class="comment"># http://localhost:8888/google/ and Wired News using</span></span><br><span class="line"><span class="comment"># http://localhost:8888/wired/news/. Neither will actually work</span></span><br><span class="line"><span class="comment"># until you uncomment ReverseMagic as they use absolute linking.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#ReversePath &quot;/google/&quot;	&quot;http://www.google.com/&quot;</span></span><br><span class="line"><span class="comment">#ReversePath &quot;/wired/&quot;	&quot;http://www.wired.com/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When using tinyproxy as a reverse proxy, it is STRONGLY recommended</span></span><br><span class="line"><span class="comment"># that the normal proxy is turned off by uncommenting the next directive.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#ReverseOnly Yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Use a cookie to track reverse proxy mappings. If you need to reverse</span></span><br><span class="line"><span class="comment"># proxy sites which have absolute links you must uncomment this.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#ReverseMagic Yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The URL that&#x27;s used to access this reverse proxy. The URL is used to</span></span><br><span class="line"><span class="comment"># rewrite HTTP redirects so that they won&#x27;t escape the proxy. If you</span></span><br><span class="line"><span class="comment"># have a chain of reverse proxies, you&#x27;ll need to put the outermost</span></span><br><span class="line"><span class="comment"># URL here (the address which the end user types into his/her browser).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># If not set then no rewriting occurs.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#ReverseBaseURL &quot;http://localhost:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>需要注意的参数:</p>
<ul>
<li>Port：端口，默认8888，可自定义。</li>
<li>Allow：允许访问的ip，默认是本地的127.0.0.1，注释掉保证所有IP可以访问。</li>
<li>BasicAuth：设置基本验证信息，注释掉则不需要。</li>
</ul>
<hr />
<h1 id="启用">启用</h1>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">/installdir/tinyproxy/src/tinyproxy  -c  /installdir/tinyproxy/etc/tinyproxy.conf</span><br></pre></td></tr></table></figure>
<h1 id="客户端配置">客户端配置</h1>
<p><code>设置---&gt;网络和internet---&gt;代理</code>，打开使用代理服务器，在其中填写ip地址和端口号。个别浏览器可能要进行代理配置。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shell</span></span><br><span class="line">curl -x ip:port url</span><br></pre></td></tr></table></figure>
<p>完成。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>vue init webpack...</title>
    <url>/2022/01/07/vue-init-webpack/</url>
    <content><![CDATA[<p>vue-cli · Failed to download repo vuejs-templates/webpack: Hostname/IP does not match certificate's altnames: Host: github.com. is not in the cert's altnames: DNS:erp.bcl-bd.com</p>
<span id="more"></span>
<p>使用<code>vue init webpack project</code>初始化工程时报错，原因是从github下载各种依赖时由于网络状况等导致下载错误，解决办法为：</p>
<ul>
<li>科学上网后执行该命令</li>
<li>时先下载webpack，以本地文件形式执行命令：</li>
</ul>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">vue init ./webpack project</span><br></pre></td></tr></table></figure>
<p>之后：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> project </span><br><span class="line"><span class="comment">## </span></span><br><span class="line">cnpm install</span><br><span class="line">cnpm run dev</span><br></pre></td></tr></table></figure>
<h1 id="vite">vite</h1>
<p>Vite 是一个 web 开发构建工具，由于其原生 ES 模块导入方式，可以实现闪电般的冷服务器启动。</p>
<p>通过在终端中运行以下命令，可以使用 Vite 快速构建 Vue 项目。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cnpm init @vitejs/app project-name</span><br></pre></td></tr></table></figure>
<ul>
<li>需要注意的是，此方式在windows下使用git bash时可能出现<strong>bug，导致无法使用箭头进行选择</strong>，推荐在cmd中使用此命令</li>
</ul>
<p>后续:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line">cnpm install</span><br><span class="line">cnpm run dev</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Web</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>一种树形结构——并查集</title>
    <url>/2021/03/06/%E4%B8%80%E7%A7%8D%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E5%B9%B6%E6%9F%A5%E9%9B%86/</url>
    <content><![CDATA[<!-- more -->
<h1 id="概述">概述</h1>
<p><strong>并查集</strong>（Disjoint-set data structure，直译为不交集数据结构）是一种数据结构，用于处理一些不交集（Disjoint sets，一系列没有重复元素的集合）的合并及查询问题。根据并查集的名字可以很直观的理解它的意思，即“合并集合”和“查找集合中的元素”两种操作的关于数据结构的一种算法。<br />
有一个联合-查找算法（Union-find Algorithm）定义了两个用于此数据结构的操作： * <strong>Find</strong>：确定元素属于哪一个子集。它可以被用来确定两个元素是否属于同一子集。 * <strong>Union</strong>：将两个子集合并成同一个集合。</p>
<h1 id="适用问题">适用问题</h1>
<p>并查集用在一些有 N 个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于同一组的元素所在的集合合并，其间要反复查找一个元素在哪个集合中。这个过程看似并不复杂，但数据量极大，若用其他的数据结构来描述的话，往往在空间上过大，计算机无法承受，也无法在短时间内计算出结果，所以只能用并查集来处理。 ## 图联通问题 * 维护无向图的连通性。支持判断两个点是否在同一连通块内。 * 判断增加一条边是否会产生环：用在求解最小生成树的Kruskal算法里。 ## 亲戚关系 判断亲戚关系是一个典型的并查集问题，一个大家族的成员很多，判断两个成员是否为亲戚是比较困难的。我们可以建立模型，把所有人划分到若干个不相交的集合中，每个集合里的人彼此是亲戚。为了判断两个人是否为亲戚，只需看它们是否属于同一个集合即可。因此，这里就可以考虑用并查集进行维护了。</p>
<h1 id="并查集基本操作">并查集基本操作</h1>
<h2 id="初始化集合">初始化集合</h2>
<p>并查集首先得制作一个集合，初始化时一般将元素本身的索引作为其的值，即将元素的父节点设置为它自己。这能保证其独一性，即每个元素在不同的集合中。 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"># 初始化</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">init</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fa[n];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i)</span><br><span class="line">        fa[i] = i;</span><br><span class="line">        <span class="keyword">return</span> fa</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> ## 查询 查询操作即为寻找元素最根本的父节点(根节点)。可以采用递归的形式来查找，当元素的索引与其值不相等时，说明元素存在父节点；当他们相等的时候说明元素的父节点为其本身，即为根节点。 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(fa[x] == x)</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> find(fa[x]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="合并">合并</h2>
<p>合并操作也是很简单的，只需将其中一个元素的根节点指向另外一个元素就可以了。两者的指向顺序暂不讨论。后续会给出一种路径压缩算法来指定合并的顺序以达到最高效率。 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> root_x = find(x);</span><br><span class="line">        <span class="keyword">int</span> root_y = find(y);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(root_x != root_y)&#123;</span><br><span class="line">            father[root_x] = root_y;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure> ## 连通性判断 直接判断两个元素的根节点是否相同即可判断两个元素是否在同一个集合内，即他们是否连通。 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">is_connected</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">       <span class="keyword">return</span> find(x) == find(y);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure> ## 路径压缩 通过上面的操作，已经可以制作了一个简单的并查集了，但是这种简单的并查集的效率一般不是很高。我们在进行合并的时候，由于一个集合的根父节点指定的位置可以是另一个集合的任意位置，在合并的过程中，集合的关系链可能会越来越长，这样在查找根节点的时候可能需要递归很多次，大大影响了效率。所以需要一种方式来避免“长链”的出现。 ### 合并（路径压缩） 我们在查询的过程中，把沿途的每个节点的父节点都设为根节点即可。下一次再查询时，我们就可以省很多事。递归的写法如下，与普通的查找根节点算法相比，多了一步设置父节点为根节点的步骤。 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == fa[x])</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        fa[x] = find(fa[x]);  <span class="comment">//父节点设为根节点</span></span><br><span class="line">        <span class="keyword">return</span> fa[x];         <span class="comment">//返回父节点</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> ### 按秩合并 我们用一个数组 <strong>rank[]</strong> 记录每个根节点对应的树的深度（如果不是根节点，其rank相当于以它作为根节点的子树的深度）。一开始，把所有元素的rank（<strong>秩</strong>）设为1。合并时比较两个根节点，把rank较小者往较大者上合并。路径压缩和按秩合并如果一起使用，<strong>时间复杂度</strong>接近O(n) ，但是很可能会破坏rank的准确性。值得注意的是，按秩合并会带来额外的<strong>空间复杂度</strong>。 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        fa[i] = i;</span><br><span class="line">        rank[i] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = find(i), y = find(j);    <span class="comment">//先找到两个根节点</span></span><br><span class="line">    <span class="keyword">if</span> (rank[x] &lt;= rank[y])</span><br><span class="line">        fa[x] = y;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        fa[y] = x;</span><br><span class="line">    <span class="keyword">if</span> (rank[x] == rank[y] &amp;&amp; x != y)</span><br><span class="line">        rank[y]++;                   <span class="comment">//如果深度相同且根节点不同，则新的根节点的深度+1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="应用例程">应用例程</h1>
<h2 id="题目描述">题目描述</h2>
<p><strong>省份的数量(Leetcode.547)</strong><br />
有 n 个城市，其中一些彼此相连，另一些没有相连。如果城市 a 与城市 b 直接相连，且城市 b 与城市 c 直接相连，那么城市 a 与城市 c 间接相连。<br />
<strong>省份</strong>是一组直接或间接相连的城市，组内不含其他没有相连的城市。 给你一个 n x n 的矩阵 isConnected ，其中 isConnected[i][j] = 1 表示第 i 个城市和第 j 个城市直接相连，而 isConnected[i][j] = 0 表示二者不直接相连。 返回矩阵中<strong>省份的数量</strong>。</p>
<p><strong>示例1：</strong><br />
<strong>输入：</strong> isConnected = [[1,1,0],[1,1,0],[0,0,1]]<br />
<strong>输出：</strong> 2<br />
对角线上的元素为其本身，一定联通，恒为1.矩阵中1与2城市联通，所以1、2构成一个省份，3单独作为一个省份，省份数量为2.</p>
<p><strong>示例2：</strong><br />
<strong>输入：</strong> isConnected = [[1,0,0],[0,1,0],[0,0,1]]<br />
<strong>输出：</strong> 3<br />
对角矩阵，表示无城市联通，所以三个城市均可单独作为一个省份，总省份数量为3.</p>
<h2 id="题解">题解</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findCircleNum</span>(<span class="params">self, isConnected: List[List[<span class="built_in">int</span>]]</span>) -&gt; int:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">find</span>(<span class="params">index: <span class="built_in">int</span></span>) -&gt; int:</span> <span class="comment">#查找根节点</span></span><br><span class="line">            <span class="keyword">if</span> parent[index] != index:</span><br><span class="line">                parent[index] = find(parent[index])</span><br><span class="line">            <span class="keyword">return</span> parent[index]</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">union</span>(<span class="params">index1: <span class="built_in">int</span>, index2: <span class="built_in">int</span></span>):</span> <span class="comment">#合并两个(城市)集合</span></span><br><span class="line">            parent[find(index1)] = find(index2)</span><br><span class="line">        </span><br><span class="line">        provinces = <span class="built_in">len</span>(isConnected) <span class="comment">#定义初始省份数量</span></span><br><span class="line">        parent = <span class="built_in">list</span>(<span class="built_in">range</span>(provinces)) <span class="comment">#每个城市作为一个省份</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(provinces):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, provinces):</span><br><span class="line">                <span class="keyword">if</span> isConnected[i][j] == <span class="number">1</span>: <span class="comment">#如果相应城市联通，将他们加入同一个集合。</span></span><br><span class="line">                    union(i, j)</span><br><span class="line">        </span><br><span class="line">        circles = <span class="built_in">sum</span>(parent[i] == i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(provinces)) <span class="comment">#统计整个父矩阵中父节点为自己的数量(根节点数量)，也就是集合数量、省份数量</span></span><br><span class="line">        <span class="keyword">return</span> circles</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>二叉树基础</title>
    <url>/2021/04/07/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p><strong>二叉树（Binary tree）</strong> 是一种基本的数据结构。许多问题可以抽象出二叉树的形式，其结构和算法均较为简单。主要用于提升搜索速度。</p>
<hr />
<h1 id="基本概念">基本概念</h1>
<h2 id="节点">节点</h2>
<p>节点是二叉树中最基本的元素。在链表形式的二叉树中，一个节点包含的信息主要有此节点的值、子节点的指针（红黑树中还存在节点颜色信息等）。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407node1.png" alt="image-20210407230430436" /><figcaption>image-20210407230430436</figcaption>
</figure>
<h2 id="节点的度">节点的度</h2>
<p>节点的度指的是此节点拥有子树的数量。在二叉树中，节点的度只能为0、1、2.</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407node2.png" alt="image-20210407230903553" /><figcaption>image-20210407230903553</figcaption>
</figure>
<h2 id="树">树</h2>
<p>由节点组成的集合。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407tree1.png" alt="image-20210407231144714" /><figcaption>image-20210407231144714</figcaption>
</figure>
<h2 id="树的度">树的度</h2>
<p>一个树的度为节点中度的最大值。</p>
<h2 id="节点层次">节点层次</h2>
<p>树的每一行为一层，从根节点开始为第一层。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407tree2.png" alt="image-20210407231513569" /><figcaption>image-20210407231513569</figcaption>
</figure>
<h2 id="树的深度">树的深度</h2>
<p>树的深度即为节点的最大层次数。</p>
<hr />
<h1 id="二叉树的分类">二叉树的分类</h1>
<h2 id="斜树">斜树</h2>
<p>所有的结点都只有左子树的二叉树叫左斜树。所有结点都是只有右子树的二叉树叫右斜树。这两者统称为斜树。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407tree3.png" alt="image-20210407232035962" /><figcaption>image-20210407232035962</figcaption>
</figure>
<h2 id="完全二叉树">完全二叉树</h2>
<p>树的每一层按从上到下，从左到右依次填充，中间不可有空（与数组的索引对应）。其特点为：</p>
<ol type="1">
<li>叶子结点只能出现在最下层和次下层；</li>
<li>最下层的叶子结点集中在树的左部；</li>
<li>倒数第二层若存在叶子结点，一定在右部连续位置；</li>
<li>如果结点度为1，则该结点只有左孩子，即没有右子树；</li>
<li>同样结点数目的二叉树，完全二叉树深度最小。</li>
</ol>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407tree4.png" alt="image-20210407232524762" /><figcaption>image-20210407232524762</figcaption>
</figure>
<h2 id="满二叉树">满二叉树</h2>
<p>在一棵二叉树中。如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树。 满二叉树的特点有：</p>
<ol type="1">
<li>叶子只能出现在最下一层。出现在其它层就不可能达成平衡。</li>
<li>非叶子结点的度一定是2。</li>
<li>在同样深度的二叉树中，满二叉树的结点个数最多，叶子数最多。</li>
</ol>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407tree5.png" alt="image-20210407232725665" /><figcaption>image-20210407232725665</figcaption>
</figure>
<h2 id="扩充二叉树">扩充二叉树</h2>
<p>扩充二叉树是对已有二叉树的扩充，扩充后的二叉树的节点都变为度数为2的分支节点。也就是说，如果原节点的度数为2，则不变，度数为1，则增加一个分支，度数为0的叶子节点则增加两个分支。</p>
<h2 id="平衡二叉树">平衡二叉树</h2>
<p>是一棵空树或它的任意节点的左右两个子树的高度差的绝对值不超过1</p>
<hr />
<h1 id="二叉树的存储方式">二叉树的存储方式</h1>
<h2 id="顺序存储">顺序存储</h2>
<p>二叉树的顺序存储结构就是使用一维数组存储二叉树中的结点，并且结点的存储位置，就是数组的下标索引。这种方式适用于完全二叉树，应为完全二叉树的节点是按照顺序来分布的。当二叉树不为完全二叉树时，特别是当树为斜树时，会造成数组中许多索引处的值无效，浪费大量的空间。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210407tree6.png" alt="image-20210407233505069" /><figcaption>image-20210407233505069</figcaption>
</figure>
<h2 id="链表存储">链表存储</h2>
<p>在结构体中直接定义两个孩子指针和一个数据储存区。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">BiTNode</span>&#123;</span></span><br><span class="line">    TElemType data;<span class="comment">//数据</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">BiTNode</span> *<span class="title">lchild</span>, *<span class="title">rchild</span>;</span><span class="comment">//左右孩子指针</span></span><br><span class="line">&#125; BiTNode, *BiTree;</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="二叉树的遍历">二叉树的遍历</h1>
<p>二叉树的遍历方式主要有前序遍历、中序遍历、后续遍历三种。</p>
<h2 id="前序遍历">前序遍历</h2>
<p>从根节点出发，按照先左后右的方式遍历所有子树（父节点 - 左子树 - 右子树）。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PreOrderTraverse</span><span class="params">(BiTree T)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T==<span class="literal">NULL</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>, T-&gt;data);  <span class="comment">/*显示结点数据，可以更改为其他对结点操作*/</span></span><br><span class="line">    PreOrderTraverse(T-&gt;lchild);    <span class="comment">/*再先序遍历左子树*/</span></span><br><span class="line">    PreOrderTraverse(T-&gt;rchild);    <span class="comment">/*最后先序遍历右子树*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="中序遍历">中序遍历</h2>
<p>从最左子节点出发，按照左子树 - 父节点 - 右子树的顺序遍历。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InOrderTraverse</span><span class="params">(BiTree T)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T==<span class="literal">NULL</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">    InOrderTraverse(T-&gt;lchild); <span class="comment">/*中序遍历左子树*/</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>, T-&gt;data);  <span class="comment">/*显示结点数据，可以更改为其他对结点操作*/</span></span><br><span class="line">    InOrderTraverse(T-&gt;rchild); <span class="comment">/*最后中序遍历右子树*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="后序遍历">后序遍历</h2>
<p>从最左子节点出发，按照左子树 - 右子树 - 父节点的顺序遍历。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PostOrderTraverse</span><span class="params">(BiTree T)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(T==<span class="literal">NULL</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">    PostOrderTraverse(T-&gt;lchild);   <span class="comment">/*先后序遍历左子树*/</span></span><br><span class="line">    PostOrderTraverse(T-&gt;rchild);   <span class="comment">/*再后续遍历右子树*/</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>, T-&gt;data);  <span class="comment">/*显示结点数据，可以更改为其他对结点操作*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="二叉树的应用">二叉树的应用</h1>
<p>普通的二叉树，很难构成现实的应用场景，但因其简单，常用于学习研究，平衡二叉树则是实际应用比较多的。常见于快速匹配、搜索等方面。<br />
在二叉树建立的时候，通常按照左子树的数小于父节点、右子树的数大于父节点的规则来排序。这样在搜索的时候，可以将时间复杂度降低至<span class="math inline">\(O(logn)\)</span>或者<span class="math inline">\(O(h)\)</span> (<span class="math inline">\(h\)</span>为树的深度)，大大提升搜索效率。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title>位运算之美</title>
    <url>/2022/03/13/%E4%BD%8D%E8%BF%90%E7%AE%97%E4%B9%8B%E7%BE%8E/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>计算机底层用的是机器码，即0和1交织而成。位运算在某些情况下，有着神奇的功效，能够提升效率，节省空间。</p>
<ul>
<li>例如：</li>
</ul>
<blockquote>
<p>一个整型数组 nums 里除两个数字之外，其他数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是O(n)，空间复杂度是O(1)。 示例 1：</p>
<p>输入：nums = [4,1,4,6] 输出：[1,6] 或 [6,1] 示例 2：</p>
<p>输入：nums = [1,2,10,4,1,4,3,3] 输出：[2,10] 或 [10,2]</p>
</blockquote>
<p>可以采用异或操作得出两个不同数字的异或值(其他所有成对的数字异或后等于0)，最终的结果每一位上若为1则表示两数在此位上不同，反之为0则相同。随便取为1的一位(由于俩数不同，必存在为1的位)，将整个数组分为两组(以此位为0和一分别分两组)，这样两个不同的数必然会分到两个不同的组中。最后两个不同的组中组内依次异或，可以计算出不同的那一位，两组得到最终两个不同的数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNumbers</span>(<span class="params">self, nums: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        x = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            x ^= nums[i] <span class="comment"># 求不同俩数异或结果</span></span><br><span class="line">        t = x - (x&amp;(x-<span class="number">1</span>)) <span class="comment"># 取最低位1，树状数组的典型操作，位运算优先级较低加括号</span></span><br><span class="line">        ans = [<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> nums: <span class="comment"># 分组并异或</span></span><br><span class="line">            <span class="keyword">if</span> t&amp;x:</span><br><span class="line">                ans[<span class="number">0</span>] ^= x </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans[<span class="number">1</span>] ^= x</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="详解">详解</h1>
<ul>
<li>使用位运算求奇数偶数：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x&amp;1&#x3D;&#x3D;1	x&amp;1&#x3D;&#x3D;0</span><br></pre></td></tr></table></figure>
<p>最低位为一为奇数，为零为偶数。用此方法的效率要高于一般的求余函数等操作。</p>
<ul>
<li>使用位运算来交换变量：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a ^&#x3D; b;</span><br><span class="line">b ^&#x3D; a;</span><br><span class="line">a ^&#x3D; b;</span><br><span class="line"></span><br><span class="line">第一步：a ^&#x3D; b ---&gt; a &#x3D; (a^b);</span><br><span class="line"></span><br><span class="line">第二步：b ^&#x3D; a ---&gt; b &#x3D; b^(a^b) ---&gt; b &#x3D; (b^b)^a &#x3D; a</span><br><span class="line"></span><br><span class="line">第三步：a ^&#x3D; b ---&gt; a &#x3D; (a^b)^a &#x3D; (a^a)^b &#x3D; b</span><br></pre></td></tr></table></figure>
<p>比使用临时变量更高效率。</p>
<ul>
<li>交换符号将正数变成负数，负数变成正数：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~x+1</span><br></pre></td></tr></table></figure>
<p>整数取反加1，正好变成其对应的负数(补码表示)；负数取反加一，则变为其原码，即正数.</p>
<ul>
<li>消去最低位1：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x&amp;(x-1)</span><br></pre></td></tr></table></figure>
<p>如果是<code>x-(x&amp;(x-1)</code>则为最取最低位1，这是树状数组中典型的lowbit操作的根本依据。</p>
<ul>
<li>使用二进制进行子集枚举：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">arr &#x3D; [3,5,7]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr class="header">
<th>编号</th>
<th>进制</th>
<th>子集</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>000</td>
<td>[]</td>
</tr>
<tr class="even">
<td>1</td>
<td>001</td>
<td>[3]</td>
</tr>
<tr class="odd">
<td>2</td>
<td>010</td>
<td>[5]</td>
</tr>
<tr class="even">
<td>3</td>
<td>011</td>
<td>[3,5]</td>
</tr>
<tr class="odd">
<td>4</td>
<td>100</td>
<td>[7]</td>
</tr>
<tr class="even">
<td>5</td>
<td>101</td>
<td>[3,7]</td>
</tr>
<tr class="odd">
<td>6</td>
<td>110</td>
<td>[5,7]</td>
</tr>
<tr class="even">
<td>7</td>
<td>111</td>
<td>[3,5,7]</td>
</tr>
</tbody>
</table>
<p>每一位代表当前数的取或者不取。</p>
<ul>
<li>找出现一次的数：</li>
</ul>
<p>在一个数组中，只有一个数出现一次，剩下都出现两次，找出出现一次的数。此时将每个数依次<strong>异或</strong>一遍，出现次数为两次的(偶数次均可)，会相互抵消为0，剩下的就是只出现一次的数。</p>
<hr />
<h1 id="bye">Bye</h1>
<p>合理运用位运算可以有出其不意的效果，如开头提到的例子，如果用哈希表的话需要花费很多额外的空间，而使用位运算则只需常数复制度就可以解决这个问题。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>免费搭建自己的私人服务器</title>
    <url>/2021/03/08/%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%A7%81%E4%BA%BA%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    <content><![CDATA[<!-- more -->
<p>一直想搭建一个私人服务器，常用的NAS成本太高，直接放弃。后来无意间发现可以用内网穿透来做这个事情，手中刚好有一台闲置不常用的电脑，便把它作成了一个私有网盘了。整个搭建过程很简单，而且零成本。</p>
<h1 id="准备工作">准备工作</h1>
<ul>
<li><a href="https://www.xp.cn/">PhPstudy</a></li>
<li><a href="https://kodcloud.com/download/">可道云</a></li>
<li><a href="https://u.tools/">utools</a></li>
</ul>
<p>下载<strong>可道云</strong>，从官网下载<strong>PhPstudy</strong>和<strong>utools</strong>并安装，从<strong>utools</strong>中搜索安装内网穿透工具。 # 搭建步骤 启动<strong>PhPstudy</strong>进行<a href="https://www.xp.cn/wenda/394.html">相关配置</a>，在<strong>PhPstudy</strong>的安装目录下找到 ***._pro** 文件夹，将文件夹中的内容删除，然后将可道云的压缩包解压到此文件夹。<br />
由于默认端口设置的是80，所以直接打开 <strong>localhost(127.0.0.1)</strong> 即可进行可道云的相关配置，设置登录账户。<br />
设置完成后已经可以通过内网访问了，现在用内网穿透工具来实现外网访问。<br />
打开 <strong>utools</strong> 后： * 选择一个节点 * 连接方式选择为http服务 * 自行设置一个外网域名 * 内网地址用 <strong>127.0.0.1</strong> * 内网端口用 <strong>80</strong> * 打开连接</p>
<p>设置完成后就可以在任何地方使用它所生成的(你设置的)域名来访问你的服务器了。</p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>内网访问VPN</title>
    <url>/2022/01/24/%E5%86%85%E7%BD%91%E8%AE%BF%E9%97%AEVPN/</url>
    <content><![CDATA[<p>--</p>
<p>当我们在不在内网网段，而想访问内网的一些资源，或借用内网的ip去访问一些资源时(如使用校园网去下载知网文献)，可以使用云服务器对内网进行穿透并搭建socks隧道，然后进行流量转发。可以使用NPS来实现此功能。有关NPS的基本配置可查看以前的博客，<a href="https://www.xiubenwu.top/2021/06/27/NPS%E6%90%AD%E5%BB%BA/">this</a>.</p>
<hr />
<h1 id="代理服务器配置">代理服务器配置</h1>
<p>在内网的一个主机中安装NPC，相应的客户端设置用户名和认证密码，连接代理时用到。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220124npsvpn1.png" /></p>
<p>在此客户端下添加一个socks代理隧道，设置服务端口。此时一个简单的代理服务器已经搭建完毕了。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220124npsvpn2.png" /></p>
<hr />
<h1 id="用户客户端配置">用户客户端配置</h1>
<p>客户端可以使用的代理软件很多，此处使用<strong>proxifier</strong>进行代理。首先配置代理服务器：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220124npsvpn3.png" /></p>
<p>添加一个代理服务器，输入服务器ip及端口，选择socks5协议，填入用户名和密码，最后检查一下是否连上了</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220124npsvpn4.png" /></p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220124npsvpn5.png" /></p>
<p>最后设置一下代理规则，选择上你要代理的服务器便大功告成了，proxifier启动后再运行程序便可成功代理。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20220124npsvpn6.png" alt="image-20220124223415131" /><figcaption>image-20220124223415131</figcaption>
</figure>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title>创建列表的正确姿势</title>
    <url>/2021/04/22/%E5%88%9B%E5%BB%BA%E5%88%97%E8%A1%A8%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/</url>
    <content><![CDATA[<h1 id="section">---</h1>
<p>此文并不介绍python中创建列表的各种方式，而源于写基数排序时的一个小Bug</p>
<hr />
<p>在写基数排序的时候，创建了一个表中表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [[]] * <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>这事用来存放对应位数上的元素的。在<code>append</code>元素的时候发现所有子列表是一起变化的。很自然的想到子列表是共用同一个地址的。查看一下内存地址果然如此。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [[]] * <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    print(<span class="built_in">id</span>(i))</span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br><span class="line"><span class="number">2502709438920</span></span><br></pre></td></tr></table></figure>
<p>不用理会在内存中的具体地址是多少，只需要知道每一个列表的地址是一样的就可以了。列表的对应关系如下图所示。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210422list1.png" alt="image-20210422145549029" /><figcaption>image-20210422145549029</figcaption>
</figure>
<hr />
<p>我们知道，在<code>python</code>中给列表赋值有直接赋值（添加一个地址指针）和拷贝赋值（copy）等方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = a</span><br><span class="line">c = a.copy()</span><br></pre></td></tr></table></figure>
<p>在上面的例子中，b和a变量指向的内存地址是一样的，改变a或b的值另外一个也会相应的变化，而c则是独立出来的新变量，c的值与ab的值之间不会相互影响，他们的关系如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210422list2.png" alt="image-20210422150330792" /><figcaption>image-20210422150330792</figcaption>
</figure>
<p>采用<code>a = [[]] * 10</code>这种方式创建10个子空列表相当于将一个空列表复制十遍但是指向地址还是同一个，所以改变其中一个子列表的值其他子列表的值相应的发生改变。</p>
<hr />
<p>那么正确的创建方式是什么呢。亲测使用<code>[[] for _ in range(10)]</code>可以创建相互独立的子列表。此方式在创建的时候每一次循环都单独生成一个新的地址空间上的列表来添加进原来的列表中。可以查看他们的内存地址：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    print(<span class="built_in">id</span>(i))</span><br><span class="line">    </span><br><span class="line">[out]:</span><br><span class="line">    <span class="number">1999785117192</span></span><br><span class="line">    <span class="number">1999785222472</span></span><br><span class="line">    <span class="number">1999785222344</span></span><br><span class="line">    <span class="number">1999785102344</span></span><br><span class="line">    <span class="number">1999784963272</span></span><br><span class="line">    <span class="number">1999785222536</span></span><br><span class="line">    <span class="number">1999785222600</span></span><br><span class="line">    <span class="number">1999785222792</span></span><br><span class="line">    <span class="number">1999785222856</span></span><br><span class="line">    <span class="number">1999785222728</span></span><br></pre></td></tr></table></figure>
<p>可以看到，这种方式创建出来的列表的地址空间是完全不一样的，结构如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210422list3.png" alt="image-20210422151109215" /><figcaption>image-20210422151109215</figcaption>
</figure>
<hr />
<p>当然，如果采用循环对同一个变量进行创建，那么它的地址任然是一样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = []</span><br><span class="line">a = [x <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    print(<span class="built_in">id</span>(i))</span><br><span class="line">    </span><br><span class="line">[out]:</span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br><span class="line">    <span class="number">3005231884744</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210422list4.png" alt="image-20210422151416752" /><figcaption>image-20210422151416752</figcaption>
</figure>
]]></content>
      <categories>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2021/03/14/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p><strong>动态规划</strong>(Dynamic programming，简称DP)，是一种在多学科中常用的复杂问题求解方法，它是一种方法而不是一种算法。它的最基本思想及为将一个问题分解为多个子问题来进行求解。 <span id="more"></span></p>
<h1 id="概述">概述</h1>
<p>动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再根据子问题的解以得出原问题的解。动态规划往往用于优化递归问题，例如斐波那契数列，如果运用递归的方式来求解会重复计算很多相同的子问题，利用动态规划的思想可以减少计算量。<br />
动态规划在查找有很多<strong>重叠子问题</strong>的情况的最优解时有效。它将问题重新组合成子问题。为了避免多次解决这些子问题，它们的结果都逐渐被<strong>计算并被保存</strong>，从简单的问题直到整个问题都被解决。因此，动态规划保存递归时的结果，因而不会在解决同样的问题时花费时间。<br />
动态规划只能应用于有<strong>最优子结构</strong>的问题。最优子结构的意思是局部最优解能决定全局最优解（对有些问题这个要求并不能完全满足，故有时需要引入一定的近似）。简单地说，问题能够分解成子问题来解决。</p>
<h2 id="重叠子问题">重叠子问题</h2>
<p>像分治法一样，动态规划包含了对子问题的解决。动态规划主要用于不断地解决相同子问题。在动态规划中，子问题的计算解被存储在表中，使得这些不必重新计算。因此，当没有公共（重叠）子问题时，就不会使用动态规划。例如，二分搜索没有公共子问题。当存在重叠子问题的时候，使用动态规划就能以牺牲少量的空间复杂度来换取时间复杂度的大量减低。比较经典的一个重叠子问题就是斐波那契数列的递归解法。斐波那契数列递归解法与动态规划解法将会在后续的例程例程中出现。</p>
<h2 id="最优子结构">最优子结构</h2>
<p>最优子结构是依赖特定问题和子问题的分割方式而成立的条件。如果可以通过各子问题的最优解来求出整个问题的最优解，此时条件成立，认为这是一个最优子结构。反之，如果不能利用子问题的最优解获得整个问题的最优解，那么这种问题就不具有最优子结构。很多问题的最优子结构都表现出非常直观的形式，以至于都不需要另外的证明过程。不过，遇到结构不是很直观的问题时，需要尝试其他的证明方式(反正法等)。</p>
<h2 id="状态转移方程">状态转移方程</h2>
<p>动态规划中当前的状态往往依赖于前一阶段的状态和前一阶段的决策结果。例如我们知道了第<span class="math inline">\(i\)</span>个阶段的状态<span class="math inline">\(S_i\)</span>以及决策<span class="math inline">\(U_i\)</span>，那么第i+1阶段的状态<span class="math inline">\(S_{i+1}\)</span>也就确定了。所以解决动态规划问题的关键就是确定状态转移方程，一旦状态转移方程确定了，那么我们就可以根据方程式进行编码。方程可以表示为：<br />
<span class="math inline">\(S_i = S_{i-1} + U_i\)</span><br />
根据问题的不同，具体的形式也是多变的，例如最值问题中需要加上<span class="math inline">\(min和max\)</span>等。</p>
<h2 id="递归递推与记忆化搜索">递归递推与记忆化搜索</h2>
<p><strong>递归</strong>就是从上往下（从n到1），递归过程不记录中间计算所产生的数据，每次需要数据时会一直算到截止条件；<strong>递推</strong>则是从下往上（从1到n），递推过程记录中间数据，每次需要数据会从记录的数据拿，由于状态方程的关系，递推过程中每次需要的数据一般都会在前面的计算中保留下来；<strong>记忆化搜索</strong>则是在递归的基础上的改进，它依然是从上往下进行计算，只不过是在计算的时候对数据进行了保留，在需要数据的时候从缓存调取。<br />
递推和记忆化搜索可以说是使用动态规划思路对递归算法的一种改进，都采用了时间换取空间，对于斐波那契数列这个问题来说，可以将时间复杂度从指数级降到一次级。动态规划最常见的实现形式为递推，某些地方不将记忆化搜索视为动态规划方法。</p>
<h2 id="动态规划步骤">动态规划步骤</h2>
<p><strong>1. 划分子问题</strong><br />
<strong>2. 确定状态转移方程</strong><br />
<strong>3. 自底而上计算最优解</strong><br />
<strong>4. 根据所得最优解求解问题</strong></p>
<h1 id="例程详解">例程详解</h1>
<h2 id="斐波那契数列">斐波那契数列</h2>
<p>公元1150年印度数学家Gopala和金月在研究箱子包装对象长宽刚好为1和2的可行方法数目时，首先描述这个数列。在西方，最先研究这个数列的人是比萨的列奥那多（意大利人斐波那契Leonardo Fibonacci），他描述兔子生长的数目时用上了这数列...(不扯了)<br />
斐波那契数列是一个递增的数列，在数学上定义如下： * <span class="math inline">\(F_0 = 0\)</span> * <span class="math inline">\(F_1 = 1\)</span> * <span class="math inline">\(F_n = F_{n-1} + F_{n-2} (n&gt;=2)\)</span></p>
<p>观察其表达式，可以很简单的写出它的递归形式 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> f(n - <span class="number">1</span>) + f(n - <span class="number">2</span>)</span><br></pre></td></tr></table></figure> 使用递归形式来进行计算，对于<span class="math inline">\(f(n-1)\)</span><span class="math inline">\(f(n-2)\)</span><span class="math inline">\(f(n-3)\)</span><span class="math inline">\(f(n-4)\)</span>......均需要一直递归计算到函数出口，即<span class="math inline">\(f(1)\)</span>和<span class="math inline">\(f(2)\)</span>，随着n的增大，计算量将随指数形式增长，时间复杂度为<span class="math inline">\(O(2^n)\)</span>.<br />
如果采用动态规划来计算会怎么样呢。状态转移方程也很简单，直接用递推表达式就可以了：<br />
<span class="math inline">\(d[i] = d[i-1] + d[i-2]\)</span> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DP</span>(<span class="params">n</span>):</span></span><br><span class="line">    dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)] <span class="comment"># 初始化数组</span></span><br><span class="line">    <span class="keyword">if</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span>                  <span class="comment"># 赋初值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">        dp[i] = dp[i - <span class="number">1</span>] + dp[i - <span class="number">2</span>]   <span class="comment"># 递推</span></span><br><span class="line">    <span class="keyword">return</span> dp[n]                        <span class="comment"># 返回最后的值</span></span><br></pre></td></tr></table></figure> 可以看到使用动态规划后的时间复杂度为<span class="math inline">\(O(n)\)</span>. ## 路径数量 对于一个矩形区域的格子场地，将其按下表进行编号，从<span class="math inline">\(0,0\)</span>处开始移动，每次移动只能往左或往右移动一格，求在路径最短的情况下从<span class="math inline">\(0,0\)</span>移动到<span class="math inline">\(i,j\)</span>的不同最短路径的数量。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">...</th>
<th style="text-align: center;">j</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>0</strong></td>
<td style="text-align: center;">0,0</td>
<td style="text-align: center;">0,1</td>
<td style="text-align: center;">0,2</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">0,j</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>1</strong></td>
<td style="text-align: center;">1,0</td>
<td style="text-align: center;">1,1</td>
<td style="text-align: center;">1,2</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">1,j</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>2</strong></td>
<td style="text-align: center;">2,0</td>
<td style="text-align: center;">2,1</td>
<td style="text-align: center;">2,2</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">2,j</td>
</tr>
<tr class="even">
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">...</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>i</strong></td>
<td style="text-align: center;">i,0</td>
<td style="text-align: center;">i,1</td>
<td style="text-align: center;">i,2</td>
<td style="text-align: center;">...</td>
<td style="text-align: center;">i,j</td>
</tr>
</tbody>
</table>
<p>既然要求最短路径，则不能移动过程中就不能越界(行超<span class="math inline">\(i\)</span>或列超<span class="math inline">\(j\)</span>)也不能折返，所以到达<span class="math inline">\((i，j)\)</span>之前的转态必然是在<span class="math inline">\((i-1,j)\)</span>或者<span class="math inline">\((i,j-1)\)</span>，从这两个路径移动到<span class="math inline">\((i，j)\)</span>也只有一种走法，于是可以得到转态转移方程：<br />
<span class="math inline">\(dp[i][j] = dp[i-1][j] + dp[i][j-1]\)</span><br />
还需要设定初值：<br />
<span class="math inline">\(dp[0][0...j] = dp[0...i][0] = 1\)</span><br />
之后就可以通过递推求解了： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DP</span>(<span class="params">row, col</span>):</span></span><br><span class="line">    dp = np.ones((row + <span class="number">1</span>, col + <span class="number">1</span>), dtype=<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, row + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, col + <span class="number">1</span>):</span><br><span class="line">            dp[i][j] = dp[i - <span class="number">1</span>][j] + dp[i][j - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> dp[row][col]</span><br></pre></td></tr></table></figure> 解决此问题的时间复杂度为<span class="math inline">\(O(n^2)\)</span>.</p>
<h2 id="最长递增子序列">最长递增子序列</h2>
<p>给定一个一维的整数数组，求其中的最长的且严格递增的子序列的长度。<br />
例如对于<code>a = [6,1,7,6,2,5,1,8]</code>，其中最长的且严格递增的为<code>[1,2,5,8]</code>所以长度为4，函数应返回4.<br />
在此处，是否可以将状态<span class="math inline">\(dp[i]\)</span>定义为<span class="math inline">\(arr[0]\)</span>至<span class="math inline">\(arr[i]\)</span>的序列中最长严格单调增子序列的长度呢？咋一看好像没什么问题，往后递推的时候加上<span class="math inline">\(arr[i+1]\)</span>再计算<span class="math inline">\(arr[0]\)</span>至<span class="math inline">\(arr[i+1]\)</span>的单调子序列的长度。可是仔细想想我们无法判断<span class="math inline">\(arr[i+1]\)</span>加上去之后最长单调增子序列的长度是否增加，因为我们并不知道原先最长的子序列是拿几个，而且这样长度相等的子序列可能不止一个。那这个问题到底能不能使用动态规划来解决呢？当然是可以的。<br />
我们将状态<span class="math inline">\(dp[i]\)</span>定义为<strong>包含</strong>元素<span class="math inline">\(arr[i]\)</span>在内的最长严格递增子序列的长度。这样在求<span class="math inline">\(dp[i+1]\)</span>的时候，从<span class="math inline">\(arr[0]\)</span>至<span class="math inline">\(arr[i]\)</span>中依次寻找比<span class="math inline">\(arr[i+1]\)</span>小的元素<span class="math inline">\(arr[j]\)</span>，再从这些元素对应的转态<span class="math inline">\(dp[j]\)</span>中找到最大(子序列最长)的一个<span class="math inline">\(max(dp[j])\)</span>，<span class="math inline">\(dp[i+1]\)</span>的值即为<span class="math inline">\(max(dp[j])+1\)</span>.最大长度子序列是数组<span class="math inline">\(dp\)</span>中最大的一个值(<strong>注意:不一定是最后一个值</strong>)。可以写出转态转移方程：<br />
<span class="math inline">\(dp[i] = max(dp[0]...dp[i-1])+1\)</span><br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DP</span>(<span class="params">arr</span>):</span></span><br><span class="line">    dp = [-<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr))]</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">        max_dp = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">            <span class="keyword">if</span> arr[i] &gt; arr[j]:</span><br><span class="line">                <span class="keyword">if</span> max_dp &lt; dp[j]:</span><br><span class="line">                    max_dp = dp[j]</span><br><span class="line">        dp[i] = max_dp + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure> 此方法的时间复杂度为<span class="math inline">\(O(n^2)\)</span>.此问题还可以使用贪心加二分查找的方式将时间复杂度降到<span class="math inline">\(O(nlog(n))\)</span>，此处不再做阐述。</p>
<h2 id="零钱置换">零钱置换</h2>
<p>(leetcode.322)<br />
给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 * <strong>示例 1：</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：coins &#x3D; [1, 2, 5], amount &#x3D; 11</span><br><span class="line">输出：3 </span><br><span class="line">解释：11 &#x3D; 5 + 5 + 1</span><br></pre></td></tr></table></figure> * <strong>示例 2：</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：coins &#x3D; [2], amount &#x3D; 3</span><br><span class="line">输出：-1</span><br></pre></td></tr></table></figure> * <strong>示例 3：</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：coins &#x3D; [1], amount &#x3D; 0</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure> * <strong>示例 4：</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：coins &#x3D; [1], amount &#x3D; 1</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure> * <strong>示例 5：</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：coins &#x3D; [1], amount &#x3D; 2</span><br><span class="line">输出：2</span><br></pre></td></tr></table></figure> 我们采用自下而上的方式进行思考。仍定义 <span class="math inline">\(F(i)\)</span> 为组成金额 <span class="math inline">\(i\)</span> 所需最少的硬币数量，假设在计算 <span class="math inline">\(F(i)\)</span> 之前，我们已经计算出 <span class="math inline">\(F(0)至F(i-1)\)</span>的答案。 则<span class="math inline">\(F(i)\)</span>对应的转移方程应为:<br />
<span class="math inline">\(F(i) = min_{j=0...i-1}F(i-c_j)+1\)</span><br />
其中 <span class="math inline">\(c_j\)</span>代表的是第 <span class="math inline">\(j\)</span> 枚硬币的面值，即我们枚举最后一枚硬币面额是 <span class="math inline">\(c_j\)</span>，那么需要从 <span class="math inline">\(i-c_j\)</span>这个金额的状态 <span class="math inline">\(F(i-c_j)\)</span> 转移过来，再算上枚举的这枚硬币数量 1 的贡献，由于要硬币数量最少，所以 <span class="math inline">\(F(i)\)</span> 为前面能转移过来的状态的最小值加上枚举的硬币数量 1 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DP</span>(<span class="params">coins, amount</span>):</span></span><br><span class="line">    dp = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(amount + <span class="number">1</span>)] <span class="comment"># 求最小值问题，先把转态都设为无穷大</span></span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span> <span class="comment"># 0元仅需要0个硬币</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(amount + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> coin <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">if</span> coin &lt;= i: <span class="comment"># i元必定能从i-coin的转态转移过来</span></span><br><span class="line">                dp[i] = <span class="built_in">min</span>(dp[i], dp[i - coin] + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> dp[amount] <span class="keyword">if</span> dp[amount] != <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>时间复杂度：<span class="math inline">\(O(Sn)\)</span>，其中 <span class="math inline">\(S\)</span> 是金额，<span class="math inline">\(n\)</span> 是面额数。我们一共需要计算 <span class="math inline">\(O(S)\)</span> 个状态，<span class="math inline">\(S\)</span> 为题目所给的总金额。对于每个状态，每次需要枚举 <span class="math inline">\(n\)</span> 个面额来转移状态，所以一共需要 <span class="math inline">\(O(Sn)\)</span> 的时间复杂度。</p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>卡特兰数</title>
    <url>/2021/11/28/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>卡特兰数是一个在组合数学里经常出现的一个数列，对卡特兰数来说，难以抽象出一个具体的意义预知对应，但它却是一个十分常见的数学规律，符合多种问题规律求解。从零开始，卡特兰数的前几项为<code>1,1,2,5,14,42,132,429,1430,4862,16796,58786,208012,742900,2674440,9694845,35357670,129644790…</code></p>
<h1 id="递推公式">递推公式</h1>
<ul>
<li>基础公式:</li>
</ul>
<p><span class="math inline">\(C_{n+1}=C_0C_n+C_1C_{n-1}+...+C_{n-1}C_1+C_nC_0\\C_0=C_1=1\)</span></p>
<p>或者：​​</p>
<p><span class="math display">\[
C_{n+1}=\left\{
\begin{aligned}
&amp;\sum_{i=0}^nC_iC_{n-i},&amp;n&gt;1 \\
&amp;1,&amp;n=0,1
\end{aligned}
\right\}
\]</span></p>
<p>其中C表示Catalan数。</p>
<ul>
<li>组合公式</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
Catalan_n&amp;=C_{2n}^n-C_{2n}^{n+1}\\
&amp;=\frac{(2n)!}{n!∗n!}−\frac{(2n)!}{(n+1)!∗(n−1)!}\\
&amp;=\frac1{n+1}(\frac{(2n)!∗(n+1)}{n!∗n!}−\frac{(2n)!}{n!∗(n−1)!})\\
&amp;=\frac1{n+1}(\frac{(2n)!∗(n+1)}{n!∗n!}−\frac{(2n)!∗n}{n!∗n!})\\
&amp;=\frac1{n+1}∗\frac{(2n)!∗(n+1)−(2n)!∗n}{n!∗n!}\\
&amp;=\frac1{n+1}∗\frac{(2n)!}{n!∗n!}\\
&amp;=\frac1{n+1}C_{2n}^n
\end{aligned}
\]</span></p>
<p>或者: <span class="math display">\[
Catalan_n=\frac{4n+2}{n+2}Catalan_n
\]</span> 推导如下: <span class="math display">\[
\begin{aligned}
Catalan_{n+1}&amp;=\frac1{n+2}C_{2n+2}^{n+1}\\
&amp;\frac1{n+2}∗\frac{(2n+2)!}{(n+1)!∗(n+1)!}\\
&amp;\frac1{n+2}∗\frac{(2n)!∗(2n+1)∗(2n+2)}{n!∗n!∗(n+1)^2}\\
&amp;\frac1{n+2}∗\frac{(2n+1)∗(2n+2)}{(n+1)}∗\frac1{n+1}∗\frac{(2n)!}{n∗n!}\\
&amp;\frac{2(2n+1)}{n+2}∗\frac1{n+1}∗C_{2n}^n\\
&amp;\frac{4n+2}{n+2}Catalan_n
\end{aligned}
\]</span></p>
<p>其中的C代表组合数标记</p>
<h1 id="应用">应用</h1>
<ol type="1">
<li>进出栈问题。将一个数，从操作数序列的头端移到栈的头端（对应数据结构栈的 push 操作）；将一个数，从栈的头端移到输出序列的尾端（对应数据结构栈的 pop 操作），使用这两种操作，由一个操作数序列就可以得到一系列的输出序列，不同的输出序列数有多少种？</li>
<li>有2n个人排成一行进入剧场。入场费 5 元。其中只有n个人有一张 5 元钞票，另外n人只有 10 元钞票，剧院无其它钞票，问有多少中方法使得只要有 10 元的人买票，售票处就有 5 元的钞票找零？</li>
<li>一位大城市的律师在她住所以北n个街区和以东n个街区处工作。每天她走2n个街区去上班。如果他从不穿越（但可以碰到）从家到办公室的对角线，那么有多少条可能的道路？</li>
<li>在圆上选择2n个点，将这些点成对连接起来使得所得到的n条线段不相交的方法数？</li>
<li>对角线不相交的情况下，将一个凸多边形区域分成三角形区域的方法数？</li>
<li>n个结点可构造多少个不同的二叉树？</li>
</ol>
<h2 id="典型进出栈解法">典型进出栈解法</h2>
<p>序列1，2，3，4....n入栈在出栈，考虑n出栈时的位置，可将栈分为在其前面出栈和在其后面出栈两部分，以<span class="math inline">\(C_n\)</span>代表序列长度为n时的不同输出序列的数量。若n为第一个出栈的数，其前面有<span class="math inline">\(C_0\)</span>种输出方式，其后有<span class="math inline">\(C_{n-1}\)</span>种输出方式，n第一个出栈的排列总数为<span class="math inline">\(C_0C_{n-1}\)</span>；若考虑n第k个数出栈，在其前面出栈的数有<span class="math inline">\(C_{k-1}\)</span>​种出栈方式，在其后面出栈的数有<span class="math inline">\(C_{n-k+1}\)</span>种出栈方式，此状态下的输出的总排列数量为<span class="math inline">\(C_{k-1}C_{n-k+1}\)</span>​；<span class="math inline">\(C_n\)</span>​为n在输出序列中不同位置时的所有不同排列数的加和：</p>
<p><span class="math inline">\(C_{n+1}=C_0C_n+C_1C_{n-1}+...+C_{n-1}C_1+C_nC_0\)</span>​</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211128catalan0.png" alt="image-20211128191823776" /><figcaption>image-20211128191823776</figcaption>
</figure>
<ul>
<li>通式解法</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n=<span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">catalan = [<span class="number">0</span>]*(n+<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> n&lt;=<span class="number">1</span>:</span><br><span class="line">    print(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    catalan[<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">    catalan[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,n+<span class="number">1</span>):</span><br><span class="line">        catalan[i] = <span class="built_in">sum</span>([catalan[j]*catalan[i-j-<span class="number">1</span>] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i)])</span><br><span class="line"></span><br><span class="line">    print(catalan[n])</span><br></pre></td></tr></table></figure>
<ul>
<li>变形公式解法</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n=<span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">c = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">    c *= (<span class="number">4</span>*(i-<span class="number">1</span>)+<span class="number">2</span>)/(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">int</span>(c))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>特色算法</tag>
      </tags>
  </entry>
  <entry>
    <title>图床迁移</title>
    <url>/2022/04/02/%E5%9B%BE%E5%BA%8A%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<p>几天不上博客，今天突然发现博客上的图片都不能用了，一经排查，发现是gitee封锁掉了图片外链的功能，无奈之下只能另寻他法。好在gitee上的图片都还能用，暂时先将仓库替换为gitlab使用了。</p>
<p>在gitlab上直接新建仓库将gitee上的导入进来。然而所有的图片外链都变了，需要批量替换一下。可以选择写脚本替换，也可以用编辑器如vscode打开文件夹，使用<code>ctrl+shift+H</code>一键查找替换，将gitee的前缀替换为gitlab新仓库的。</p>
<h1 id="善后工作">善后工作</h1>
<p>原先的使用的是picgo在typora编写时直接上传图片，现在需要重新设置picgo中的图床来将图片上传到我们的新图床。与gitee中的操作类似。在gitlab中建立一个token令牌，记录下来备用。</p>
<p>在picgo中安装gitlab插件：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/pictures/2022/04/2_20_28_53_20220402gitlabphotos1.png" /></p>
<p>配置新的gitlab图床：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/pictures/2022/04/2_20_30_32_20220402gitlabphotos2.png" /></p>
<p>项目id在项目首页可以找到，token填入刚才新创建的token其他自选就可以了。</p>
<hr />
<p>只需稍微操作一下就可以像以前一样便捷的使用了。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>外链图床(PicGo+Gitee)</title>
    <url>/2021/04/06/%E5%A4%96%E9%93%BE%E5%9B%BE%E5%BA%8A-PicGo-Gitee/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p>刚开始写建立博客的时候，头疼过图片该如何处理：</p>
<ul>
<li>直接放github的仓库里吧。可是随着图片越来越多，每次pull、clone都需要耗费大量的时间。而且github的仓库加载很慢，影响博客的流畅性。</li>
<li>外链图床吧。可是国内基本上没有什么好用的免费图床，七牛云之类的都需要收费的。国外的图床加载速度又是特别的慢。</li>
<li>自己搭建。</li>
</ul>
<p>当初的自己为了偷懒，选择了不在博客中放图片——没错，就是写纯文本的内容。可是终究是禁不起时间这把杀猪刀的折磨。图片是一种极具表达力的东西，仅仅插入一张图片就可以代替使用大量文字描述而且还描述不清晰的地方。</p>
<p>在一篇文章中图形可使文章不限枯燥，可以使表达更具体。现在的我，已受够没有图片的枯燥码字(效率低、煎熬)，于是搭建了自己的一个图床。 ***</p>
<h1 id="图床搭建">图床搭建</h1>
<p>所使用的工具为<strong>PicGo</strong>、<strong>Gitee</strong>、<strong>Typora</strong>. 在这之前，我一般在<strong>VSCode</strong>上面写<strong>Markdown</strong>文档，但是现在开始添加图片到博客中去，转而使用Typora，以便能够更好的契合PicGo使用(插入直接上传)，而VSCode版本的PicGo插件暂时不支持Gitee图床的插件。</p>
<h2 id="工具下载">工具下载</h2>
<ul>
<li>PicGo: <a href="https://molunerfinn.com/PicGo/" class="uri">https://molunerfinn.com/PicGo/</a></li>
<li>Tyora: <a href="https://www.typora.io/" class="uri">https://www.typora.io/</a></li>
</ul>
<h2 id="gitee端配置">Gitee端配置</h2>
<p>在<a href="https://gitee.com/">Gitee</a>上面注册一个账号，新建立一个仓库，在个人中心的设置中开启一个私人密匙(token). 此密匙需记录好，离开页面后就会消失，不会再次出现，只能重新生成。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210406token.png" alt="image-20210406204053178" /> ***</p>
<h2 id="picgo端配置">PicGo端配置</h2>
<p>现在打开安装好的PicGo, 默认的图床中是没有gitee图床的，需要安装插件。在插件选项中搜索gitee，安装好gitee图床插件。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210406PicGo1.png" alt="image-20210406204506519" /> *** 现在需要对gitee图床进行配置，使它和gitee的服务端对接起来：</p>
<ul>
<li>repo填写仓库名称；</li>
<li>branch填写master分支，默认即可；</li>
<li>token填写刚才生成的密匙；</li>
<li>path自定义填写，既然用作图床，文件夹名就命名为img；</li>
<li>其他的可不给予填写，留空即可。</li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210406PicGo2.png" alt="image-20210406205014650" /> ***</p>
<p>此外，PicGo还有其他的一些设置可按自己喜好配置，推荐将上传前重命名选上，方便为每张图做自己的标识(但是每次上传会弹出重命名窗口，效率会率低，不喜欢可以关掉)，时间戳重命名可以有效避免文件的冲突，也推荐选上。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210406PicGo3.png" alt="image-20210406205215384" /> ***</p>
<h2 id="typora配置">Typora配置</h2>
<p>在Typora中打开文件 ---&gt;偏好设置(Ctrl+逗号)。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210406Typora1.png" alt="image-20210406205534526" /> ***</p>
<p>在偏好设置中将图像中的插入图片时自动上传图片启用，然后上传服务配置为PicGo.</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210406Typora2.png" alt="image-20210406205827909" /> ***</p>
<h1 id="尾">尾</h1>
<p>至此，一个私人图床就搭建完成了，简单而高效。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title>多重背包</title>
    <url>/2021/09/08/%E5%A4%9A%E9%87%8D%E8%83%8C%E5%8C%85/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>在0-1背包中，每个物品只有一个，要么拿，要么不拿；完全背包的每种物品的数量是无限的，可以重复装；而在多重背包中，物品<strong>可以重复装</strong>，但是每种物品都有着<strong>数量限制</strong>，此种物品装完后即使背包有容量也不可以再装了。</p>
<hr />
<h1 id="解法">解法</h1>
<p>设有以下几种物品：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">物品(i)</th>
<th style="text-align: center;">重量(<span class="math inline">\(w_i\)</span>)</th>
<th style="text-align: center;">价值(<span class="math inline">\(v_i\)</span>)</th>
<th style="text-align: center;">数量(<span class="math inline">\(num_i\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>物品1</strong></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品2</strong></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品3</strong></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品4</strong></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>多重背包与完全背包的解法相似，只是多了一个限制条件，即物品数量有限。可以将物品展开当成0-1背包来计算，也可以按照多重背包的解法二来计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">v = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">num = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">w_a = <span class="number">9</span>  <span class="comment"># 背包容量</span></span><br><span class="line">n = <span class="built_in">len</span>(w)</span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span>] * (w_a + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]  <span class="comment"># 初始化动态规划数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):  <span class="comment"># 状态转移</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w_a + <span class="number">1</span>):</span><br><span class="line">        dp[i][j] = dp[i - <span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">min</span>(j // w[i - <span class="number">1</span>], num[i - <span class="number">1</span>]) + <span class="number">1</span>): <span class="comment"># 最大数量和最大容量中的最小值</span></span><br><span class="line">            dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j - k * w[i - <span class="number">1</span>]] + k * v[i - <span class="number">1</span>], dp[i][j])</span><br><span class="line"><span class="comment"># 最终结果</span></span><br><span class="line">print(dp[-<span class="number">1</span>][-<span class="number">1</span>])</span><br><span class="line">[out]: <span class="number">15</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 状态转移结果</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">14</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">15</span>]</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="end">END</h1>
<p><a href="https://www.xiubenwu.top/categories/笔记/算法/背包问题/">其他背包问题</a></p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>背包问题</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>完全背包</title>
    <url>/2021/09/08/%E5%AE%8C%E5%85%A8%E8%83%8C%E5%8C%85/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>在0-1背包中，每个物品只有一个，要么拿，要么不拿。完全背包与0-1背包的不同点在于，完全背包的每种物品的数量是无限的，可以重复装，也就是说只要背包容量够，每种物品可以选择不装、装一个、装两个......装n个。求这种情况下一定容量下的背包可以装下的物品的最大价值。依旧采用动态规划求解。</p>
<hr />
<h1 id="解法">解法</h1>
<p>设有以下几种物品：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">物品(i)</th>
<th>重量(<span class="math inline">\(w_i\)</span>)</th>
<th>价值(<span class="math inline">\(v_i\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>物品0</strong></td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品1</strong></td>
<td>3</td>
<td>4</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品2</strong></td>
<td>4</td>
<td>5</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品3</strong></td>
<td>5</td>
<td>6</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品4</strong></td>
<td>6</td>
<td>7</td>
</tr>
</tbody>
</table>
<p>背包总容量(<span class="math inline">\(w_a\)</span>​)为9，如何选择物品才能让背包的能够装的价值最大？</p>
<h2 id="解法一">解法一</h2>
<p>虽然物品有限，但是背包的容量是给定的，也就是说，对于每种物品，其最多可以装<span class="math inline">\(floor(w_a/w_i)\)</span>​​个，因此我们可以把每种物品展开，对于展开后的物品，就可以当做0-1背包问题来解决：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">物品(i)</th>
<th>重量(<span class="math inline">\(w_i\)</span>)</th>
<th>价值(<span class="math inline">\(v_i\)</span>)</th>
<th>可展开数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>物品0</strong></td>
<td>2</td>
<td>3</td>
<td>9//2=4</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品1</strong></td>
<td>3</td>
<td>4</td>
<td>9//3=3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品2</strong></td>
<td>4</td>
<td>5</td>
<td>9//4=2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品3</strong></td>
<td>5</td>
<td>6</td>
<td>9//5=1</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品4</strong></td>
<td>6</td>
<td>7</td>
<td>9//6=1</td>
</tr>
</tbody>
</table>
<p>展开后物品情况:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">物品(i)</th>
<th>重量(<span class="math inline">\(w_i\)</span>)</th>
<th>价值(<span class="math inline">\(v_i\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>物品0-1</strong></td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品0-2</strong></td>
<td>2</td>
<td>3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品0-3</strong></td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品0-4</strong></td>
<td>2</td>
<td>3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品1-1</strong></td>
<td>3</td>
<td>4</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品1-2</strong></td>
<td>3</td>
<td>4</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品1-3</strong></td>
<td>3</td>
<td>4</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品2-1</strong></td>
<td>4</td>
<td>5</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品2-2</strong></td>
<td>4</td>
<td>5</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>物品3</strong></td>
<td>5</td>
<td>6</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>物品4</strong></td>
<td>6</td>
<td>7</td>
</tr>
</tbody>
</table>
<p>如此一来，就可以当做0-1背包来求解了，但是这样做的话会大量消耗而外的空间时间复杂度。</p>
<h2 id="解法二">解法二</h2>
<p>在0-1背包中，我们用<span class="math inline">\(dp[i][j]\)</span>​​来表示仅从前<strong>i</strong>个物品里面选择且背包总容量为<strong>j</strong>时物品的最大价值。对于0-1背包来说，第<span class="math inline">\(i\)</span>个物品只有选或者不选两种情况，而完全背包面临着第<span class="math inline">\(i\)</span>​个物品不选、选一个、选两个....等情况。因此状态转移方程改为如下:</p>
<p><span class="math inline">\(dp[i][j] = max(dp[i][j-k*w_i]+v_i | 0&lt;k&lt;=j//w_a,dp[i-1][j])\)</span></p>
<p>此时，对于每一个<span class="math inline">\(dp[i][j]\)</span>，需要比较装不同个<span class="math inline">\(w_a\)</span>时的最大值，示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化重量、价值</span></span><br><span class="line">w = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">6</span>))</span><br><span class="line">v = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">3</span>, <span class="number">7</span>))</span><br><span class="line">n = <span class="built_in">len</span>(w)</span><br><span class="line"></span><br><span class="line">w_a = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化状态</span></span><br><span class="line">dp = [[<span class="number">0</span>] * (w_a + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"><span class="comment"># 转态初值设置</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(w_a + <span class="number">1</span>):</span><br><span class="line">    dp[<span class="number">0</span>][i] = i // w[<span class="number">0</span>] * v[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w_a + <span class="number">1</span>): <span class="comment"># 重量需要取到w_a</span></span><br><span class="line">        dp[i][j] = dp[i - <span class="number">1</span>][j] <span class="comment"># 若不装</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, j // w[i] + <span class="number">1</span>): <span class="comment"># 装不同个的最大值</span></span><br><span class="line">            dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j - k * w[i]] + k * v[i], dp[i][j])</span><br><span class="line"><span class="comment"># 最终结果</span></span><br><span class="line">print(dp[-<span class="number">1</span>][w_a])</span><br><span class="line">[out]: <span class="number">13</span></span><br><span class="line"></span><br><span class="line">转态转移结果</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">12</span>, <span class="number">12</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">13</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">13</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">13</span>]</span><br></pre></td></tr></table></figure>
<h2 id="解法三">解法三</h2>
<p>多重背包中当前装入第<span class="math inline">\(i\)</span>​个物品的数量会影响之后此物品是否能再次装入背包，因此转态转移方程可以写成:</p>
<p><span class="math inline">\(dp[i][j] = max(dp[i][j-w[i]]+v[i],dp[i-1][j])\)</span>​</p>
<p>此状态方程与0-1背包的相似，不同点在于0-1背包是从前一个物品转移，完全背包是从当前物品转移。依据状态方程可以写出示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">6</span>))</span><br><span class="line">v = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">3</span>, <span class="number">7</span>))</span><br><span class="line"><span class="comment"># 倒序，增加数据多样性</span></span><br><span class="line">w.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">v.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">n = <span class="built_in">len</span>(w)</span><br><span class="line"></span><br><span class="line">w_a = <span class="number">9</span></span><br><span class="line"></span><br><span class="line">dp = [[<span class="number">0</span>] * (w_a + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(w_a + <span class="number">1</span>):</span><br><span class="line">    dp[<span class="number">0</span>][i] = i // w[<span class="number">0</span>] * v[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, w_a + <span class="number">1</span>):</span><br><span class="line">        dp[i][j] = dp[i - <span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">if</span> j - w[i] &gt;= <span class="number">0</span>:</span><br><span class="line">            dp[i][j] = <span class="built_in">max</span>(dp[i][j - w[i]] + v[i], dp[i][j])</span><br><span class="line"><span class="comment"># 最终结果</span></span><br><span class="line">print(dp[-<span class="number">1</span>][w_a])</span><br><span class="line">[out]:<span class="number">13</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 状态转移结果</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">11</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">13</span>]</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="end">END</h1>
<p><a href="https://www.xiubenwu.top/categories/笔记/算法/背包问题/">其他背包问题</a></p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>背包问题</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>巧解有效括号</title>
    <url>/2021/09/12/%E5%B7%A7%E8%A7%A3%E6%9C%89%E6%95%88%E6%8B%AC%E5%8F%B7/</url>
    <content><![CDATA[<p>给定一个只包含三种字符的字符串：（ ，） 和 *，写一个函数来检验这个字符串是否为有效字符串。有效字符串具有如下规则：</p>
<span id="more"></span>
<ol type="1">
<li><strong>任何左括号 <code>(</code> 必须有相应的右括号<code>)</code>。</strong></li>
<li><strong>任何右括号 <code>)</code> 必须有相应的左括号 <code>(</code> 。</strong></li>
<li><p><strong>左括号 <code>(</code> 必须在对应的右括号之前 <code>)</code>。</strong></p></li>
<li><strong><code>*</code>可以被视为单个右括号<code>)</code> ，或单个左括号 <code>(</code> ，或一个空字符串。</strong></li>
<li><p><strong>一个空字符串也被视为有效字符串。</strong></p></li>
</ol>
<p>(Leetcode.678)</p>
<p>测试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;()&quot;</span><br><span class="line">&quot;(*)&quot;</span><br><span class="line">&quot;(*))&quot;</span><br><span class="line">&quot;**()()&quot;</span><br><span class="line">&quot;*&quot;</span><br><span class="line">&quot;(&quot;</span><br><span class="line">&quot;)&quot;</span><br><span class="line">&quot;()(*)*))&quot;</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">true</span><br><span class="line">true</span><br><span class="line">true</span><br><span class="line">true</span><br><span class="line">true</span><br><span class="line">false</span><br><span class="line">false</span><br><span class="line">true</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="解法一栈">解法一(栈)</h1>
<p>使用两个栈来分别存放左括号<code>(</code>和<code>*</code>，基本步骤如下：</p>
<ul>
<li>遇到左括号时，存放左括号的栈此左括号的<strong>索引下标入栈</strong>；</li>
<li>遇到星号时，存放星号的栈将此星号的<strong>索引下标入栈</strong>；</li>
<li>遇到右括号时，<strong>优先弹出左括号</strong>的栈顶元素；</li>
<li>如果左括号栈为空，弹出星号的栈顶元素；</li>
<li>如果星号的栈也为空，而此时还有新的右括号，说明不能构成有效的括号对；</li>
<li>如果字符串遍历完后左括号的栈不为空，则需要取星号栈中的星号来当做右括号与其配对，此时需要注意，<strong>星号栈中的索引值必须大于左括号栈中的元素的索引值；</strong></li>
<li>如果最终可以将所用左括号配对完毕，则是一个有效的字符括号对。</li>
</ul>
<p>在存取的时候要注意将左括号与星号的索引值入栈，在右括号匹配的时候，因为栈中的元素索引是严格小于当前元素的索引的，此时右括号一定在所有元素的右边，所以不必要对比索引大小，直接按照优先级进行出栈配对即可。而最终在对比左括号和星号的配对情况的时候，星号如果出现在左括号的左边，则不能组合成一个有效的括号对，所以需要额外对比他们的索引值。</p>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">checkValidString</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; bool:</span></span><br><span class="line">        stack1 = [] <span class="comment"># 存放星号索引</span></span><br><span class="line">        stack2 = [] <span class="comment"># 存放左括号索引</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            <span class="keyword">if</span> s[i] == <span class="string">&#x27;(&#x27;</span>:</span><br><span class="line">                stack1.append(i) <span class="comment"># (</span></span><br><span class="line">            <span class="keyword">elif</span> s[i]==<span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">                stack2.append(i) <span class="comment"># *</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> stack1: <span class="comment"># 优先弹出左括号</span></span><br><span class="line">                    stack1.pop()</span><br><span class="line">                <span class="keyword">elif</span> stack2:</span><br><span class="line">                    stack2.pop()</span><br><span class="line">                <span class="keyword">else</span>: <span class="comment"># 两者皆空，右括号超标，直接返回</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> stack1==[]:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 当左括号还有剩余时</span></span><br><span class="line">            <span class="keyword">while</span> stack1:</span><br><span class="line">                <span class="keyword">if</span> stack2==[]: <span class="comment"># 没有足够的星号，直接返回</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                t1 = stack1.pop()</span><br><span class="line">                t2 = stack2.pop()</span><br><span class="line">                <span class="keyword">if</span> t2&gt;t1: <span class="comment"># 星号的索引必须大于左括号的索引</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<hr />
<h1 id="解法二贪心">解法二(贪心)</h1>
<p>采用两个变量<code>left_max,left_min</code>记录<strong>未配对的左括号的最大值和最小值</strong>，</p>
<ul>
<li>当遇到左括号时，最大值(<code>left_max</code>)加一，最小值(<code>left_min</code>)加一；</li>
<li>当遇到右括号时，最大值减一，最小值减一；</li>
<li>当遇到星号时，最大值加一，最小值减一；</li>
<li>当最小值小于0时，将其赋0，保持非负；</li>
<li>当最大值小于0时，构成无效括号对，直接返回；</li>
<li>遍历完成后左括号的数量为0说明括号对有效；</li>
</ul>
<p>遇到左括号和右括号的计算方式很好理解，直接严格加减一就可以了，遇到星号由于其可以代表左括号。右括号和普通字符串，所以最大值加一、最小值减一；在遍历过程中如果需要将最小值设置为非负，这是要保证未匹配的左括号总是大于等于0的；当最大值也降到0以下，则说明无论如何改变星号也不能弥补左括号的不足，所以返回False.</p>
<p>示例代码:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">checkValidString</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; bool:</span></span><br><span class="line">        left_max = <span class="number">0</span></span><br><span class="line">        left_min = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> c==<span class="string">&#x27;(&#x27;</span>:</span><br><span class="line">                left_max += <span class="number">1</span></span><br><span class="line">                left_min += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> c==<span class="string">&quot;)&quot;</span>:</span><br><span class="line">                left_max -= <span class="number">1</span></span><br><span class="line">                left_min -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left_max += <span class="number">1</span></span><br><span class="line">                left_min -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> left_max&lt;<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            left_min = <span class="built_in">max</span>(left_min,<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> left_min</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>题录</category>
      </categories>
      <tags>
        <tag>题录</tag>
      </tags>
  </entry>
  <entry>
    <title>差分数组</title>
    <url>/2021/07/23/%E5%B7%AE%E5%88%86%E6%95%B0%E7%BB%84/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>差分数组是一种对频繁修改区间数据（频繁对一个区间中的每一个元素进行修改）问题的一种优化时间复杂度的操作（空间换时间）。</p>
<p>假如有一个很大的数据<code>a.size = 1e+999</code>，需要多次对一个特定的区间<code>[left,right]</code>中的所有数据加上或减去一个固定的数值。一个通常的做法是进行模拟，依次对<code>a[left]~a[right]</code>中的每一个数据进行增加或删除操作，其时间复杂度为<span class="math inline">\(O(n*max(right-left))\)</span>，其中<span class="math inline">\(n\)</span>为操作次数。若采用差分数组，其时间复杂度为<span class="math inline">\(O(n+l)\)</span>，其中<span class="math inline">\(n\)</span>为查询次数，<span class="math inline">\(l\)</span>为差分数组长度。以下为其主要思想。</p>
<hr />
<h1 id="method">Method</h1>
<ul>
<li><code>a</code>为初始未进行任何操作的数组；</li>
<li>建立差分数组<code>diff</code>，其尺寸与<code>a</code>一致，<code>diff[0]=a[0];diff[i]=a[i]-a[i-1]</code>，即记录<code>a</code>中相邻元素的差值；</li>
<li>当对<code>a</code>中的区间<code>[left,right]</code>进行一次修改操作时(例如增加<code>x</code>)，仅需<code>diff[left]+=x;diff[right+1]-=x</code>，<code>diff</code>中的其他数据不需要进行变动，因为对整个区间做修改仅仅会影响到边界的差值，其他位置的差值保持不变，每一次修改的时间复杂度为<span class="math inline">\(O(1)\)</span>；</li>
<li>多次修改操作后需要将差分数组变换成原数组修改后的数组，根据差分数组的性质，直接求差分数组的前缀和即可得到修改后的数组。</li>
</ul>
<p>简单差分示例：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210723diffarr.png" alt="image-20210723203808006" /><figcaption>image-20210723203808006</figcaption>
</figure>
<hr />
<h1 id="例子">例子</h1>
<p><strong>(取自leetcode1893)</strong></p>
<p>给你一个二维整数数组 ranges 和两个整数 <code>left</code>和 <code>right</code> 。每个<code>ranges[i] = [starti, endi]</code> 表示一个从 <code>starti</code>到<code>endi</code>的 闭区间 。</p>
<p>如果闭区间<code>[left, right]</code>内每个整数都被<code>ranges</code>中 至少一个 区间覆盖，那么请你返回<code>true</code> ，否则返回 <code>false</code> 。</p>
<p>已知区间 <code>ranges[i] = [starti, endi]</code> ，如果整数<code>x</code> 满足<code>starti &lt;= x &lt;= endi</code> ，那么我们称整数x 被覆盖了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例 1：</span><br><span class="line"></span><br><span class="line">输入：ranges &#x3D; [[1,2],[3,4],[5,6]], left &#x3D; 2, right &#x3D; 5</span><br><span class="line">输出：true</span><br><span class="line">解释：2 到 5 的每个整数都被覆盖了：</span><br><span class="line"></span><br><span class="line">- 2 被第一个区间覆盖。</span><br><span class="line">- 3 和 4 被第二个区间覆盖。</span><br><span class="line">- 5 被第三个区间覆盖。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- 示例 2：</span><br><span class="line"></span><br><span class="line">输入：ranges &#x3D; [[1,10],[10,20]], left &#x3D; 21, right &#x3D; 21</span><br><span class="line">输出：false</span><br><span class="line">解释：21 没有被任何一个区间覆盖。</span><br></pre></td></tr></table></figure>
<p>此外，题中限定了输入数据的范围：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 &lt;&#x3D; ranges.length &lt;&#x3D; 50</span><br><span class="line">1 &lt;&#x3D; starti &lt;&#x3D; endi &lt;&#x3D; 50</span><br><span class="line">1 &lt;&#x3D; left &lt;&#x3D; right &lt;&#x3D; 50</span><br></pre></td></tr></table></figure>
<p>我们可以据此方便的建立差分数组。</p>
<h2 id="暴力解法">暴力解法</h2>
<p>建立一个数组，0代表没有覆盖，1代表有覆盖，遍历ranges中的区间，将a中的可覆盖的索引处都置1，然后遍历a中的<code>[left,right]</code>区间查看是否有0，有则未全覆盖，无则全覆盖。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isCovered</span>(<span class="params">self, ranges: List[List[<span class="built_in">int</span>]], left: <span class="built_in">int</span>, right: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        a = [<span class="number">0</span>]*<span class="number">52</span> <span class="comment"># 建立数组尺寸稍大，防止越界</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> ranges: <span class="comment"># 遍历每一个区间</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(r[<span class="number">0</span>],r[<span class="number">1</span>]+<span class="number">1</span>): <span class="comment"># 遍历每一个区间中的每一个索引</span></span><br><span class="line">                a[i]=<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left,right+<span class="number">1</span>): <span class="comment"># 对目标区间进行检查</span></span><br><span class="line">            <span class="keyword">if</span> a[i]==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h2 id="差分数组解法">差分数组解法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isCovered</span>(<span class="params">self, ranges: List[List[<span class="built_in">int</span>]], left: <span class="built_in">int</span>, right: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        diff = [<span class="number">0</span>]*<span class="number">52</span> <span class="comment"># 建立差分数组</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> ranges: <span class="comment"># 对差分数组进行修改操作，记录每个索引被区间覆盖的次数</span></span><br><span class="line">            diff[r[<span class="number">0</span>]] += <span class="number">1</span></span><br><span class="line">            diff[r[<span class="number">1</span>]+<span class="number">1</span>] -= <span class="number">1</span></span><br><span class="line">        prefix = <span class="number">0</span> <span class="comment"># 未建立前缀和数组，节约空间</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(right+<span class="number">1</span>): <span class="comment"># 还原每个索引处的覆盖次数</span></span><br><span class="line">            prefix += diff[i]</span><br><span class="line">            <span class="keyword">if</span> prefix&lt;=<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> i&gt;=left: <span class="comment"># 在目标区间中发现覆盖次数小于1（即未被覆盖），直接返回False</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>特色算法</tag>
      </tags>
  </entry>
  <entry>
    <title>幂指模运算特别方式</title>
    <url>/2021/05/05/%E5%B9%82%E6%8C%87%E6%A8%A1%E8%BF%90%E7%AE%97%E7%89%B9%E5%88%AB%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<ul>
<li>给定一个数<span class="math inline">\(a\)</span>，求它对<span class="math inline">\(m\)</span>取模的结果，该如何计算？</li>
<li>给定一个数<span class="math inline">\(a^b\)</span>，求它对<span class="math inline">\(m\)</span>取模的结果，该如何计算？</li>
<li>给定一个数<span class="math inline">\(a^{b}\)</span>，其中<span class="math inline">\(b\)</span>以数组的形式给出<span class="math inline">\(b=[b_1,b_2,b_3,...,b_n]\)</span>，此时乘积会很大，数值类型将会溢出，如何计算其对<span class="math inline">\(m\)</span>取模的值？</li>
</ul>
<hr />
<h1 id="方法">方法</h1>
<h2 id="数组拆分">数组拆分</h2>
<p>幂的计算可以写成递归的形式：<br />
<span class="math inline">\(a^{[b_1,b_2,...,b_n]}=a^{b_n}*{(a^{[b_1,b_2,...,b_{n-1}]})}^{10}\)</span></p>
<h2 id="取模转换">取模转换</h2>
<p>俩数相乘取模的结果分别为俩数取模结果的乘积在取模。<br />
<span class="math inline">\((A * B) \% k = (A \% k)(B \% k) \% k\)</span><br />
假设：</p>
<p><span class="math inline">\(A = a_1k+a_2;B = b_1k+b_2\)</span></p>
<p><span class="math inline">\((A*B) \% k = (a_1k+a_2)(b_1k+b_2) \% k\)</span></p>
<p><span class="math inline">\(=(a_1b_1k^2+(a_1b_2+b_1a_2)k+a_2b_2) \% k\)</span></p>
<p>$=(a_2*b_2) %k $</p>
<p>再有：</p>
<p><span class="math inline">\(A \% k = a_2; B \% k = b_2\)</span></p>
<p>所以：</p>
<p><span class="math inline">\(=(a_2*b_2) \% k = (a_2 \% k)(b_2 \% k) \% k\)</span></p>
<p>初式得证。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>弗洛伊德法</title>
    <url>/2021/09/13/%E5%BC%97%E6%B4%9B%E4%BC%8A%E5%BE%B7%E6%B3%95/</url>
    <content><![CDATA[<p><strong>弗洛伊德（Floyd）</strong>算法是解决任意两点间的最短路径的一种算法，适用有向图（含负权值）但不可用于存在负权环路的图（因为每过一次负权值环路路劲长度就会减小）。此算法时间复杂度为<span class="math inline">\(O(n^3)\)</span>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始距离</span></span><br><span class="line">d = [[<span class="number">0</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>],</span><br><span class="line">     [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>), <span class="number">0</span>, <span class="number">3</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)],</span><br><span class="line">     [<span class="number">7</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>), <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">5</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>), <span class="number">12</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            d[i][j] = <span class="built_in">min</span>(d[i][k]+d[k][j],d[i][j])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最短距离</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> d:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
<p>注意：对中间节点k的遍历应当保证在最外层循环，以正确更新每个点间的最小距离。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>特色算法</tag>
      </tags>
  </entry>
  <entry>
    <title>得到子序列的最少操作次数</title>
    <url>/2021/07/27/%E5%BE%97%E5%88%B0%E5%AD%90%E5%BA%8F%E5%88%97%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%AC%A1%E6%95%B0/</url>
    <content><![CDATA[<p>给你一个数组<code>target</code> ，包含若干互不相同的整数，以及另一个整数数组 <code>arr</code> ，<code>arr</code>可能包含重复元素。</p>
<p>每一次操作中，你可以在 <code>arr</code>的任意位置插入任一整数。比方说，如果 <code>arr = [1,4,1,2]</code>，那么你可以在中间添加 3 得到<code>[1,4,3,1,2]</code>。你可以在数组最开始或最后面添加整数。</p>
<p>请你返回最少操作次数，使得<code>target</code>成为 <code>arr</code>的一个子序列。</p>
<span id="more"></span>
<p>一个数组的子序列指的是删除原数组的某些元素（可能一个元素都不删除），同时不改变其余元素的相对顺序得到的数组。比方说，<code>[2,7,4]</code> 是<code>[4,2,3,7,2,1,4]</code>的子序列，但 <code>[2,4,2]</code> 不是子序列。</p>
<p><strong>LeetCode1713</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例 1：</span><br><span class="line"></span><br><span class="line">输入：target &#x3D; [5,1,3], arr &#x3D; [9,4,2,3,4]</span><br><span class="line">输出：2</span><br><span class="line">解释：你可以添加 5 和 1 ，使得 arr 变为 [5,9,4,1,2,3,4] ，target 为 arr 的子序列。</span><br><span class="line">示例 2：</span><br><span class="line"></span><br><span class="line">输入：target &#x3D; [6,4,8,1,3,2], arr &#x3D; [4,7,6,2,3,8,6,1]</span><br><span class="line">输出：3</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr />
<h1 id="解析">解析</h1>
<p>要求把一个数组变成另一个数组的子序列的最少操作次数，实际上可以转化为求两个数组的最长公共子序列问题。通常的最长公共子序列问题可以采用动态规划来解决：</p>
<p><span class="math display">\[
dp[i][j]=\begin{cases}dp[i-1][j-1]+1,a_1[i-1]=a_2[j-1]\\ max(dp[i-1][j],dp[i][j-1]),a_1[i-1]\neq a_2[j-1] \end{cases}
\]</span></p>
<p>此时时间复杂度为<span class="math inline">\(O(n*m)\)</span>​​ ,简要代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span>(<span class="params">text1: <span class="built_in">str</span>, text2: <span class="built_in">str</span></span>) -&gt; int:</span></span><br><span class="line">        m, n = <span class="built_in">len</span>(text1), <span class="built_in">len</span>(text2)</span><br><span class="line">        dp = [[<span class="number">0</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text1[i - <span class="number">1</span>] == text2[j - <span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="进阶方法">进阶方法</h1>
<p>有没有一种能够再次缩减时间复杂度的方式呢？当然是有的。注意到题中所给的条件，<code>target</code>中的元素是互不相同的，所以可以说它们和它们的下标是一一对应的。于是我们可以遍历<code>arr</code>数组，当其中有与<code>target</code>相等的元素时，用<code>target</code>中相应元素的下标索引来作为当前值，没有时跳过，这样可以得到一个新的数组，这样的数组中的最大递增子序列的长度就是两个数组的最大公共子序列的长度。(由于新数组由<code>target</code>的索引组成，作为<code>target</code>的子序列，新数组必定递增)。原来的问题就转化为一个求最大递增子序列的长度的问题。将最大递增子序列的长度求出来后用<code>target</code>的长度减去这个最大递增子序列的长度就可以得到最少的操作次数。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210726LSC2LIS.png" alt="image-20210726211253413" /><figcaption>image-20210726211253413</figcaption>
</figure>
<p>我们可以从此得出一个结论：<strong>当其中一个数组元素各不相同时，最长公共子序列问题（LCS）可以转换为最长上升子序列问题（LIS）进行求解。</strong></p>
<p>而数组的最长递增子序列问题我们朴素的解法也是用动态规划求解： <span class="math inline">\(dp[i] = max(dp[i], dp[j] + 1),0≤j&lt;i\)</span></p>
<p>其时间复杂度为<span class="math inline">\(O(n^2)\)</span>，如此一来时间复杂度上没有任何优势，当然不能用此解法。这里需要用一种<strong>贪心+二分查找</strong>的方式来优化时间复杂度。</p>
<p>考虑一个简单的贪心，如果我们要使上升子序列尽可能的长，则我们需要让序列上升得尽可能慢，因此我们希望每次在上升子序列最后加上的那个数尽可能的小。因此我们可以维护一个数组<code>minEnd</code>，<code>minEnd[i-1]</code>表示子序列长度为<code>i</code>时，其为<strong>最小的结尾元素</strong>。遍历<code>arr</code>数组，当<code>arr[i]&gt;minEnd[end]</code>时，将<code>arr[i]</code>添加到<code>minEnd</code>的末尾，如果<code>arr[i]&lt;minEnd[end]</code>则采用二分法将其插入到<code>minEnd</code>中合适的位置(<code>arr[i]&gt;minEnd[x]则minEnd[x+1]=arr[i]</code>)。最终最大递增子序列的长度即为<code>minEnd</code>数组的长度。此方式的时间复杂度为<span class="math inline">\(O(m+nlog(n))\)</span>，其中<span class="math inline">\(m\)</span>是<code>target</code>数组的长度，<span class="math inline">\(n\)</span>​是<code>arr</code>数组的长度。实现实例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minOperations</span>(<span class="params">target, arr</span>) -&gt; int:</span></span><br><span class="line">    hashSet = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(target)): <span class="comment"># target数组转化为索引</span></span><br><span class="line">        <span class="keyword">if</span> target[i] <span class="keyword">not</span> <span class="keyword">in</span> hashSet:</span><br><span class="line">            hashSet[target[i]] = i</span><br><span class="line"></span><br><span class="line">    lift_a = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr: <span class="comment"># 以索引值更新arr数组</span></span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> hashSet:</span><br><span class="line">            lift_a.append(hashSet[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> lift_a == []:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(target)</span><br><span class="line">    d = []</span><br><span class="line">    d.append(lift_a[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(lift_a)): <span class="comment"># 二分查找更新</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="built_in">len</span>(d) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> d[-<span class="number">1</span>] &lt; lift_a[i]:</span><br><span class="line">            d.append(lift_a[i])</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            middle = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> d[middle] &gt;= lift_a[i]:</span><br><span class="line">                right = middle</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = middle + <span class="number">1</span></span><br><span class="line">        d[right] = lift_a[i]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(target) - <span class="built_in">len</span>(d)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    print(minOperations([<span class="number">6</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line">    <span class="number">3</span> <span class="comment"># 最少需要操作三次</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>题录</category>
      </categories>
      <tags>
        <tag>题录</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——冒泡</title>
    <url>/2021/04/24/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%86%92%E6%B3%A1/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p><strong>冒泡排序</strong>(Bubble Sort)就是元素两两比较，大的放后面，相等则不理会。交换完一次序列后最大的元素在序列尾部，第二次遍历时可以不予理会。反复遍历直到所有数排序完成。冒牌排序的最大时间复杂度为<span class="math inline">\(O(n^2)\)</span>，即所有元素以反序排列时。 冒泡排序中，最大元素一个个漂浮到序列尾部，如同水泡冒起，因而得名。</p>
<hr />
<h1 id="实现">实现</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BubbleSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">44</span>, <span class="number">45</span>, <span class="number">23</span>, <span class="number">67</span>, <span class="number">21</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span> t;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = a.length; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i - <span class="number">1</span>; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (a[j] &gt; a[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                    t = a[j];</span><br><span class="line">                    a[j] = a[j + <span class="number">1</span>];</span><br><span class="line">                    a[j + <span class="number">1</span>] = t;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> value : a) &#123;</span><br><span class="line">            System.out.println(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>比较排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——基数排序</title>
    <url>/2021/04/20/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<p><strong>基数排序</strong>(radix sort)算法，就是按照整数的个位、十位、百位...等依次排列元素，局部最优排列最终可以获得全局最优。</p>
<p>基数排序可以分为LSD和MSD两种，LSD就是从低位往高位排(个十百...)，MSD是从高位往低位排(...百十个)。</p>
<p>基数排序的时间复杂度为<span class="math inline">\(O(n*k)\)</span>，其中<span class="math inline">\(n\)</span>为元素数量，<span class="math inline">\(k\)</span>为最大元素的最高位(个位为1，十位为2......)，当元素不是很大时(即<span class="math inline">\(k\)</span>很小)可认为时间复杂度为<span class="math inline">\(O(n)\)</span>.</p>
<hr />
<h1 id="实现过程">实现过程</h1>
<p>算法步骤：</p>
<ol type="1">
<li><p>取得数组中的最大数，并取得位数；</p></li>
<li><p>对数位较短的数前面补零；</p></li>
<li><p>分配，先从个位开始，根据位值(0-9)分别放到0~9号桶中;</p></li>
<li><p>收集，再将放置在0~9号桶中的数据按顺序放到数组中;</p></li>
<li><p>重复3~4过程，直到最高位，即可完成排序。</p></li>
</ol>
<p>对如下序列进行基数排序：<code>[345,21,342,786,55,2,453,66,98,145,46,76,5,674]</code>.</p>
<p>采用LSD的方式来进行基数排序。</p>
<ul>
<li>第一步，按照个位分组</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">个位</th>
<th style="text-align: center;">分组</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">21</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">342，2</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;">453</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">674</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">345，55，145，5</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">786，66，46，76</td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">8</td>
<td style="text-align: center;">98</td>
</tr>
<tr class="even">
<td style="text-align: center;">9</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>依次连接后新的数组：<code>[21, 342, 2, 453, 674, 345, 55, 145, 5, 786, 66, 46, 76, 98]</code></p>
<ul>
<li>第二步，按照十位分组</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">十位</th>
<th style="text-align: center;">分组</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">2，5</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">21</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">342，345，145，46</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">453，55</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">66</td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: center;">674，76</td>
</tr>
<tr class="odd">
<td style="text-align: center;">8</td>
<td style="text-align: center;">786</td>
</tr>
<tr class="even">
<td style="text-align: center;">9</td>
<td style="text-align: center;">98</td>
</tr>
</tbody>
</table>
<p>依次连接后的新数组：<code>[2, 5, 21, 342, 345, 145, 46, 453, 55, 66, 674, 76, 786, 98]</code></p>
<ul>
<li>第三步，按照百位分组</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">百位</th>
<th style="text-align: center;">分组</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">2，5，21，46，55，66，76，98</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">145</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: center;">342，345</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: center;">453</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: center;">674</td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: center;">786</td>
</tr>
<tr class="odd">
<td style="text-align: center;">8</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: center;">9</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>连接后的最终排序数组：<code>[2, 5, 21, 46, 55, 66, 76, 98, 145, 342, 345, 453, 674, 786]</code></p>
<p>实例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radix</span>(<span class="params">nums</span>):</span></span><br><span class="line">    factor = <span class="number">1</span></span><br><span class="line">    max_num = <span class="built_in">max</span>(nums)</span><br><span class="line">    n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> max_num &gt;= <span class="number">10</span>:  <span class="comment"># 获取最高位</span></span><br><span class="line">        max_num //= <span class="number">10</span></span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> factor &lt; <span class="number">10</span> ** n:</span><br><span class="line">        temp = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            temp[i // factor % <span class="number">10</span>].append(i)  <span class="comment"># 元素归并到相应位的相应位置</span></span><br><span class="line">        nums = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> temp:</span><br><span class="line">            nums += i</span><br><span class="line">        factor *= <span class="number">10</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    print(radix([<span class="number">345</span>, <span class="number">21</span>, <span class="number">342</span>, <span class="number">786</span>, <span class="number">55</span>, <span class="number">2</span>, <span class="number">453</span>, <span class="number">66</span>, <span class="number">98</span>, <span class="number">145</span>, <span class="number">46</span>, <span class="number">76</span>, <span class="number">5</span>, <span class="number">674</span>]))</span><br><span class="line">    </span><br><span class="line">[out]:</span><br><span class="line">[<span class="number">2</span>, <span class="number">5</span>, <span class="number">21</span>, <span class="number">46</span>, <span class="number">55</span>, <span class="number">66</span>, <span class="number">76</span>, <span class="number">98</span>, <span class="number">145</span>, <span class="number">342</span>, <span class="number">345</span>, <span class="number">453</span>, <span class="number">674</span>, <span class="number">786</span>]</span><br></pre></td></tr></table></figure>
<h1 id="尾记">尾记</h1>
<p>基数排序思想也较为巧妙，时间复杂度达到了线性。但其应用受到限制，常用于非负整数序列的排序。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>非比较排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——堆排序</title>
    <url>/2021/05/29/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%A0%86%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<p><strong>堆排序</strong>（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆是一个近似完全二叉树的结构，并同时满足堆的性质：即子节点的键值或索引总是小于（或者大于）它的父节点。如果要从大到小排序，则应该使用大根堆，即根节点的值最大。时间复度为<span class="math inline">\(O(nlog(n))\)</span>.</p>
<hr />
<h1 id="步骤">步骤</h1>
<ul>
<li>建堆（最大堆），常采用向上调整建堆</li>
<li>将堆第一个元素与已排好数据前一位交换，进行下沉操作调整堆，同时乱序数据量减少一个</li>
<li>直到所有数据排序完成</li>
</ul>
<p>可以看出，堆排序也是选择排序的一种，通过大小根堆的性质一步步挑选出最值，然后完成排序。用堆取最值的时间复杂度为<span class="math inline">\(O(logn)\)</span>，所用元素都遍历一遍为<span class="math inline">\(O(n)\)</span>，总时间复杂度为<span class="math inline">\(O(nlog(n))\)</span></p>
<hr />
<h1 id="示例代码">示例代码</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">57</span>, <span class="number">65</span>, <span class="number">54</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">26</span>, <span class="number">35</span>, <span class="number">96</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span>[] arr_new = heapSort(arr);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : arr_new) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] heapSort(<span class="keyword">int</span>[] a) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = builtHeap(a);</span><br><span class="line">        <span class="keyword">int</span> index = arr.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (index &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            swap(arr, index);</span><br><span class="line">            index--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] builtHeap(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">        <span class="keyword">int</span> len = arr.length;</span><br><span class="line">        <span class="keyword">int</span>[] temp = <span class="keyword">new</span> <span class="keyword">int</span>[len];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">            temp[i] = arr[i];</span><br><span class="line">            swim(temp, i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> temp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swim</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (arr[index] &gt; arr[(index - <span class="number">1</span>) / <span class="number">2</span>]) &#123;</span><br><span class="line">            <span class="keyword">int</span> t = arr[index];</span><br><span class="line">            arr[index] = arr[(index - <span class="number">1</span>) / <span class="number">2</span>];</span><br><span class="line">            arr[(index - <span class="number">1</span>) / <span class="number">2</span>] = t;</span><br><span class="line">            index = (index - <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> t = arr[<span class="number">0</span>];</span><br><span class="line">        arr[<span class="number">0</span>] = arr[index];</span><br><span class="line">        arr[index] = t;</span><br><span class="line">        sink(arr, index);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sink</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i1 = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> i2 = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> ((<span class="number">2</span> * i1 + <span class="number">1</span>) &lt; index) &#123;</span><br><span class="line">            i2 = <span class="number">2</span> * i1 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> ((<span class="number">2</span> * i1 + <span class="number">2</span>) &lt; index) &#123;</span><br><span class="line">                <span class="keyword">if</span> (arr[<span class="number">2</span> * i1 + <span class="number">1</span>] &gt; arr[<span class="number">2</span> * i1 + <span class="number">2</span>]) &#123;</span><br><span class="line">                    i2 = <span class="number">2</span> * i1 + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    i2 = <span class="number">2</span> * i1 + <span class="number">2</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (arr[i1] &lt; arr[i2]) &#123;</span><br><span class="line">                <span class="keyword">int</span> t = arr[i1];</span><br><span class="line">                arr[i1] = arr[i2];</span><br><span class="line">                arr[i2] = t;</span><br><span class="line">                i1 = i2;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">[out]:</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">26</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">35</span></span><br><span class="line"><span class="number">45</span></span><br><span class="line"><span class="number">54</span></span><br><span class="line"><span class="number">57</span></span><br><span class="line"><span class="number">65</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapSort</span>(<span class="params">arr</span>):</span></span><br><span class="line">    arr = builtHeap(arr)</span><br><span class="line">    index = <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> index &gt; <span class="number">0</span>:</span><br><span class="line">        swap(arr, index)</span><br><span class="line">        index -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sink</span>(<span class="params">arr, index</span>):</span>  <span class="comment"># 结束于index-1</span></span><br><span class="line">    i1 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">2</span> * i1 + <span class="number">1</span>) &lt; index:</span><br><span class="line">        i2 = <span class="number">2</span> * i1 + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">2</span> * i1 + <span class="number">2</span> &lt; index:</span><br><span class="line">            <span class="keyword">if</span> arr[<span class="number">2</span> * i1 + <span class="number">1</span>] &gt; arr[<span class="number">2</span> * i1 + <span class="number">2</span>]:</span><br><span class="line">                i2 = <span class="number">2</span> * i1 + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i2 = <span class="number">2</span> * i1 + <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[i1] &lt; arr[i2]:</span><br><span class="line">            arr[i1], arr[i2] = arr[i2], arr[i1]</span><br><span class="line">            i1 = i2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">swim</span>(<span class="params">arr, index</span>):</span></span><br><span class="line">    <span class="keyword">while</span> arr[index] &gt; arr[<span class="built_in">max</span>((index - <span class="number">1</span>) // <span class="number">2</span>, <span class="number">0</span>)]:</span><br><span class="line">        arr[index], arr[(index - <span class="number">1</span>) // <span class="number">2</span>] = arr[(index - <span class="number">1</span>) // <span class="number">2</span>], arr[index]</span><br><span class="line">        index = (index - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">swap</span>(<span class="params">arr, index</span>):</span></span><br><span class="line">    arr[<span class="number">0</span>], arr[index] = arr[index], arr[<span class="number">0</span>]</span><br><span class="line">    sink(arr, index)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">builtHeap</span>(<span class="params">arr</span>):</span></span><br><span class="line">    length = <span class="built_in">len</span>(arr)</span><br><span class="line">    temp = [<span class="number">0</span>] * length</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        temp[i] = arr[i]</span><br><span class="line">        swim(temp, i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    a = [<span class="number">59</span>, <span class="number">56</span>, <span class="number">97</span>, <span class="number">19</span>, <span class="number">40</span>, <span class="number">48</span>, <span class="number">13</span>, <span class="number">75</span>, <span class="number">51</span>, <span class="number">65</span>]</span><br><span class="line">    out = heapSort(a)</span><br><span class="line">    print(out)</span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line">[<span class="number">13</span>, <span class="number">19</span>, <span class="number">40</span>, <span class="number">48</span>, <span class="number">51</span>, <span class="number">56</span>, <span class="number">59</span>, <span class="number">65</span>, <span class="number">75</span>, <span class="number">97</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>选择排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——希尔排序</title>
    <url>/2021/04/30/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p><strong>希尔排序</strong>(Shell Sort)按其设计者希尔（Donald Shell）的名字命名，该算法由希尔 1959 年公布。是一种插入排序的改进方式。插入排序在排序的时候，需要逐一比较两两元素之间的大小，效率不高。希尔排序按照下标的增量将序列先分组，每个组里使用插入排序，这样可以跳跃地比较元素的大小，确定元素间的宏观位置。再经过慢慢缩减增量(有不同增量选取方式，通常取原增量的二分之一)，可以微调元素位置，直到增量达到1时，所有元素排序完成。</p>
<p>看起来希尔排序的步骤要比插入排序的多很多，那它的时间复杂度应该比插入排序大很多才对呢，为什么效率更高了呢。可以拿希尔排序的增量为一的最后一步排序为例，经过前面的处理，希尔排序到最后一步大小相近的元素都被放到了相邻的位置，再进行插入排序至多一步就可以找到自己的最终位置跳出循环；而插入排序则可能一直比较到序列头部才找到自己的位置，所以效率低。</p>
<p>希尔排序的步骤虽复杂，但是整体上减少了时间复杂度，希尔排序的时间复杂度为<span class="math inline">\(O(n^{1-2})\)</span>，并没有一个确定的值，取决于增量的选择。</p>
<hr />
<h1 id="实现">实现</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShellSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">20</span>, <span class="number">48</span>, <span class="number">27</span>, <span class="number">98</span>, <span class="number">15</span>, <span class="number">78</span>, <span class="number">85</span>, <span class="number">80</span>, <span class="number">59</span>, <span class="number">47</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span> gap = arr.length;</span><br><span class="line">        <span class="keyword">while</span> (gap &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">            gap = gap / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; gap; i++) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; arr.length; j += gap) &#123;</span><br><span class="line">                    <span class="keyword">int</span> k = j;</span><br><span class="line">                    <span class="keyword">while</span> (k &gt; i) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (arr[k] &lt; arr[k - gap]) &#123;</span><br><span class="line">                            <span class="keyword">int</span> t = arr[k];</span><br><span class="line">                            arr[k] = arr[k - gap];</span><br><span class="line">                            arr[k - gap] = t;</span><br><span class="line">                        &#125;</span><br><span class="line">                        k -= gap;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : arr) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">[out]:</span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">27</span></span><br><span class="line"><span class="number">47</span></span><br><span class="line"><span class="number">48</span></span><br><span class="line"><span class="number">59</span></span><br><span class="line"><span class="number">78</span></span><br><span class="line"><span class="number">80</span></span><br><span class="line"><span class="number">85</span></span><br><span class="line"><span class="number">98</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span>(<span class="params">a</span>):</span></span><br><span class="line">    gap = <span class="built_in">len</span>(a)</span><br><span class="line">    <span class="keyword">while</span> gap &gt;= <span class="number">1</span>:</span><br><span class="line">        gap = gap // <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(gap):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, <span class="built_in">len</span>(a), gap):</span><br><span class="line">                k = j</span><br><span class="line">                <span class="keyword">while</span> k &gt; i:</span><br><span class="line">                    <span class="keyword">if</span> a[k] &lt; a[k - gap]:</span><br><span class="line">                        a[k], a[k - gap] = a[k - gap], a[k]</span><br><span class="line">                    k -= gap</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    arr = [<span class="number">20</span>, <span class="number">48</span>, <span class="number">27</span>, <span class="number">98</span>, <span class="number">15</span>, <span class="number">78</span>, <span class="number">85</span>, <span class="number">80</span>, <span class="number">59</span>, <span class="number">47</span>]</span><br><span class="line">    arr = shell_sort(arr)</span><br><span class="line">    print(arr)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">[out]:</span><br><span class="line"><span class="number">15</span>, <span class="number">20</span>, <span class="number">27</span>, <span class="number">47</span>, <span class="number">48</span>, <span class="number">59</span>, <span class="number">78</span>, <span class="number">80</span>, <span class="number">85</span>, <span class="number">98</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>插入排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——归并排序</title>
    <url>/2021/05/09/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p><strong>归并排序</strong>(MergeSort)，是采用分治思想的一种算法，通过把要排序的数组依次递归对半拆分，当子数组达到长度1时，即只有一个元素，认为其为有序的，结束递归。使用递归排序时，无论原数组序列的排序情况如何，都要递归到最底层，所以时间复杂度相对固定。为<span class="math inline">\(O(nlog(n))\)</span>. 归并排序可分为二路归并和多路归并，最常用的是二路归并，下文示例亦是针对二路归并排序的。另外，归并排序是稳定的。</p>
<hr />
<h1 id="图解">图解</h1>
<p>拆分阶段：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210509MergeSort1.png" alt="image-20210509204359870" /><figcaption>image-20210509204359870</figcaption>
</figure>
<p>合并阶段：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210509MergeSort2.png" alt="image-20210509204925849" /><figcaption>image-20210509204925849</figcaption>
</figure>
<hr />
<h1 id="示例代码">示例代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeSort</span>(<span class="params">arr</span>):</span>  <span class="comment"># 拆分</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(arr) // <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    <span class="keyword">return</span> merge(mergeSort(arr[<span class="number">0</span>:<span class="built_in">len</span>(arr) // <span class="number">2</span>]), mergeSort(arr[<span class="built_in">len</span>(arr) // <span class="number">2</span>:]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span>(<span class="params">a_l, a_r</span>):</span>  <span class="comment"># 合并</span></span><br><span class="line">    a = [<span class="number">0</span>] * (<span class="built_in">len</span>(a_l) + <span class="built_in">len</span>(a_r))</span><br><span class="line">    p_l, p_r = <span class="number">0</span>, <span class="number">0</span>  <span class="comment"># 左右数组的左右指针</span></span><br><span class="line">    <span class="keyword">while</span> p_l &lt; <span class="built_in">len</span>(a_l) <span class="keyword">and</span> p_r &lt; <span class="built_in">len</span>(a_r):</span><br><span class="line">        <span class="keyword">if</span> a_l[p_l] &gt; a_r[p_r]:</span><br><span class="line">            a[p_l + p_r] = a_r[p_r]</span><br><span class="line">            p_r += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            a[p_l + p_r] = a_l[p_l]</span><br><span class="line">            p_l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> p_l &lt; <span class="built_in">len</span>(a_l):  <span class="comment"># 若左数组剩余，依次填入</span></span><br><span class="line">        a[p_l + p_r] = a_l[p_l]</span><br><span class="line">        p_l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> p_r &lt; <span class="built_in">len</span>(a_r):  <span class="comment"># 若右数组剩余，依次填入</span></span><br><span class="line">        a[p_l + p_r] = a_r[p_r]</span><br><span class="line">        p_r += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    a = [<span class="number">16</span>, <span class="number">55</span>, <span class="number">64</span>, <span class="number">35</span>, <span class="number">24</span>, <span class="number">4</span>, <span class="number">85</span>, <span class="number">68</span>, <span class="number">84</span>, <span class="number">41</span>]</span><br><span class="line">    out = mergeSort(a)</span><br><span class="line">    print(out)</span><br><span class="line">    </span><br><span class="line">[out]:</span><br><span class="line">[<span class="number">4</span>, <span class="number">16</span>, <span class="number">24</span>, <span class="number">35</span>, <span class="number">41</span>, <span class="number">55</span>, <span class="number">64</span>, <span class="number">68</span>, <span class="number">84</span>, <span class="number">85</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MergeSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">97</span>, <span class="number">76</span>, <span class="number">84</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">53</span>, <span class="number">59</span>, <span class="number">62</span>, <span class="number">93</span>, <span class="number">53</span>, <span class="number">37</span>, <span class="number">19</span>, <span class="number">9</span>&#125;;</span><br><span class="line">        arr = mergeSort(arr);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : arr) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] mergeSort(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">        <span class="keyword">int</span> len = arr.length;</span><br><span class="line">        <span class="keyword">if</span> (len / <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> arr;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[] arr_l = mergeSort(Arrays.copyOfRange(arr, <span class="number">0</span>, len / <span class="number">2</span>));</span><br><span class="line">        <span class="keyword">int</span>[] arr_r = mergeSort(Arrays.copyOfRange(arr, len / <span class="number">2</span>, len));</span><br><span class="line">        <span class="keyword">return</span> merge(arr_l, arr_r);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] merge(<span class="keyword">int</span>[] arr_l, <span class="keyword">int</span>[] arr_r) &#123;</span><br><span class="line">        <span class="keyword">int</span> len_l = arr_l.length;</span><br><span class="line">        <span class="keyword">int</span> len_r = arr_r.length;</span><br><span class="line">        <span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[len_l + len_r];</span><br><span class="line">        <span class="keyword">int</span> p_l = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> p_r = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (p_l &lt; len_l &amp;&amp; p_r &lt; len_r) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr_l[p_l] &lt; arr_r[p_r]) &#123;</span><br><span class="line">                arr[p_l + p_r] = arr_l[p_l];</span><br><span class="line">                p_l++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                arr[p_l + p_r] = arr_r[p_r];</span><br><span class="line">                p_r++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (p_l &lt; len_l) &#123;</span><br><span class="line">            arr[p_l + p_r] = arr_l[p_l];</span><br><span class="line">            p_l++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (p_r &lt; len_r) &#123;</span><br><span class="line">            arr[p_r + p_l] = arr_r[p_r];</span><br><span class="line">            p_r++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arr;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">[out]:</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">19</span></span><br><span class="line"><span class="number">35</span></span><br><span class="line"><span class="number">37</span></span><br><span class="line"><span class="number">53</span></span><br><span class="line"><span class="number">53</span></span><br><span class="line"><span class="number">59</span></span><br><span class="line"><span class="number">60</span></span><br><span class="line"><span class="number">62</span></span><br><span class="line"><span class="number">76</span></span><br><span class="line"><span class="number">84</span></span><br><span class="line"><span class="number">93</span></span><br><span class="line"><span class="number">97</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>归并排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——快速排序</title>
    <url>/2021/04/27/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p><strong>快速排序</strong>（Quick Sort）是对冒泡排序的一种改进。它运用到了分治思想，对每一个子序列分别排序。其平均时间复杂度为<span class="math inline">\(O(nlog(n))\)</span>，最坏时间复杂度为<span class="math inline">\(O(n^2)\)</span>。虽然在时间复杂度上并没有显著的优势，但在实际运用中效率比同时间复杂度的一些算法要高。</p>
<hr />
<h1 id="实现">实现</h1>
<h2 id="基本步骤">基本步骤</h2>
<ul>
<li>选取一个基元素，作为排序的对比标准，一般选取数组的第一个元素，也可以在中间随机选取。</li>
<li>选取左指针和右指针（数组的以一个元素索引和最后一个元素索引）。</li>
<li>先从大往小移动右指针，当指针所指元素小于基元素时，将此元素覆盖到左指针处（此时左指针处的元素由基元素备份，此位置视为一个坑，可覆盖，同理覆盖后此时的右指针位置成为一个坑，可由接下来的元素填充），然后左指针右移一位。</li>
<li>之后右指针不变从左往右遍历左指针，当左指针所指元素大于基元素时，将此元素填到右指针所指位置。</li>
<li>重复上面两个步骤直到左右指针相等，将基元素填在指针相等的位置，此时所有大于基元素的数都在其右边，所有小于基元素的数都在其左边。至此完成了一个排序。</li>
<li>对由基元素分割出来的两个数组再次按此步骤进行排序，一直递归下去，直至子数组的元素不需要排序（数组长度小于1）。</li>
</ul>
<h2 id="图解">图解</h2>
<p>将原数组中第一个元素作为基元素，同时设置左右指针。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210427QuickSort1.png" alt="image-20210427224521335" /><figcaption>image-20210427224521335</figcaption>
</figure>
<p>移动移动指针，直到找到小于基元素的数(13)，将其填入左指针位置，将左指针右移一位。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210427QuickSort2.png" alt="image-20210427225034407" /><figcaption>image-20210427225034407</figcaption>
</figure>
<p>移动左指针，直到找到比基元素大的值(120)，将其覆盖到右指针位置，右指针左移一位。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210427QuickSort3.png" alt="image-20210427225339326" /><figcaption>image-20210427225339326</figcaption>
</figure>
<p>轮到右指针的轮次，移动右指针找到比基元素小的，显然120和40都比基元素20大，当左右指针重合时(数组索引为2)，跳出循环，将基元素填入指针位置。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210427QuickSort4.png" alt="image-20210427225719310" /><figcaption>image-20210427225719310</figcaption>
</figure>
<p>此时20的两边分别为小于其和大于其的数。对两个子数组，再次采用相同的方式来排序，直到所有元素排序完成。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210427QuickSort5.png" alt="image-20210427230155431" /><figcaption>image-20210427230155431</figcaption>
</figure>
<p>最终排序完成数组为：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210427QuickSort6.png" alt="image-20210427230412301" /><figcaption>image-20210427230412301</figcaption>
</figure>
<hr />
<h1 id="源码">源码</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QuickSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">20</span>, <span class="number">5</span>, <span class="number">120</span>, <span class="number">44</span>, <span class="number">13</span>, <span class="number">55</span>, <span class="number">49</span>, <span class="number">83</span>&#125;;</span><br><span class="line">        partial(arr, <span class="number">0</span>, arr.length - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : arr) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">partial</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (left &gt;= right) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> key = a[left];</span><br><span class="line">        <span class="keyword">int</span> p_left = left;</span><br><span class="line">        <span class="keyword">int</span> p_right = right;</span><br><span class="line">        <span class="keyword">boolean</span> flag = <span class="keyword">true</span>; <span class="comment">// true移动右指针，false移动左指针</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                <span class="keyword">if</span> (a[p_right] &lt; key) &#123;  <span class="comment">// 基元素大于右指针所指元素，交换位置</span></span><br><span class="line">                    a[p_left] = a[p_right];</span><br><span class="line">                    flag = <span class="keyword">false</span>;</span><br><span class="line">                    p_left++;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    p_right--;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (a[p_left] &gt; key) &#123;  <span class="comment">// 基元素小于左指针所指元素，交换位置</span></span><br><span class="line">                    a[p_right] = a[p_left];</span><br><span class="line">                    p_right--;</span><br><span class="line">                    flag = <span class="keyword">true</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    p_left++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (p_left == p_right) &#123;  <span class="comment">// 指针重合，基元素覆盖到指针处，跳出循环</span></span><br><span class="line">                a[p_left] = key;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        partial(a, left, p_left - <span class="number">1</span>); <span class="comment">//排列左子数组</span></span><br><span class="line">        partial(a, p_right + <span class="number">1</span>, right); <span class="comment">//排列右子数组</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">44</span></span><br><span class="line"><span class="number">49</span></span><br><span class="line"><span class="number">55</span></span><br><span class="line"><span class="number">83</span></span><br><span class="line"><span class="number">120</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span>(<span class="params">a, left, right</span>):</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= right:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    p_left = left</span><br><span class="line">    p_right = right</span><br><span class="line">    key = a[left]  <span class="comment"># 取区间第一个元素为基元素</span></span><br><span class="line">    flag = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="keyword">if</span> a[p_right] &lt; key:</span><br><span class="line">                a[p_left] = a[p_right]</span><br><span class="line">                p_left = + <span class="number">1</span></span><br><span class="line">                flag = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p_right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> a[p_left] &gt; key:</span><br><span class="line">                a[p_right] = a[p_left]</span><br><span class="line">                p_right -= <span class="number">1</span></span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p_left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> p_left == p_right:</span><br><span class="line">            a[p_left] = key</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    quick_sort(a, left, p_left - <span class="number">1</span>)</span><br><span class="line">    quick_sort(a, p_right + <span class="number">1</span>, right)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    arr = [<span class="number">34</span>, <span class="number">3</span>, <span class="number">65</span>, <span class="number">101</span>, <span class="number">11</span>, <span class="number">5</span>, <span class="number">23</span>, <span class="number">67</span>, <span class="number">344</span>, <span class="number">895</span>, <span class="number">56</span>, <span class="number">72</span>, <span class="number">45</span>]</span><br><span class="line">    quick_sort(arr, <span class="number">0</span>, <span class="built_in">len</span>(arr) - <span class="number">1</span>)</span><br><span class="line">    print(arr)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line">[<span class="number">3</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">23</span>, <span class="number">34</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>, <span class="number">67</span>, <span class="number">72</span>, <span class="number">101</span>, <span class="number">344</span>, <span class="number">895</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>比较排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——插入排序</title>
    <url>/2021/04/29/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p>插入排序是一种比较简单的排序方式通过将新的元素插入到已经排好的序列中来完成排序的目的。其最坏时间复杂度为<span class="math inline">\(O(n^2)\)</span>，最好时间复杂度为<span class="math inline">\(O(n)\)</span>，平均时间复杂度我为<span class="math inline">\(O(n^2)\)</span>. 是一种比较基础而又低效的排序算法，在数据量较小时可以使用，此外，插入排序是稳定的。</p>
<hr />
<h1 id="实现">实现</h1>
<h2 id="步骤">步骤</h2>
<ul>
<li>将数组第一个元素作为已排序序列。</li>
<li>依次遍历后续元素，将每个元素与以排好序列中的元素依次比较，直到找到一个合适的位置将其插入进去。</li>
</ul>
<h2 id="图解">图解</h2>
<p>对<code>[17,5,23,12]</code>排序。</p>
<p>17(0索引)视为已排序序列，从5(1索引)开始进行插入排序。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210429InsertSort1.png" alt="image-20210429231317261" /><figcaption>image-20210429231317261</figcaption>
</figure>
<p>17比5更大，将5插入到17前面，现在已排序的元素为<code>[5,17]</code></p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210429InsertSort2.png" alt="image-20210429231645763" /><figcaption>image-20210429231645763</figcaption>
</figure>
<p>对元素23进行排列，23比17大，直接插到其后面，现在已排序数组为：<code>[5,17,23]</code></p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210429InsertSort3.png" alt="image-20210429231939026" /><figcaption>image-20210429231939026</figcaption>
</figure>
<p>最后是元素12，依次对比23，比其小，前移，再对比17，依然小，再前移，比5大，插入到5的后面，整个排序完成。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210429InsertSort4.png" alt="image-20210429232431402" /><figcaption>image-20210429232431402</figcaption>
</figure>
<hr />
<h1 id="示例">示例</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">92</span>, <span class="number">88</span>, <span class="number">49</span>, <span class="number">25</span>, <span class="number">76</span>, <span class="number">17</span>, <span class="number">61</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">69</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span> j;</span><br><span class="line">        <span class="keyword">int</span> t;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">            j = i;</span><br><span class="line">            <span class="keyword">while</span> (arr[j - <span class="number">1</span>] &gt; arr[j]) &#123;</span><br><span class="line">                t = arr[j - <span class="number">1</span>];</span><br><span class="line">                arr[j - <span class="number">1</span>] = arr[j];</span><br><span class="line">                arr[j] = t;</span><br><span class="line">                j--;</span><br><span class="line">                <span class="keyword">if</span> (j == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : arr) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">[out]:</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">17</span></span><br><span class="line"><span class="number">25</span></span><br><span class="line"><span class="number">49</span></span><br><span class="line"><span class="number">61</span></span><br><span class="line"><span class="number">69</span></span><br><span class="line"><span class="number">76</span></span><br><span class="line"><span class="number">88</span></span><br><span class="line"><span class="number">92</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertion_sort</span>(<span class="params">a</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(a)):</span><br><span class="line">        j = i</span><br><span class="line">        <span class="keyword">while</span> a[j - <span class="number">1</span>] &gt; a[j]:</span><br><span class="line">            a[j - <span class="number">1</span>], a[j] = a[j], a[j - <span class="number">1</span>]</span><br><span class="line">            j -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    arr = [<span class="number">66</span>, <span class="number">93</span>, <span class="number">98</span>, <span class="number">96</span>, <span class="number">38</span>, <span class="number">8</span>, <span class="number">74</span>, <span class="number">56</span>, <span class="number">64</span>, <span class="number">51</span>]</span><br><span class="line">    arr = insertion_sort(arr)</span><br><span class="line">    print(arr)</span><br><span class="line">    </span><br><span class="line">[out]:</span><br><span class="line">[<span class="number">8</span>, <span class="number">38</span>, <span class="number">51</span>, <span class="number">56</span>, <span class="number">64</span>, <span class="number">66</span>, <span class="number">74</span>, <span class="number">93</span>, <span class="number">96</span>, <span class="number">98</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>插入排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——桶排序</title>
    <url>/2021/04/18/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%A1%B6%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="介绍">介绍</h1>
<p><strong>桶排序</strong>(Bucket sort)是将待排序集合中处于同一个<strong>值域</strong>的元素存入同一个桶中，也就是根据元素值特性将集合拆分为多个区域，则拆分后形成的多个桶，从值域上看是处于有序状态的。对每个桶中元素进行排序，则所有桶中元素构成的集合是已排序的。</p>
<p>如果桶的大小划分得足够小，到达每个元素之间的最小差值，则可以保证每一个桶里面所有的数据都是一样的，入桶后的数据也就不需要再次进行排序，这种情况也就是桶排序时间复杂度最优的情况即<span class="math inline">\(O(n)\)</span>.一般情况下桶排序的时间复杂度为<span class="math inline">\(O(n+nlog(\frac{n}{m}))\)</span>，其中n为元素个数，m为桶个数。</p>
<hr />
<h1 id="实现">实现</h1>
<p>桶排序的大致步骤如下：</p>
<ul>
<li>确定桶的大小与个数，一般根据要排序的元素的值域区间取定。</li>
<li>设计一种方式使元素能映射至对应值域的桶的索引。</li>
<li>遍历所有元素，将它们入桶。</li>
<li>每个桶内元素排序。</li>
<li>从桶内依次提取各元素重新排列。</li>
</ul>
<p>以下是一个简单的桶排序过程。显而易见，所有元素的值都在50~59之间，因此可以使用<span class="math inline">\(a-50\)</span>这样一个映射关系将所有元素映射至10个桶中，例如 <span class="math inline">\(55-50=5\)</span> ，可以放入编号为5的桶。由于按照此种方式来来入桶每个桶里的值就是一样的，所以桶内可以存放一个数值信息，代表此处的元素有多少个(通常一个桶里存放的是一个值域范围的元素，需要存放元素的具体数值信息)。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210418BucketSort1.png" alt="image-20210418213046344" /><figcaption>image-20210418213046344</figcaption>
</figure>
<p>入桶后每个桶中的元素数量：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210418Bucketsort2.png" alt="image-20210418215621271" /><figcaption>image-20210418215621271</figcaption>
</figure>
<p>将上面的例子排序后输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BucketSort</span>(<span class="params">nums</span>):</span></span><br><span class="line">    buckets = [<span class="number">0</span>] * <span class="number">10</span>  <span class="comment"># 建立桶</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">        buckets[i - <span class="number">50</span>] += <span class="number">1</span>   <span class="comment"># 索引计算</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(buckets)):  <span class="comment"># 输出排序后的值</span></span><br><span class="line">        <span class="keyword">while</span> buckets[i] != <span class="number">0</span>:</span><br><span class="line">            print(i + <span class="number">50</span>)</span><br><span class="line">            buckets[i] -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<hr />
<h1 id="特殊应用">特殊应用</h1>
<p>给定一个整数数组 <code>nums</code>和两个整数 <code>k</code>和 <code>t</code>。请你判断是否存在 两个不同下标 <code>i</code> 和<code>j</code>，使得 <code>abs(nums[i] - nums[j]) &lt;= t</code> ，同时又满足 <code>abs(i - j) &lt;= k</code>。</p>
<p><strong>示例 1：</strong></p>
<p>输入：<code>nums = [1,2,3,1], k = 3, t = 0</code> 输出：<code>true</code></p>
<p><strong>示例 2：</strong></p>
<p>输入：<code>nums = [1,5,9,1,5,9], k = 2, t = 3</code> 输出：<code>false</code></p>
<p>这种问题，直接一个暴力破解，啪一下就好了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">containsNearbyAlmostDuplicate</span>(<span class="params">nums, k, t</span>):</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">min</span>(i+<span class="number">1</span>+k,<span class="built_in">len</span>(nums))):</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(nums[i]-nums[j])&lt;=t:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>但是此方法的时间复杂度为<span class="math inline">\(O(nk)\)</span>，并不快啊。</p>
<p>用桶排序可将时间复杂度降至<span class="math inline">\(O(n)\)</span>. 这不是一个排序问题，为了避免空间浪费，我们申请一组临时的桶，在元素遍历的过程中，创建新的桶并释放没用的桶（同时保证桶里的元素是符合要求的，在可取索引范围内的）。</p>
<p>对于元素 x，其影响的区间为 [x - t, x + t]。于是我们可以设定桶的大小为 t + 1。如果两个元素同属一个桶，那么这两个元素必然符合条件。如果两个元素属于相邻桶，那么我们需要校验这两个元素是否差值不超过 t。如果两个元素既不属于同一个桶，也不属于相邻桶，那么这两个元素必然不符合条件。我们遍历该序列，假设当前遍历到元素 x，那么我们首先检查 x 所属于的桶是否已经存在元素，如果存在，那么我们就找到了一对符合条件的元素，否则我们继续检查两个相邻的桶内是否存在符合条件的元素。遍历时需要不断更新桶内元素的状态，确保目前桶内的元素都是在有效的索引范围内的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">containsNearbyAlmostDuplicate</span>(<span class="params">nums, k, t</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getIdx</span>(<span class="params">u</span>):</span></span><br><span class="line">    	<span class="keyword">return</span> ((u + <span class="number">1</span>) // size) - <span class="number">1</span> <span class="keyword">if</span> u &lt; <span class="number">0</span> <span class="keyword">else</span> u // size</span><br><span class="line"></span><br><span class="line">    <span class="built_in">map</span> = &#123;&#125;</span><br><span class="line">    size = t + <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i,u <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">    	idx = getIdx(u)</span><br><span class="line">        <span class="comment"># 目标桶已存在（桶不为空），说明前面已有 [u - t, u + t] 范围的数字</span></span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> <span class="built_in">map</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 检查相邻的桶</span></span><br><span class="line">        l, r = idx - <span class="number">1</span>, idx + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> l <span class="keyword">in</span> <span class="built_in">map</span> <span class="keyword">and</span> <span class="built_in">abs</span>(u - <span class="built_in">map</span>[l]) &lt;= t:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> r <span class="keyword">in</span> <span class="built_in">map</span> <span class="keyword">and</span> <span class="built_in">abs</span>(u - <span class="built_in">map</span>[r]) &lt;= t:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 建立目标桶</span></span><br><span class="line">        <span class="built_in">map</span>[idx] = u</span><br><span class="line">        <span class="comment"># 维护个数为k</span></span><br><span class="line">        <span class="keyword">if</span> i &gt;= k:</span><br><span class="line">            <span class="built_in">map</span>.pop(getIdx(nums[i-k]))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<hr />
<h1 id="结语">结语</h1>
<p>桶排序，对于值域范围不大的排序问题来说，是一个上乘之选。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>非比较排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——计数排序</title>
    <url>/2021/04/19/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="概述">概述</h1>
<p>计数排序是一个非基于比较的[序算法，该算法于1954年由 Harold H. Seward 提出。它的优势在于在对一定范围内的整数排序时，它的复杂度为<span class="math inline">\(Ο(n+k)\)</span>（其中k是整数的范围），快于任何比较排序算法。</p>
<p>但是，计数排序算法的快速是使用空间来换取的。当整数范围过大时，其效率甚至不如复杂度为<span class="math inline">\(O(nlog(n))\)</span>的比较排序类算法。</p>
<p>计数排序适用于对整数进行排序，首先建立一个数组，初始值为0.数组的尺寸为需要排序元素的最大最小值之差减一，定义映射关系(通常减去排序元素中的最小值即可)，将原来元素映射到数组中。元素映射后的值便对应数组的索引。遍历元素，将元素所对应数组处的值加1，最后依次取出数组中的值完成排序。</p>
<p>其实这就是<a href="http://xiubenwu.github.io/2021/04/18/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%A1%B6%E6%8E%92%E5%BA%8F/">桶排序</a>的时间最优的特殊情况。</p>
<hr />
<h1 id="实现">实现</h1>
<p>此处沿用桶排序的特例。所有元素的值都在50~59之间，因此可以使用<span class="math inline">\(a-50\)</span>这样一个映射关系将所有元素映射至数组的索引0~9。遍历元素依次给相应的位置上的数组元素加一。 <img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210418BucketSort1.png" alt="image-20210418213046344" /></p>
<p>遍历结束后每个计数数组中的元素数量：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210418Bucketsort2.png" alt="image-20210418215621271" /><figcaption>image-20210418215621271</figcaption>
</figure>
<p>将上面的例子排序后输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Sort</span>(<span class="params">nums</span>):</span></span><br><span class="line">    buckets = [<span class="number">0</span>] * <span class="number">10</span>  <span class="comment"># 建立固定长度数组</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">        buckets[i - <span class="number">50</span>] += <span class="number">1</span>   <span class="comment"># 索引计算</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(buckets)):  <span class="comment"># 输出排序后的值</span></span><br><span class="line">        <span class="keyword">while</span> buckets[i] != <span class="number">0</span>:</span><br><span class="line">            print(i + <span class="number">50</span>)</span><br><span class="line">            buckets[i] -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>非比较排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法——选择排序</title>
    <url>/2021/05/18/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>选择排序(Selection sort)不断从乱序数列(后半部分)中选择最小(最大)值放入有序数列(前半部分)中，无论数据初始排列如何，其时间复杂度恒为<span class="math inline">\(O(n^2)\)</span>.好处是不占额外的空间复杂度。</p>
<hr />
<h1 id="示例">示例</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SelectionSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">57</span>, <span class="number">65</span>, <span class="number">54</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">26</span>, <span class="number">35</span>, <span class="number">96</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; arr.length; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (arr[i] &gt; arr[j]) &#123;</span><br><span class="line">                    <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">                    arr[i] = arr[j];</span><br><span class="line">                    arr[j] = temp;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : arr) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">[out]:</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">26</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">35</span></span><br><span class="line"><span class="number">45</span></span><br><span class="line"><span class="number">54</span></span><br><span class="line"><span class="number">57</span></span><br><span class="line"><span class="number">65</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span>(<span class="params">arr</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr) - <span class="number">1</span>):  <span class="comment"># 可len(arr)</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(arr)):  <span class="comment"># 可i</span></span><br><span class="line">            <span class="keyword">if</span> arr[i] &gt; arr[j]:</span><br><span class="line">                arr[i], arr[j] = arr[j], arr[i]</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    a = [<span class="number">18</span>, <span class="number">59</span>, <span class="number">41</span>, <span class="number">54</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">88</span>, <span class="number">6</span>]</span><br><span class="line">    out = selection_sort(a)</span><br><span class="line">    print(out)</span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line">    [<span class="number">6</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">41</span>, <span class="number">54</span>, <span class="number">59</span>, <span class="number">79</span>, <span class="number">88</span>, <span class="number">91</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
        <category>排序算法</category>
        <category>选择排序</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构-堆</title>
    <url>/2021/04/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A0%86/</url>
    <content><![CDATA[<p>在计算机的世界里，很多的应用场景只需要取得当前数据集中最大或者最小的元素，而对于数据集中其它数据，并不需要他们一定是有序的。堆就是一种帮助我们高效快速获取最大最小元素的数据结构。 <span id="more"></span></p>
<hr />
<h1 id="堆的概念">堆的概念</h1>
<p><strong>堆(heap)</strong> 是一个基于树的特殊数据结构，支持插入、删除、查询最值。它满足每一个子树的根节点一定保存着这棵子树中的最值。大根堆保存的就是最大值，小根堆保存的就是最小值。</p>
<p>堆有很多种变体，包括二项式堆、斐波那契堆等，但是最常见的就是二叉堆，所以说堆可以看做一个(完全)二叉树，从另一方面来说，堆是二叉树的数组实现方式。堆的存储可以看成是数组存储的变形。<br />
二叉堆就是一棵满足“堆性质”的完全二叉树。在使用数组来存储堆的情况下，左子节点的下标就是父节点下标乘以2，右子节点的下标就是父节点下标乘以2+1.这样的方法可以推广到k叉堆。针对数组索引从0开始的情况：<br />
<span class="math inline">\(heapChildLeft = heap[(i+1)*2-1]\)</span><br />
<span class="math inline">\(heapChildRight = heap[(i+1)*2]\)</span></p>
<p><span class="math inline">\(heapParent = heap[i/2-1]\)</span></p>
<p>数据在数组中分层排放，如<code>index = 0</code>为第一层，即根节点，<code>index = 1,2</code>为第二层，<code>index = 3,4,5,6</code>为第三层<code>index = 7,8,9,10,11,12,13,14</code>为第四层...以此类推。 ***</p>
<h1 id="最大堆与最小堆">最大堆与最小堆</h1>
<p>堆一般用于解决优先队列等优化问题，存储形式一般也是以最大堆和最小堆的形。 * 最大堆就是父节点的值恒大于子节点。最大值存放于根节点。 * 最小堆就是父节点的值恒小于子节点。最小值存放于根节点。</p>
<h1 id="堆的基本操作">堆的基本操作</h1>
<p>堆的主要操有<strong>创建</strong>、<strong>调整</strong>(向上或向下调整)、<strong>插入</strong>、<strong>删除</strong>等。</p>
<h2 id="堆的创建">堆的创建</h2>
<p>堆可以看做是由一个数组转化而来的。因此，堆的创建也可以进行一些转化。 * 第一步，将一个数组里面的元素通过下标按层·排序的方式构建一个完全二叉树 * 第二步，将这个二叉树通过向下调整的方法调整成为最小或最大堆。 ***</p>
<h2 id="堆的调整">堆的调整</h2>
<h3 id="向上调整">向上调整</h3>
<p>一般在插入的时候需要用到向上调整，插入数据后原先堆的大小顺序有可能被改变，我们直接将要插入的节点放在最后，由<code>void insert(node temp)</code>实现。然后我们从这个节点开始向上维护堆，由<code>void shiftUp(ll son)</code>实现，让这个节点去到该去的地方，根据大小根堆的不同，调整判断交换的条件。调整方法：<br />
需要调整的只是从该叶子节点到根节点的子树，先找到该节点的父亲节点，对于最小堆，如果小于父亲节点，那么就需要进行向上调整，使其调整之后满足小堆的性质，如此重复多次，直到整个堆都满足小堆的性质。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void shiftUp(ll son)</span><br><span class="line">&#123;</span><br><span class="line">	while(son&gt;1)</span><br><span class="line">	&#123;</span><br><span class="line">		if(heap[son].val&lt;heap[son&#x2F;2].val)</span><br><span class="line">		&#123;</span><br><span class="line">			swap(heap[son],heap[son&#x2F;2]);</span><br><span class="line">            son -&#x3D; 1;</span><br><span class="line">			son &gt;&gt;&#x3D;1;</span><br><span class="line">		&#125;</span><br><span class="line">		else</span><br><span class="line">			break;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">void insert(node temp)</span><br><span class="line">&#123;</span><br><span class="line">	heap[++count1]&#x3D;temp;</span><br><span class="line">	up(count1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> ### 向下调整 向下调整一般出现在删除的时候，我们直接将要删除的节点和最后的节点交换，并将count1减1，表示最后一个节点被舍弃，由<code>void Remove()</code>实现。然后我们从原来的那个节点的位置开始向上向下维护堆，由<code>void shiftUp(ll son)</code>和<code>void shiftDown(ll fa)</code>实现。调整方法：<br />
将堆中最后一个元素替代堆顶元素，由于堆的元素个数是按下标控制，因此只需要将下标减一就可以将堆中最后一个元素删除此时，如果堆的结构被破坏，只需要进行简单的向下调整就可以使其重新满足小堆的性质，而其时间复杂度只有<code>logN</code>，因此用堆来进行排序也是可以行得通的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void shiftDown(ll fa)</span><br><span class="line">&#123;</span><br><span class="line">	ll son&#x3D;fa*2;</span><br><span class="line">	while(son&lt;&#x3D;count1)</span><br><span class="line">	&#123;</span><br><span class="line">		if(son&lt;count1 &amp;&amp; heap[son].val&gt;heap[son+1].val)</span><br><span class="line">			son++;</span><br><span class="line">		if(heap[son].val&lt;heap[fa].val)</span><br><span class="line">		&#123;</span><br><span class="line">			swap(heap[son],heap[fa]);</span><br><span class="line">			fa&#x3D;son;</span><br><span class="line">			son&#x3D;fa*2;</span><br><span class="line">		&#125;</span><br><span class="line">		else</span><br><span class="line">			break;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">void Remove(ll p)</span><br><span class="line">&#123;</span><br><span class="line">	heap[p]&#x3D;heap[count1--];</span><br><span class="line">	up(p);down(p);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="堆的运用">堆的运用</h1>
<p><strong>海量数据topk问题：100亿个数中找出最大的前k个数</strong><br />
方法： 1. 创建一个有k个元素的小堆 2. 利用循环，将最后一个元素和第一个元素进行替换，然后进行向下调整使其满足小堆的性质，如果这个数比根节点都小，那就直接舍弃，否则进行调整，直到这个堆里面所有的数据都是你要找的最大的那几个数为止，这样再进行打印就可以了。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构-栈</title>
    <url>/2021/03/27/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88/</url>
    <content><![CDATA[<p><strong>栈(Stack)</strong> 作为一种数据结构，是一种只能在一端进行插入和删除操作的特殊线性表。它按照先进后出的原则存储数据，先进入的数据被压入栈底，最后的数据在栈顶，需要读数据的时候从栈顶开始弹出数据(最后一个数据被第一个读出来)。 <span id="more"></span> 栈只能在一端进行操作，即遵循先进后出的原则。就如同在抽屉里存放书，先放入的书会被压倒最底下，后放入的书在上面，取书的时候先放入的书最后才能被取出来。</p>
<h1 id="栈的基本操作">栈的基本操作</h1>
<p>关于栈的操作有<strong>栈的建立(顺序栈和链式栈)</strong> ， <strong>栈的初始化</strong> ，<strong>栈的初始化</strong> ，<strong>栈遍历</strong> ，<strong>栈清空/销毁</strong> ，<strong>判断栈是否为空</strong> ，<strong>求栈的长度</strong> ，<strong>入栈出栈</strong>等。其中 <strong>入栈和出栈</strong> 是最基本的操作。 * 入栈(push)：top指针上移，元素入栈。 * 出栈(pop)：top指针下移。<br />
入栈时要判断是否满栈。出栈时要判断是否栈空。栈顶指针位置始终在栈顶元素加一处。</p>
<h1 id="单调栈">单调栈</h1>
<p>单调栈是栈的扩展运用。单调栈就是栈内元素单调递增或者单调递减的栈，单调栈只能在栈顶操作。<br />
对于单调栈来说，当新加入的元素如果加到栈顶后，如果栈里的元素不再是单调递增了（新元素破坏了栈的单调性），那么我们就删除加入前的栈顶元素。 * 使用单调栈可以找到元素向左遍历第一个比他小的元素，也可以找到元素向左遍历第一个比他大的元素。</p>
<h1 id="栈的运用">栈的运用</h1>
<h2 id="消除字符串中相邻的重复项">消除字符串中相邻的重复项：</h2>
<p>比如给定一个字符串：<code>s='acddcyyrc'</code>，删除后结果为<code>'arc'</code>.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">removeDuplicates</span><span class="params">(String S)</span> </span>&#123;</span><br><span class="line">       StringBuffer stack = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">       <span class="keyword">int</span> top = -<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; S.length(); ++i) &#123;</span><br><span class="line">           <span class="keyword">char</span> ch = S.charAt(i);</span><br><span class="line">           <span class="keyword">if</span> (top &gt;= <span class="number">0</span> &amp;&amp; stack.charAt(top) == ch) &#123;</span><br><span class="line">               stack.deleteCharAt(top);</span><br><span class="line">               --top;</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               stack.append(ch);</span><br><span class="line">               ++top;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> stack.toString();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>文件迁移</title>
    <url>/2021/03/09/%E6%96%87%E4%BB%B6%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<!-- more -->
<p>我们在安装各种软件的时候都是尽量避免把其装在系统盘，但是不乏有一些难以驯服之辈，硬是要往系统盘挤(VC之流).如何能够在不影响<strong>软件缓存</strong>和后续<strong>更新</strong>及<strong>插件安装</strong>的情况下将系统盘的空间抢夺回来呢？其实Windowns官方已经给出了解决方案。</p>
<h1 id="解决方案">解决方案</h1>
<p>将系统盘要移动的文件夹剪切到想存放的位置。然后只需在命令行中键入如下的命令即可完成文件的迁移。命令行应用管理员身份运行，<strong>link</strong>应不存在而<strong>target</strong>应存在。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink &#x2F;d &quot;link&quot; &quot;target&quot;</span><br></pre></td></tr></table></figure> # 命令说明 此命令的完整形式为： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink [&#x2F;d] [&#x2F;h] [&#x2F;j] &quot;link&quot; &quot;target&quot;</span><br></pre></td></tr></table></figure> 参数|——|解释 :--:|:--:|:-- <strong>/d</strong>|可选|创建<strong>目录</strong>符号链接。 默认情况下，此命令将创建<strong>文件</strong>符号链接。 <strong>/h</strong>|可选|创建硬链接，而不是符号链接。 <strong>/j</strong>|可选|创建目录连接。 <strong>&quot;link&quot;</strong>|必须|指定正在创建的连接名称 <strong>&quot;target&quot;</strong>|必须|指定新符号链接引用的路径(相对或绝对)。 <strong>/?</strong>|其他|help命令。</p>
<h1 id="补充说明">补充说明</h1>
<p>使用 <strong>/d</strong> 方式所创建的文件连接想当于一个快捷方式。存在此处存放了目标文件的地址。在此打开文件时相当于自动跳转至目标文件夹。往里面写文件夹也相当于对目标原文件夹进行直接操作。</p>
<h2 id="软连接和硬连接的区别">软连接和硬连接的区别</h2>
<p><strong>软连接</strong>指的就是由 <strong>/d</strong> 创建的<strong>符号连接</strong>，<strong>硬连接</strong>是由 <strong>/h</strong> 创建的文件连接。简单来说他们各自的含义如下： * <strong>软连接：</strong> 创建的连接相当于目标文件的一个快捷方式。链接不占用空间，将目标文件删除后链接也会失效。 * <strong>硬连接</strong> 创建的硬链接相当于一个镜像备份，对其中的一个文件内容修改将会同步到其他文件，单独删除其中一个链接文件对其他无影响，只有将全部的备份删除后才能真正的将原文件删除。</p>
<h2 id="目录符号连接与目录连接">目录符号连接与目录连接</h2>
<p><strong>目录符号链接/d</strong>与<strong>目录连接/j</strong>非常相似，但是本质是不同的。目录符号链接依旧是符号链接，是指向目录的符号链接，而目录连接点不属于符号链接。 * <strong>目录符号链接</strong>允许 Target 使用相对路径，当使用相对路径时创建目录符号链接之后，如果移动了符号链接文件，操作系统将无法再找到原来的目标。 * <strong>目录连接</strong>只允许 Target 使用绝对路径，当创建目录连接点时，如果传入的参数是相对路径，mklink 命令会自动将相对路径补全为绝对路径。 * 当目录符号链接使用绝对路径时，其行为与目录连接点完全一样。 * 此外，目录符号链接还可以指定 SMB 远程网络中的路径，而目录连接点不可以。</p>
]]></content>
      <categories>
        <category>Tips</category>
      </categories>
      <tags>
        <tag>PC</tag>
      </tags>
  </entry>
  <entry>
    <title>树状数组，了解三点就够了</title>
    <url>/2021/12/01/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%EF%BC%8C%E4%BA%86%E8%A7%A3%E4%B8%89%E7%82%B9%E5%B0%B1%E5%A4%9F%E4%BA%86/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>何为树状数组？首先引用百度百科的定义：<strong>树状数组</strong>或<strong>二叉索引树</strong>（英语：Binary Indexed Tree），又以其发明者命名为Fenwick树，最早由Peter M. Fenwick于1994年以A New Data Structure for Cumulative Frequency Tables为题发表在SOFTWARE PRACTICE AND EXPERIENCE。其初衷是解决数据压缩里的累积频率（Cumulative Frequency）的计算问题，现多用于高效计算数列的前缀和， 区间和。</p>
<p>说白了树状数组就是用来高效计算并修改数列前缀和的一种高级数据结构，和线段树类似，但是实现起来比线段树复杂，能够实现的功能相较于线段树也会较少一些。</p>
<p>普通的<strong>前缀和数组</strong>求取前缀和的时间复杂度为<span class="math inline">\(O(1)\)</span>，而更新前缀和的时间复杂度为<span class="math inline">\(O(n)\)</span>​​，而<strong>树状数组</strong>求取和更新前缀和的时间复杂度均为<span class="math inline">\(O(log(n))\)</span>。因此，当对数组的更新较为频繁时，可以使用树状数组来进行维护。而更新较少时使用普通的前缀和数组维护可得到更好的时间复制度。</p>
<h1 id="要点">要点</h1>
<p>一图概括树状数组。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211201TreeArr0.png" /></p>
<p>上图中，arr表示原始数组，treeArr表示树状数组，注意，建立树状数组的时候，索引<strong>从1开始</strong>，将索引0处留出来，这是为了方便进行位寻址操作。所以对一个长度为n的原始数组，建立成树状数组后长度将变为n+1.</p>
<h2 id="树状数组的三个核心操作">树状数组的三个核心操作</h2>
<ul>
<li>取二进制中的最低位1：<code>lowbit（x）</code></li>
</ul>
<p>根据补码知识，一个数与上它的相反数就可以取出其的最低位1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lowbit</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x &amp; (-x)</span><br></pre></td></tr></table></figure>
<ul>
<li>更新操作（往父节点传播），索引变化：<code>x+lowbit(x)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, index, value</span>):</span></span><br><span class="line">    self.arr[index] += value</span><br><span class="line">    <span class="keyword">while</span> index &lt;= self.length:</span><br><span class="line">        self.treeArr[index] += value</span><br><span class="line">        index += lowbit(index)</span><br></pre></td></tr></table></figure>
<ul>
<li>求和操作(下级传播)，索引变化：<code>x-lowbit(x)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prefix_sum</span>(<span class="params">self, index</span>):</span></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> index:</span><br><span class="line">        ans += self.treeArr[index]</span><br><span class="line">        index -= lowbit(index)</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>树状数组的每一个节点管理的数据数量为其最低位1的值，节点的值表示所管理的所有数之和。例如，索引为4(100)的节点最低位1为(100)，所管理的数据量为4，表示<span class="math inline">\(arr_1+arr_2+arr_3+arr_4\)</span>;索引为5(101)的节点管理的数据量为1（最低位1为1），于是t5值所表示的数为arr5，对于任何一个<span class="math inline">\(t_i\)</span>，其值为<span class="math inline">\(sum(arr(i-lowbit(i),i])\)</span>​​.</p>
<p>树状数组求前缀和时，只需将所有当前索引不断去掉低位1的数据进行求和即可。以t7求和为例，7的二进制为111，去掉低位一后为110，继续得到100，最终得到000（000没用到，其值也定位0，不予理会）。所以前7个数的前缀和就可以使用t7(111)+t6(110)+t4(100)求得。求区间和时可以使用两前缀和相减：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interval_sum</span>(<span class="params">self, start, end</span>):</span></span><br><span class="line">    <span class="keyword">return</span> self.prefix_sum(end - <span class="number">1</span>) - self.prefix_sum(start - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>树状数组进行更新时需要更新当前节点及其所有的父节点，如对arr3加上4，需要对t3(11)+4，再对t4(100)+4，再对t8(1000)+4，这个过程进行的操作为<code>x+lowbit(x)</code>.</p>
<h2 id="树状数组的初始化">树状数组的初始化</h2>
<p>树状数组在初始化时，可直接使用更新操作来建立，时间复制度为<span class="math inline">\(O(nlog(n))\)</span>​.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, origin</span>):</span></span><br><span class="line">    self.length = <span class="built_in">len</span>(origin)</span><br><span class="line">    self.arr = [<span class="number">0</span>] * (self.length + <span class="number">1</span>)</span><br><span class="line">    self.treeArr = [<span class="number">0</span>] * (self.length + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.length + <span class="number">1</span>):</span><br><span class="line">        self.update(i, origin[i - <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>也可以使用直接求和来建立，时间复杂度为<span class="math inline">\(O(n^2)\)</span>​​​，(但是此方法在python中比上面的快很多，或许python的sum有特殊机制，迷惑~~)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, origin</span>):</span></span><br><span class="line">    self.length = <span class="built_in">len</span>(origin)</span><br><span class="line">    self.arr = [<span class="number">0</span>] + origin[:]</span><br><span class="line">    self.treeArr = [<span class="number">0</span>] * (self.length + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.length + <span class="number">1</span>):</span><br><span class="line">        self.treeArr[i] = <span class="built_in">sum</span>(self.arr[i - lowbit(i) + <span class="number">1</span>:i + <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h1 id="end">End</h1>
<p>最后放个完整示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeArray</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, origin</span>):</span></span><br><span class="line">        self.length = <span class="built_in">len</span>(origin)</span><br><span class="line">        <span class="comment"># self.arr = [0] + origin[:]</span></span><br><span class="line">        self.arr = [<span class="number">0</span>] * (self.length + <span class="number">1</span>)</span><br><span class="line">        self.treeArr = [<span class="number">0</span>] * (self.length + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.length + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># self.treeArr[i] = sum(self.arr[i - lowbit(i) + 1:i + 1])</span></span><br><span class="line">            self.update(i, origin[i - <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, index, value</span>):</span> <span class="comment"># 索引index处加上value</span></span><br><span class="line">        self.arr[index] += value</span><br><span class="line">        <span class="keyword">while</span> index &lt;= self.length:</span><br><span class="line">            self.treeArr[index] += value</span><br><span class="line">            index += lowbit(index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prefix_sum</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> index:</span><br><span class="line">            ans += self.treeArr[index]</span><br><span class="line">            index -= lowbit(index)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">interval_sum</span>(<span class="params">self, start, end</span>):</span> <span class="comment"># [start,end)左闭右开</span></span><br><span class="line">        <span class="keyword">return</span> self.prefix_sum(end - <span class="number">1</span>) - self.prefix_sum(start - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_current</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;arr:&#x27;</span>, self.arr)</span><br><span class="line">        print(<span class="string">&#x27;Tree:&#x27;</span>, self.treeArr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lowbit</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x &amp; (-x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    origin = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>)]</span><br><span class="line">    s = TreeArray(origin)</span><br><span class="line">    s.get_current()</span><br><span class="line">    print(s.interval_sum(<span class="number">3</span>, <span class="number">8</span>))</span><br><span class="line">    s.update(<span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line">    s.get_current()</span><br><span class="line">    print(s.interval_sum(<span class="number">3</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">[out]:</span><br><span class="line">arr: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">Tree: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">7</span>, <span class="number">36</span>, <span class="number">9</span>]</span><br><span class="line"><span class="number">25</span></span><br><span class="line">arr: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">Tree: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">11</span>, <span class="number">7</span>, <span class="number">42</span>, <span class="number">9</span>]</span><br><span class="line"><span class="number">31</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> origin []<span class="keyword">int</span></span><br><span class="line">	n := <span class="number">8</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt;= n; i++ &#123;</span><br><span class="line">		origin = <span class="built_in">append</span>(origin, i)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">var</span> tree *treeArr</span><br><span class="line">	tree = <span class="built_in">new</span>(treeArr)</span><br><span class="line">	tree.init(origin)</span><br><span class="line">	tree.<span class="built_in">print</span>()</span><br><span class="line">	fmt.Printf(<span class="string">&quot;sum[3:8]=%d\n&quot;</span>, tree.interval_sum(<span class="number">3</span>, <span class="number">8</span>))</span><br><span class="line">	tree.update(<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">	tree.<span class="built_in">print</span>()</span><br><span class="line">	fmt.Printf(<span class="string">&quot;sum[3:8]=%d\n&quot;</span>, tree.interval_sum(<span class="number">3</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> treeArr <span class="keyword">struct</span> &#123;</span><br><span class="line">	arr     []<span class="keyword">int</span></span><br><span class="line">	treeArr []<span class="keyword">int</span></span><br><span class="line">	length  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(the *treeArr)</span> <span class="title">init</span><span class="params">(arr []<span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	the.length = <span class="built_in">len</span>(arr)</span><br><span class="line">	the.arr = <span class="built_in">make</span>([]<span class="keyword">int</span>, the.length+<span class="number">1</span>)</span><br><span class="line">	the.treeArr = <span class="built_in">make</span>([]<span class="keyword">int</span>, the.length+<span class="number">1</span>)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">1</span>; i &lt;= the.length; i++ &#123;</span><br><span class="line">		the.update(i, arr[i<span class="number">-1</span>])</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(the *treeArr)</span> <span class="title">update</span><span class="params">(index, value <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">	the.arr[index] = value</span><br><span class="line">	<span class="keyword">for</span> index &lt;= the.length &#123;</span><br><span class="line">		the.treeArr[index] += value</span><br><span class="line">		index += lowbit(index)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(the *treeArr)</span> <span class="title">prefix_sum</span><span class="params">(index <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	ans := <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> index &gt; <span class="number">0</span> &#123;</span><br><span class="line">		ans += the.treeArr[index]</span><br><span class="line">		index -= lowbit(index)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> ans</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(the *treeArr)</span> <span class="title">print</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(the.arr)</span><br><span class="line">	fmt.Println(the.treeArr)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(the *treeArr)</span> <span class="title">interval_sum</span><span class="params">(start, end <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> the.prefix_sum(end<span class="number">-1</span>) - the.prefix_sum(start<span class="number">-1</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">lowbit</span><span class="params">(x <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> x &amp; (-x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[out]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">3</span> <span class="number">3</span> <span class="number">10</span> <span class="number">5</span> <span class="number">11</span> <span class="number">7</span> <span class="number">36</span>]</span><br><span class="line">sum[<span class="number">3</span>:<span class="number">8</span>]=<span class="number">25</span></span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">6</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">3</span> <span class="number">3</span> <span class="number">16</span> <span class="number">5</span> <span class="number">11</span> <span class="number">7</span> <span class="number">42</span>]</span><br><span class="line">sum[<span class="number">3</span>:<span class="number">8</span>]=<span class="number">31</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>树形结构</tag>
      </tags>
  </entry>
  <entry>
    <title>死亡密码</title>
    <url>/2021/06/25/%E6%AD%BB%E4%BA%A1%E5%AF%86%E7%A0%81/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><strong>(Leetcode 752)</strong></p>
<p>你有一个带有四个圆形拨轮的转盘锁。每个拨轮都有10个数字： '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' 。每个拨轮可以自由旋转：例如把 '9' 变为 '0'，'0' 变为 '9' 。每次旋转都只能旋转一个拨轮的一位数字。</p>
<p>锁的初始数字为 '0000' ，一个代表四个拨轮的数字的字符串。</p>
<p>列表 <code>deadends</code>包含了一组死亡数字，一旦拨轮的数字和列表里的任何一个元素相同，这个锁将会被永久锁定，无法再被旋转。</p>
<p>字符串 <code>target</code> 代表可以解锁的数字，你需要给出解锁需要的最小旋转次数，如果无论如何不能解锁，返回 -1 。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：deadends &#x3D; [&quot;0201&quot;,&quot;0101&quot;,&quot;0102&quot;,&quot;1212&quot;,&quot;2002&quot;], target &#x3D; &quot;0202&quot;</span><br><span class="line">输出：6</span><br><span class="line">解释：</span><br><span class="line">可能的移动序列为 &quot;0000&quot; -&gt; &quot;1000&quot; -&gt; &quot;1100&quot; -&gt; &quot;1200&quot; -&gt; &quot;1201&quot; -&gt; &quot;1202&quot; -&gt; &quot;0202&quot;。</span><br><span class="line">注意 &quot;0000&quot; -&gt; &quot;0001&quot; -&gt; &quot;0002&quot; -&gt; &quot;0102&quot; -&gt; &quot;0202&quot; 这样的序列是不能解锁的，</span><br><span class="line">因为当拨动到 &quot;0102&quot; 时这个锁就会被锁定。</span><br></pre></td></tr></table></figure>
<p><strong>示例2：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: deadends &#x3D; [&quot;8888&quot;], target &#x3D; &quot;0009&quot;</span><br><span class="line">输出：1</span><br><span class="line">解释：</span><br><span class="line">把最后一位反向旋转一次即可 &quot;0000&quot; -&gt; &quot;0009&quot;。</span><br></pre></td></tr></table></figure>
<p><strong>示例3：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: deadends &#x3D; [&quot;8887&quot;,&quot;8889&quot;,&quot;8878&quot;,&quot;8898&quot;,&quot;8788&quot;,&quot;8988&quot;,&quot;7888&quot;,&quot;9888&quot;], target &#x3D; &quot;8888&quot;</span><br><span class="line">输出：-1</span><br><span class="line">解释：</span><br><span class="line">无法旋转到目标数字且不被锁定。</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="题解">题解</h1>
<h2 id="bfs">BFS</h2>
<p>最短路径问题，采用广度优先搜索(BFS)可以解决。为节省空间，采用双向BFS.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">openLock</span>(<span class="params">deadends, target</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;0000&quot;</span> <span class="keyword">in</span> deadends:</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> target == <span class="string">&quot;0000&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    Qhead = [<span class="string">&quot;0000&quot;</span>] <span class="comment"># 第一队列，从头遍历</span></span><br><span class="line">    Qend = [target] <span class="comment"># 第二队列，从尾遍历</span></span><br><span class="line">    sethead = <span class="built_in">dict</span>() <span class="comment"># 记录头队列所遍历过的数据及步数</span></span><br><span class="line">    setend = <span class="built_in">dict</span>() <span class="comment"># 记录尾队列所遍历过的数据及步数</span></span><br><span class="line">    sethead[<span class="string">&quot;0000&quot;</span>] = <span class="number">0</span></span><br><span class="line">    setend[target] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">upchange</span>(<span class="params">ch</span>):</span> <span class="comment"># 向上调整(转动)一次密码盘</span></span><br><span class="line">        num = <span class="built_in">int</span>(ch)</span><br><span class="line">        <span class="keyword">if</span> num == <span class="number">9</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(num + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">downchange</span>(<span class="params">ch</span>):</span> <span class="comment"># 向下调整(转动)一次密码盘</span></span><br><span class="line">        num = <span class="built_in">int</span>(ch)</span><br><span class="line">        <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;9&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(num - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">broad</span>(<span class="params">key</span>):</span> <span class="comment"># 得到当前密码周围的8个其他密码</span></span><br><span class="line">        newKey = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            tempStr1 = <span class="string">&quot;&quot;</span></span><br><span class="line">            tempStr2 = <span class="string">&quot;&quot;</span></span><br><span class="line">            t1 = upchange(key[i])</span><br><span class="line">            t2 = downchange(key[i])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> j == i:</span><br><span class="line">                    tempStr1 += t1</span><br><span class="line">                    tempStr2 += t2</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    tempStr1 += key[j]</span><br><span class="line">                    tempStr2 += key[j]</span><br><span class="line">            newKey.append(tempStr1)</span><br><span class="line">            newKey.append(tempStr2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> newKey</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BFS</span>(<span class="params">sethead, setend</span>):</span></span><br><span class="line">        <span class="keyword">while</span> Qhead != [] <span class="keyword">and</span> Qend != []: <span class="comment"># 一旦某一个队列为空，说明被死亡密码封锁，无法转动到目标密码，跳出循环返回-1</span></span><br><span class="line">            QheadLength = <span class="built_in">len</span>(Qhead)</span><br><span class="line">            QendLength = <span class="built_in">len</span>(Qend)</span><br><span class="line">            <span class="keyword">if</span> QheadLength &lt; QendLength: <span class="comment"># 队列小的优先扩散，平衡空间复杂度</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(QheadLength):</span><br><span class="line">                    step = sethead[Qhead[<span class="number">0</span>]] <span class="comment"># 转到当前密码所需步数</span></span><br><span class="line">                    temp = []</span><br><span class="line">                    <span class="keyword">for</span> c <span class="keyword">in</span> Qhead[<span class="number">0</span>]:</span><br><span class="line">                        temp.append(c)</span><br><span class="line">                    temp = broad(temp)</span><br><span class="line">                    <span class="keyword">for</span> t <span class="keyword">in</span> temp:</span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> sethead: <span class="comment"># 记录以遍历密码，防止重复遍历</span></span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> deadends: <span class="comment"># 遇到死亡密码，进行下一个</span></span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> Qhead: <span class="comment"># 前有哈希表判断，此条件可以省略</span></span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        sethead[t] = step + <span class="number">1</span> <span class="comment"># 新的密码加入哈希表，步数加1</span></span><br><span class="line">                        Qhead.append(t) <span class="comment"># 新密码加入队列</span></span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> setend:</span><br><span class="line">                            <span class="keyword">return</span> sethead[t] + setend[t]</span><br><span class="line">                    Qhead.pop(<span class="number">0</span>) <span class="comment"># 移除所有方向已遍历完成的密码</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 结构同上</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(QendLength):</span><br><span class="line">                    step = setend[Qend[<span class="number">0</span>]]</span><br><span class="line">                    temp = []</span><br><span class="line">                    <span class="keyword">for</span> c <span class="keyword">in</span> Qend[<span class="number">0</span>]:</span><br><span class="line">                        temp.append(c)</span><br><span class="line">                    temp = broad(temp)</span><br><span class="line">                    <span class="keyword">for</span> t <span class="keyword">in</span> temp:</span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> setend:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> deadends:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> Qend:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        setend[t] = step + <span class="number">1</span></span><br><span class="line">                        Qend.append(t)</span><br><span class="line">                        <span class="keyword">if</span> t <span class="keyword">in</span> sethead:</span><br><span class="line">                            <span class="keyword">return</span> sethead[t] + setend[t]</span><br><span class="line">                    Qend.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    step = BFS(sethead, setend)</span><br><span class="line">    <span class="keyword">return</span> step</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    step = openLock([<span class="string">&quot;0201&quot;</span>, <span class="string">&quot;0101&quot;</span>, <span class="string">&quot;0102&quot;</span>, <span class="string">&quot;1212&quot;</span>, <span class="string">&quot;2002&quot;</span>], <span class="string">&quot;0202&quot;</span>)</span><br><span class="line">    print(step)</span><br><span class="line"></span><br><span class="line">[out]:<span class="number">6</span></span><br></pre></td></tr></table></figure>
<h2 id="启发式搜索">启发式搜索</h2>
<p>可以使用启发式搜索更快找到最小旋转次数，此处可以使用可以使用<span class="math inline">\(A^*\)</span>算法。</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>题录</category>
      </categories>
      <tags>
        <tag>题录</tag>
      </tags>
  </entry>
  <entry>
    <title>浅析HTTP</title>
    <url>/2021/04/13/%E6%B5%85%E6%9E%90HTTP/</url>
    <content><![CDATA[<h1 id="http简介">HTTP简介</h1>
<p>HTTP 是超⽂文本传输协议，即HyperText Transfer Protocol.</p>
<p>正如其名，HTTP可以分为以下三个部分：</p>
<ul>
<li>超文本</li>
<li>传输</li>
<li>协议</li>
</ul>
<p>它们之间的关系如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210413http1.png" alt="image-20210413214817316" /><figcaption>image-20210413214817316</figcaption>
</figure>
<h2 id="超文本">超文本</h2>
<p>在互联⽹网早期的时候文本仅仅指代只是简单的字符⽂字，但现在文本的涵义已经可以扩展为图片、视频、压缩包等，在 HTTP 眼⾥里这些都算作文本。</p>
<p>超文本就是超过普通文本的一种文本，它是⽂字、图片、视频等的混合体，最关键有超链接，能从一个超⽂文本跳转到另外一个超文本。</p>
<p>比如说常见的HTML就是一个超文本，它本身是纯文本文件，但是其中可以使用标签来定义图片、视频的链接，再通过浏览器对它进行解析，就能得到一个文字、画面共存的网页了。</p>
<h2 id="传输">传输</h2>
<p>传输就是把一堆东西从一个地方搬到另一个地方，在网络上体现为服务器、客户端之间的数据交换。我们在浏览网页的时候，浏览器是请求⽅方 ，网站就是应答⽅。双方约定用 HTTP 协议来通信，于是浏览器把请求数据发送给网站，网站再把一些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图片、视频了。</p>
<h2 id="协议">协议</h2>
<p>协议代表的是对参与者的一种行为约定和规范。</p>
<p>HTTP是一个用在计算机世界里的协议。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范(两个以上的参与者),以及相关的各种控制和错误处理方式(行为约定和规范）。</p>
<p>最终可以将HTTP简单定义为：一个在计算机世界里专门在<strong>两点</strong>之间<strong>传输</strong>文字、图片、音频、视频等<strong>超文本</strong>数据 的<strong>约定和规范 </strong>。</p>
<hr />
<h1 id="htttp的常见状态码">HTTTP的常见状态码</h1>
<p>HTTP常见的状态码有以下五大类：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">状态码</th>
<th style="text-align: left;">含义</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1xx</td>
<td style="text-align: left;">提示信息，表示目前是协议处理的中间状态，还需进行后续操作</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">2xx</td>
<td style="text-align: left;">成功，报文已接收并被正确处理</td>
<td>200、204、206</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3xx</td>
<td style="text-align: left;">重定向，资源位置发生变动，需要客户端重新发送请求</td>
<td>301、302、304</td>
</tr>
<tr class="even">
<td style="text-align: left;">4xx</td>
<td style="text-align: left;">客户端错误，请求报文有误，服务器无法处理</td>
<td>400、403、404</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5xx</td>
<td style="text-align: left;">服务器错误，服务器在请求时内部发生了错误</td>
<td>500、501、502、503</td>
</tr>
</tbody>
</table>
<p>示例解析：</p>
<blockquote>
<p>1xx</p>
<blockquote>
<p>此类状态码属于提示信息,是协议处理中的一种中间状态,实际用到的比较少。</p>
</blockquote>
<p>2xx</p>
<blockquote>
<p>此类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。</p>
<p><strong>200 OK</strong> 请求成功。一般用于GET与POST请求。</p>
<p><strong>201 Created</strong> 已创建。成功请求并创建了新的资源。</p>
<p><strong>202 Accepted</strong> 已接受。已经接受请求，但未处理完成。</p>
</blockquote>
<p>3xx</p>
<blockquote>
<p><strong>301 Moved Permanently</strong> 表示永久重定向,说明请求的资源已经不存在了,需改用新的URL再次访问。</p>
<p><strong>302 Found</strong> 表示临时重定向，说明请求的资源还在，但暂时需要⽤用另一个 URL 来访问。</p>
<p><strong>304 Not Modified</strong> 不具有跳转的含义，表示资源未修改，重定向已存在的缓冲⽂文件，也称缓存重定向，用于缓存控制。</p>
<p><strong>305 Use Proxy</strong> 使用代理。所请求的资源必须通过代理访问。</p>
<p><strong>306 Unused</strong> 已经被废弃的HTTP状态码。</p>
</blockquote>
<p>4xx</p>
<blockquote>
<p><strong>400 Bad Reques</strong> 表示客户端请求的报文有错误，但只是个笼统的错误。</p>
<p><strong>401 Unauthorized</strong> 请求要求用户的身份认证。</p>
<p><strong>402 Payment Required</strong> 保留，将来使用。</p>
<p><strong>403 Forbidden</strong> 表示服务器禁止访问资源，并不是客户端的请求出错。</p>
<p><strong>404 Not Found</strong> 表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。</p>
<p><strong>405 Method Not Allowed</strong> 客户端请求中的方法被禁止。</p>
<p><strong>406 Not Acceptable</strong> 服务器无法根据客户端请求的内容特性完成请求。</p>
<p><strong>408 Request Time-out</strong> 服务器等待客户端发送的请求时间过长，超时。</p>
</blockquote>
<p>5xx</p>
<blockquote>
<p><strong>500 Internal Server Error</strong> 与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。</p>
<p><strong>501 Not Implemented</strong> 表示客户端请求的功能还不支持。</p>
<p><strong>502 Bad Gateway</strong> 通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。</p>
<p><strong>503 Service Unavailable</strong> 表示服务器当前很忙，暂时无法响应服务器。</p>
<p><strong>504 Gateway Time-out</strong> 充当网关或代理的服务器，未及时从远端服务器获取请求。</p>
<p><strong>505 HTTP Version not supported</strong> 服务器不支持请求的HTTP协议的版本，无法完成处理。</p>
</blockquote>
</blockquote>
<hr />
<h1 id="http的常见字段">HTTP的常见字段</h1>
<h2 id="host">Host</h2>
<p>Host 的作用是客户端发送请求时，用来指定服务器的域名。例如<code>Host: www.baidu.com.</code></p>
<h2 id="content-length">Content-Length</h2>
<p>服务器在返回数据时，会有Content-Length字段，表明本次回应的数据长度。<code>Content-Length: 1000</code></p>
<h2 id="connection">Connection</h2>
<p>Connection字段最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用。<code>Connection: keep-alive</code></p>
<h2 id="content-type">Content-Type</h2>
<p>用于服务器回应时，告诉客户端，本次数据是什么格式。<code>Content-Type: text/html; charset=utf-8</code></p>
<h2 id="accept">Accept</h2>
<p>客户端请求的时候，可以使用Accept字段声明自己可以接受哪些数据格式。<code>Accept: */*</code>表明接受任何格式的数据。</p>
<h2 id="content-encoding">Content-Encoding</h2>
<p>说明数据的压缩⽅方法。表示服务器返回的数据使⽤用了什么压缩格式。<code>Content-Encoding: gzip</code></p>
<h2 id="accept-encoding">Accept-Encoding</h2>
<p>客户端在请求时，⽤用Accept-Encoding字段说明自己可以接受哪些压缩方法。<code>Accept-Encoding: gzip, deflate</code></p>
<hr />
<h1 id="get方法与post方法">GET方法与POST方法</h1>
<h2 id="get">GET</h2>
<p>Get为获取的意思，此方法的含义是请求从服务器获取资源，这个资源可以是静态的文本、页面、图片视频等。</p>
<p>当打开一个网页，浏览器就会发送Get请求到服务器，服务器将会返回文本及资源。</p>
<h2 id="post">Post</h2>
<p>此方法是想服务器提交数据，具体内容放在报文的body里面，与请求头拼接在一起发送给服务器。</p>
<h2 id="两者安全且等幂">两者安全且等幂？</h2>
<ul>
<li>安全指的是请求方法不会破坏服务器上的资源</li>
<li>等幂指的是多次执行相同的操作，结果都是相同的</li>
</ul>
<p>显然，Get方法是安全且等幂的，Post方法是不安全且不等幂的。</p>
<hr />
<h1 id="http-与-https">HTTP 与 HTTPS</h1>
<p>HTTP 由于是明文传输，所以安全上存在以下三个风险：</p>
<ul>
<li>窃听风险，比如通信链路路上可以获取通信内容，用户号容易被盗。</li>
<li>篡改风险，比如强制植入垃圾广告，视觉污染。</li>
<li>冒充风险，比如冒充淘宝网等购物网站。</li>
</ul>
<p>HTTPS在 HTTP 与 TCP 层之间加入了SSL/TLS协议，可以很好的解决了上述的风险。</p>
<p>HTTPS（全称：Hyper Text Transfer Protocol over SecureSocket Layer），主要由两部分组成：HTTP + SSL / TLS，也就是在 HTTP 上又加了一层处理加密信息的模块。服务端和客户端的信息传输都会通过 TLS 进行加密，所以传输的数据都是加密后的数据。</p>
<h2 id="https工作流程">HTTPS工作流程</h2>
<ol type="1">
<li>客户端将它所支持的算法列表和一个用作产生密钥的随机数发送给服务器；</li>
<li>服务器从算法列表中选择一种加密算法，并将它和一份包含服务器公用密钥的证书发送给客户端；该证书还包含了用于认证目的的服务器标识，服务器同时还提供了一个用作产生密钥的随机数；</li>
<li>客户端对服务器的证书进行验证（有关验证证书，可以参考数字签名），并抽取服务器的公用密钥；然后，再产生一个称作 pre_master_secret 的随机密码串，并使用服务器的公用密钥对其进行加密（参考非对称加 / 解密），并将加密后的信息发送给服务器；</li>
<li>客户端与服务器端根据 pre_master_secret 以及客户端与服务器的随机数值独立计算出加密和 MAC密钥（参考 DH密钥交换算法）；</li>
<li>客户端将所有握手消息的 MAC 值发送给服务器</li>
<li>服务器将所有握手消息的 MAC 值发送给客户端</li>
</ol>
<h2 id="https的优缺点">HTTPS的优缺点</h2>
<ul>
<li>优点：确保数据发送到正确的客户机和服务器，防止数据在传输过程中被窃取、改变，确保数据的完整性。</li>
<li>缺点：页面的加载时间延长近 50%，影响缓存，增加数据开销和功耗，增加成本；HTTPS 协议的安全是有范围的，在黑客攻击、拒绝服务攻击和服务器劫持等方面几乎起不到什么作用。</li>
</ul>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>消失的cmp</title>
    <url>/2021/12/04/%E6%B6%88%E5%A4%B1%E7%9A%84cmp/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>cmp是compare的缩写，顾名思义，它的作用用于比较。在python2或C/C++等语言中，cmp函数允许自定义排序函数，即接收两个参数，根据两个参数的关系来决定返回-1（参数1排在参数2之前），0（相等），1（参数1排在参数2之后）三种数值。cmp常用于对列表进行客制化排序。</p>
<h1 id="python2中的cmp">python2中的cmp</h1>
<p>在python2中，sorted排序有三个参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sorted</span>(iterable[,cmp,[,key[,reverse=<span class="literal">True</span>]]])</span><br></pre></td></tr></table></figure>
<p>默认情况下返回从小到大排序的列表。</p>
<p>第一个参数是一个iterable，返回值是一个对iterable中元素进行排序后的列表(list)。</p>
<p>可选的参数有三个，cmp、key和reverse，各自作用如下：</p>
<ul>
<li><p>cmp指定一个定制的<a href="https://www.zhihu.com/search?q=比较函数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A139574674%7D">比较函数</a>，这个函数接收两个参数（iterable的元素），如果第一个参数小于第二个参数，返回一个负数；如果第一个参数等于第二个参数，返回零；如果第一个参数大于第二个参数，返回一个正数。默认值为None。</p></li>
<li><p>key指定一个接收一个参数的函数，这个函数用于从每个元素中提取一个用于比较的关键字。默认值为None。key参数的值应该是一个函数，这个函数接收一个参数并且返回一个用于比较的关键字。对复杂对象的比较通常是使用对象的切片作为关键字。</p></li>
<li><p>reverse是一个<a href="https://www.zhihu.com/search?q=布尔值&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A139574674%7D">布尔值</a>。如果设置为True，列表元素将被倒序排列。</p></li>
</ul>
<h1 id="python3中的cmp">python3中的cmp</h1>
<p>python3废弃了python2 中sorted函数的cmp参数，python3中的sorted形式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sorted</span>(iterable，key=<span class="literal">None</span>,reverse=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>它的声明如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sorted</span>(<span class="params">*args, **kwargs</span>):</span> <span class="comment"># real signature unknown</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return a new list containing all items from the iterable in ascending order.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    A custom key function can be supplied to customize the sort order, and the</span></span><br><span class="line"><span class="string">    reverse flag can be set to request the result in descending order.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li>key主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。</li>
<li>reverse是一个布尔值。如果设置为True，列表元素将被倒序排列，默认为False。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">例子：</span><br><span class="line">s &#x3D; &#39;asdf234GDSdsf23&#39; #排序:小写-大写-奇数-偶数</span><br><span class="line"> </span><br><span class="line">print(&quot;&quot;.join(sorted(s, key&#x3D;lambda x: (x.isdigit(),x.isdigit() and int(x) % 2 &#x3D;&#x3D; 0,x.isupper(),x))))</span><br><span class="line">原理：先比较元组的第一个值，FALSE&lt;TRUE，如果相等就比较元组的下一个值，以此类推。</span><br><span class="line"></span><br><span class="line">先看一下Boolean value 的排序：</span><br><span class="line">print(sorted([True,Flase]))&#x3D;&#x3D;&#x3D;&gt;结果[False,True]</span><br><span class="line">Boolean 的排序会将 False 排在前，True排在后 .</span><br><span class="line">1.x.isdigit()的作用是把数字放在后边,字母放在前边.</span><br><span class="line">2.x.isdigit() and int(x) % 2 &#x3D;&#x3D; 0的作用是保证奇数在前，偶数在后。</span><br><span class="line">3.x.isupper()的作用是在前面基础上,保证字母小写在前大写在后.</span><br><span class="line">4.最后的x表示在前面基础上,对所有类别数字或字母排序。</span><br><span class="line">最后结果：addffssDGS33224</span><br></pre></td></tr></table></figure>
<p>在python3中，取消了cmp自定义排序函数。自定义排序只能从key入手，而key参数又只能够接收一个参数，如果要实现两个元素之间的自定义比较方式，可借助cmp_to_key函数。它是functools下的一个函数，用于将一个cmp函数变成一个key函数，从而赋给sorted中的key参数，实现自定义排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">例题：</span><br><span class="line">给定一组非负整数 nums，重新排列每个数的顺序（每个数不可拆分）使之组成一个最大的整数。</span><br><span class="line"></span><br><span class="line">注意：输出结果可能非常大，所以你需要返回一个字符串而不是整数。</span><br><span class="line">输入：nums &#x3D; [3,30,34,5,9]</span><br><span class="line">输出：&quot;9534330&quot;</span><br><span class="line">输入：nums &#x3D; [0,0]</span><br><span class="line">输出：&quot;0&quot;</span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def largestNumber(self, nums) -&gt; str:</span><br><span class="line">    	from functools import cmp_to_key</span><br><span class="line">        if sum(nums)&#x3D;&#x3D;0: # 连续0情况</span><br><span class="line">            return &#39;0&#39;</span><br><span class="line">        nums &#x3D; sorted(nums, key&#x3D;cmp_to_key(lambda x, y: -1 if str(x) + str(y) &gt; str(y) + str(x) else 1))</span><br><span class="line">        return &quot;&quot;.join([str(x) for x in nums])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>爬坑</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>突破封锁线</title>
    <url>/2021/09/17/%E7%AA%81%E7%A0%B4%E5%B0%81%E9%94%81%E7%BA%BF/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p>在一个<span class="math inline">\(10^6 * 10^6\)</span>​​ 的网格中，每个网格上方格的坐标为 <code>(x, y)</code>。</p>
<p>现在从源方格 <code>source = [sx, sy]</code>开始出发，意图赶往目标方格 <code>target = [tx, ty]</code> 。数组 <code>blocked</code>是封锁的方格列表，其中每个 <code>blocked[i] = [xi, yi]</code> 表示坐标为 <code>(xi, yi)</code>的方格是禁止通行的。</p>
<p>每次移动，都可以走到网格中在四个方向上相邻的方格，只要该方格 不 在给出的封锁列表 blocked 上。同时，不允许走出网格。</p>
<p>只有在可以通过一系列的移动从源方格 source 到达目标方格 target 时才返回 true。否则，返回 false。</p>
<p>(Leet1036)</p>
<ul>
<li><strong>示例 1：</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：blocked &#x3D; [[0,1],[1,0]], source &#x3D; [0,0], target &#x3D; [0,2]</span><br><span class="line">输出：false</span><br><span class="line">解释：</span><br><span class="line">从源方格无法到达目标方格，因为我们无法在网格中移动。</span><br><span class="line">无法向北或者向东移动是因为方格禁止通行。</span><br><span class="line">无法向南或者向西移动是因为不能走出网格。</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>示例 2：</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：blocked &#x3D; [], source &#x3D; [0,0], target &#x3D; [999999,999999]</span><br><span class="line">输出：true</span><br><span class="line">解释：</span><br><span class="line">因为没有方格被封锁，所以一定可以到达目标方格。</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>数据量：</strong>
<ul>
<li><code>0 &lt;= blocked.length &lt;= 200</code></li>
<li><code>blocked[i].length == 2</code></li>
<li><code>0 &lt;= xi, yi &lt; 106</code></li>
<li><code>source.length == target.length == 2</code></li>
<li><code>0 &lt;= sx, sy, tx, ty &lt; 106</code></li>
<li><code>source != target</code></li>
<li><code>source 和 target 不在封锁列表内</code></li>
</ul></li>
</ul>
<hr />
<h1 id="解析">解析</h1>
<p>朴素解法采用BFS进行暴力搜索，采用双向BFS来进行优化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEscapePossible</span>(<span class="params">self, blocked, source, target</span>) -&gt; bool:</span></span><br><span class="line">        m = <span class="number">10</span> ** <span class="number">6</span></span><br><span class="line">        hashBlock = <span class="built_in">dict</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [<span class="built_in">tuple</span>(x), <span class="number">0</span>], blocked))</span><br><span class="line">        queue_s = [<span class="built_in">tuple</span>(source)]</span><br><span class="line">        queue_t = [<span class="built_in">tuple</span>(target)]</span><br><span class="line">        hashPoint_s = <span class="built_in">dict</span>()</span><br><span class="line">        hashPoint_t = <span class="built_in">dict</span>()</span><br><span class="line">        hashPoint_s[<span class="built_in">tuple</span>(source)] = <span class="number">0</span></span><br><span class="line">        hashPoint_t[<span class="built_in">tuple</span>(target)] = <span class="number">0</span></span><br><span class="line">        direct = [(-<span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, -<span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">while</span> queue_s != [] <span class="keyword">and</span> queue_t != []:</span><br><span class="line">            len_s = <span class="built_in">len</span>(queue_s)</span><br><span class="line">            len_t = <span class="built_in">len</span>(queue_t)</span><br><span class="line">            <span class="keyword">if</span> len_s &lt; len_t:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_s):</span><br><span class="line">                    point = queue_s.pop(<span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">for</span> d <span class="keyword">in</span> direct:</span><br><span class="line">                        new_point = (point[<span class="number">0</span>] + d[<span class="number">0</span>], point[<span class="number">1</span>] + d[<span class="number">1</span>])</span><br><span class="line">                        <span class="keyword">if</span> new_point[<span class="number">0</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">0</span>] &gt;= m <span class="keyword">or</span> new_point[<span class="number">1</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">1</span>] &gt;= m:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashBlock:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_s:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_t:</span><br><span class="line">                            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                        queue_s.append(new_point)</span><br><span class="line">                        hashPoint_s[new_point] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_t):</span><br><span class="line">                    point = queue_t.pop(<span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">for</span> d <span class="keyword">in</span> direct:</span><br><span class="line">                        new_point = (point[<span class="number">0</span>] + d[<span class="number">0</span>], point[<span class="number">1</span>] + d[<span class="number">1</span>])</span><br><span class="line">                        <span class="keyword">if</span> new_point[<span class="number">0</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">0</span>] &gt;= m <span class="keyword">or</span> new_point[<span class="number">1</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">1</span>] &gt;= m:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashBlock:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_t:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_s:</span><br><span class="line">                            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                        queue_t.append(new_point)</span><br><span class="line">                        hashPoint_t[new_point] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>但是此题数据量较大，使用这样的方式肯定会超时，寻找进一步优化。</p>
<p>此题最终要求解的是能不能突破封锁，不需要求路劲问题，可以进行剪枝。具体做法是：求出所给的封锁点所能够封锁的的总数据点的数量，双向BFS的时候，若最小的一方遍历到的路径点大于所能封锁的最大点的数量，则说明题中所给的封锁点不能封锁所有的路径，直接返回True即可。所给点能封锁的最大点数量可以使用公式<span class="math inline">\(n*(n-1)/2\)</span>​​计算，此时包围情况为封锁线与边界成等腰直角三角形的情形，下图是11个封锁点所能封锁的最大点数量：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210917miss1.png" alt="image-20210917150841216" /><figcaption>image-20210917150841216</figcaption>
</figure>
<p>$$ maxpoints=1+2+...+10== <span class="math inline">\(​​​\)</span></p>
<p>剪枝后的BFS算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEscapePossible</span>(<span class="params">self, blocked, source, target</span>) -&gt; bool:</span></span><br><span class="line">        m = <span class="number">10</span> ** <span class="number">6</span></span><br><span class="line">        hashBlock = <span class="built_in">dict</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [<span class="built_in">tuple</span>(x), <span class="number">0</span>], blocked))</span><br><span class="line">        n = <span class="built_in">len</span>(hashBlock)</span><br><span class="line">        n = n*(n-<span class="number">1</span>)//<span class="number">2</span>  <span class="comment"># 计算可封锁的点的最大数量</span></span><br><span class="line">        queue_s = [<span class="built_in">tuple</span>(source)]</span><br><span class="line">        queue_t = [<span class="built_in">tuple</span>(target)]</span><br><span class="line">        hashPoint_s = <span class="built_in">dict</span>()</span><br><span class="line">        hashPoint_t = <span class="built_in">dict</span>()</span><br><span class="line">        hashPoint_s[<span class="built_in">tuple</span>(source)] = <span class="number">0</span></span><br><span class="line">        hashPoint_t[<span class="built_in">tuple</span>(target)] = <span class="number">0</span></span><br><span class="line">        direct = [(-<span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, -<span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">while</span> queue_s != [] <span class="keyword">and</span> queue_t != []:</span><br><span class="line">            len_s = <span class="built_in">len</span>(queue_s)</span><br><span class="line">            len_t = <span class="built_in">len</span>(queue_t)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">min</span>(<span class="built_in">len</span>(hashPoint_s),<span class="built_in">len</span>(hashPoint_t))&gt;n: <span class="comment"># 判断是否可以冲破封锁</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> len_s &lt; len_t:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_s):</span><br><span class="line">                    point = queue_s.pop(<span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">for</span> d <span class="keyword">in</span> direct:</span><br><span class="line">                        new_point = (point[<span class="number">0</span>] + d[<span class="number">0</span>], point[<span class="number">1</span>] + d[<span class="number">1</span>])</span><br><span class="line">                        <span class="keyword">if</span> new_point[<span class="number">0</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">0</span>] &gt;= m <span class="keyword">or</span> new_point[<span class="number">1</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">1</span>] &gt;= m:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashBlock:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_s:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_t:</span><br><span class="line">                            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                        queue_s.append(new_point)</span><br><span class="line">                        hashPoint_s[new_point] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_t):</span><br><span class="line">                    point = queue_t.pop(<span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">for</span> d <span class="keyword">in</span> direct:</span><br><span class="line">                        new_point = (point[<span class="number">0</span>] + d[<span class="number">0</span>], point[<span class="number">1</span>] + d[<span class="number">1</span>])</span><br><span class="line">                        <span class="keyword">if</span> new_point[<span class="number">0</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">0</span>] &gt;= m <span class="keyword">or</span> new_point[<span class="number">1</span>] &lt; <span class="number">0</span> <span class="keyword">or</span> new_point[<span class="number">1</span>] &gt;= m:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashBlock:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_t:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">if</span> new_point <span class="keyword">in</span> hashPoint_s:</span><br><span class="line">                            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                        queue_t.append(new_point)</span><br><span class="line">                        hashPoint_t[new_point] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
        <category>题录</category>
      </categories>
      <tags>
        <tag>题录</tag>
      </tags>
  </entry>
  <entry>
    <title>薅羊毛之Google</title>
    <url>/2021/07/27/%E8%96%85%E7%BE%8A%E6%AF%9B%E4%B9%8BGoogle/</url>
    <content><![CDATA[<h1 id="简介">简介</h1>
<p><a href="https://colab.research.google.com/">Google Colab</a>是一个云端<strong>Jupyter</strong> 笔记本环境，它是完全<strong>免费</strong>的，唯一的限制条件是需要挂个梯子，毕竟是谷歌的东西。</p>
<hr />
<h1 id="使用方式">使用方式</h1>
<h2 id="创建colaboratory">1.创建Colaboratory</h2>
<p>在谷歌云盘中新建中选择<strong>更多——&gt;Google Colaboratory</strong>建立一个Jupyter文件。</p>
<ul>
<li>创建文件</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210727Colab1.png" alt="image-20210727211250174" /><figcaption>image-20210727211250174</figcaption>
</figure>
<ul>
<li>文件概况</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/202107272Colab2.png" alt="image-20210727211535104" /><figcaption>image-20210727211535104</figcaption>
</figure>
<h2 id="基本使用">2.基本使用</h2>
<p>此文件中的基本命令使用与Jupyter相同，但是它还支持dos命令，只需要加上!前缀即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!pwd</span><br><span class="line">!ls</span><br><span class="line">!pip install pyqt5</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210727Colab3.png" alt="image-20210727212306863" /><figcaption>image-20210727212306863</figcaption>
</figure>
<p>此运行环境本质上是一个linux虚拟机。新建的环境默认使用CPU我们需要将其更改为GPU，在<strong>代码执行程序——&gt;更改运行时类型</strong>中将其改为GPU：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210727Colab4.png" alt="image-20210727212827529" /><figcaption>image-20210727212827529</figcaption>
</figure>
<p><code>!nvidia-smi</code>查看一下Google所分配的GPU，是<strong>Tesla T4</strong>,16G的显存足以应付一般的模型。</p>
<p>但是仅仅在上面简单的运行一些基础命令是不够的。我们需要将我们本地编写的工程文件移植到云端来运行。这需要挂载drive云盘。</p>
<h2 id="挂载drive云盘">3.挂载Drive云盘</h2>
<h3 id="连接云盘">连接云盘</h3>
<p>首先安装一些必要的系统库，并进行授权。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!apt-get install -y -qq software-properties-common python-software-properties module-init-tools</span><br><span class="line">!add-apt-repository -y ppa:alessandro-strada&#x2F;ppa 2&gt;&amp;1 &gt; &#x2F;dev&#x2F;null</span><br><span class="line">!apt-get update -qq 2&gt;&amp;1 &gt; &#x2F;dev&#x2F;null</span><br><span class="line">!apt-get -y install -qq google-drive-ocamlfuse fuse</span><br><span class="line">from google.colab import auth</span><br><span class="line">auth.authenticate_user()</span><br><span class="line">from oauth2client.client import GoogleCredentials</span><br><span class="line">creds &#x3D; GoogleCredentials.get_application_default()</span><br><span class="line">import getpass</span><br><span class="line">!google-drive-ocamlfuse -headless -id&#x3D;&#123;creds.client_id&#125; -secret&#x3D;&#123;creds.client_secret&#125; &lt; &#x2F;dev&#x2F;null 2&gt;&amp;1 | grep URL</span><br><span class="line">vcode &#x3D; getpass.getpass()</span><br><span class="line">!echo &#123;vcode&#125; | google-drive-ocamlfuse -headless -id&#x3D;&#123;creds.client_id&#125; -secret&#x3D;&#123;creds.client_secret&#125;</span><br></pre></td></tr></table></figure>
<p>运行之后出现如下结果：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210727Colab5.png" alt="image-20210727214018841" /><figcaption>image-20210727214018841</figcaption>
</figure>
<p>此时需要点击链接登录google账号复制密匙进行授权(上图为授权完毕后的状态)。每一个Notebook需要进行一次授权。</p>
<h3 id="挂载云盘">挂载云盘</h3>
<p>使用如下命令创建并挂载云盘。运行如下命令后，Notebook运行环境中的drive文件与云盘根目录连接。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!mkdir -p drive</span><br><span class="line">!google-drive-ocamlfuse drive</span><br></pre></td></tr></table></figure>
<h3 id="上传自己的文件">上传自己的文件</h3>
<p>在云盘中建立一个文件夹(例如test)，将自己的工程上传进去</p>
<h3 id="更改工作目录">更改工作目录</h3>
<p>使用os更改工作目录：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">&quot;drive/test&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="运行模型">运行模型</h3>
<p>直接使用dos运行以编写好的代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!python3 test.py</span><br></pre></td></tr></table></figure>
<h1 id="end">End</h1>
<p>但是，Colab的GPU资源并不是无限制使用的，每天有一定的使用限制，而且Notebook有空闲超时自动断开的缺陷。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>贪心</title>
    <url>/2021/03/18/%E8%B4%AA%E5%BF%83/</url>
    <content><![CDATA[<p>最优化问题的求解往往需要通过一系列步骤，每个步骤面临许多选择。对于一些简单的问题若使用动态规划来求解有些大材小用的感觉。 <strong>贪心算法(Greedy)</strong> 便是一种更简单、更高效的算法。 <span id="more"></span> # 概述 贪心算法对一个多步骤逐步求解的问题，将在每一步都选择当前最优的策略，而不去关注之前或后续的步骤。贪心算法在大部分问题上都不会取得最优解，因为它并没有测试所有的可能解，容易过早的做决定。对于有的问题只能得到一个近似最优解，但是在某些问题上它确实能够获得最优解，如求图中的最小生成树、求哈夫曼编码。一旦某问题可以用贪心来求解，那么贪心一般是最高效的解决方式。可以用贪心算法解决的题目需要满足以下性质： * 最优子结构：一个问题的最优解包含其子问题的最优解 * 贪心选择性：所求问题的整体最优解可以通过一系列局部最优的选择来到达，即通过贪心选择来达到</p>
<h1 id="贪心与动态规划">贪心与动态规划</h1>
<p>在动态规划中，每一个步骤都要进行选择，这个选择通常依赖于子问题；而贪心算法并不需要考虑子问题，直接选择看起来最优的解。所以动态规划通常是递推形式，先计算较小的子问题，为较大的问题提供铺垫；在贪心算法中，我们常常自顶向下求解问题，每一次选择局部最优解，最后留下一个唯一的子问题。</p>
<h1 id="贪心过程">贪心过程</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从问题的某一初始解出发</span><br><span class="line">    while (能朝给定总目标前进一步) </span><br><span class="line">        do</span><br><span class="line">            选择当前最优解作为可行解的一个解元素；</span><br><span class="line">    由所有解元素组合成问题的一个可行解。</span><br></pre></td></tr></table></figure>
<h1 id="背包和分数背包">0-1背包和分数背包</h1>
<p>若有三个物体，他们的重量和价值分别为<code>weight = [1, 2, 3]</code>；<code>value = [6, 10, 12]</code>. 他们的平均价值分别为<code>[6, 5, 4]</code>，现在有一个可以装5重量的背包，要如何装才能使得收益最大化？<br />
* 0-1背包：每一种物体只能选择装或者不装。 * 分数背包： 每一种物体可以划分开来装。</p>
<h2 id="背包">0-1 背包</h2>
<p>对于0-1背包问题采用贪心策略：<strong>优先装平均收益最大的物体</strong>。最终的背包将会装下物体1和物体2而不能装下物体3，背包占用3剩余2，总价值为16.但是显而易见的装物体2和物体3才是最优策略，此时背包刚好被占满，总价值为22.所以贪心在0-1背包并不适用。</p>
<h2 id="分数背包">分数背包</h2>
<p>分数背包可以适用贪心获取最优解。证明也是很简单的：背包的容量有限，要是价值最大，其中物体的平均价值就得最大，所以优先装平均价值最大的物体，最终的总价值也将会最高。最终背包装了物体1、物体2和<span class="math inline">\(\frac{2}{3}\)</span>的物体3，最终价值为24.</p>
<h1 id="贪心适用性证明">贪心适用性证明</h1>
<p>贪心策略很容易理解，但一个问题是否适用于贪心算法的证明是最难的。</p>
<h2 id="归纳法">归纳法</h2>
<p>数学归纳法的一般步骤为： 1. 证明<span class="math inline">\(n=1\)</span>时命题成立 2. 假设<span class="math inline">\(n=k\)</span>时命题成立 3. 证明<span class="math inline">\(n=k+1\)</span>时命题成立</p>
<h2 id="交换论证法">交换论证法</h2>
<ul>
<li>分析一般最优解与贪心法的解的区别，然后定义一种转换规则，使得从任意一个最优解出发，经过不断对解的某些成分的排列次序进行交换或者用其他元素替换，将这个解最终能够转变成贪心法的解。</li>
<li>证明在上述转换中解得优化函数值不会变坏。</li>
<li>证明上述转换在有限步结束。</li>
</ul>
]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker使用基础</title>
    <url>/2021/08/12/Docker%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="docker的安装">Docker的安装</h1>
<p>官方脚本自动安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -fsSL https:&#x2F;&#x2F;get.docker.com | bash -s docker --mirror aliyun</span><br></pre></td></tr></table></figure>
<p>国内daocloud自动安装:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh</span><br></pre></td></tr></table></figure>
<p>若存在旧版本docker,先将旧版本卸载：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br></pre></td></tr></table></figure>
<h2 id="docker镜像仓库">Docker镜像仓库</h2>
<p>默认的DockerHub仓库在国外，从国内拉取速度较慢，国内许多云服务商提供了镜像加速服务。以阿里云为例，前往<a href="https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors">容器镜像服务网站</a>:</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210812Docker1.png" alt="image-20210812133532470" /><figcaption>image-20210812133532470</figcaption>
</figure>
<p>获取自己的加速地址并按照操作文档中的步骤进行配置。</p>
<h1 id="docker容器使用">Docker容器使用</h1>
<p>直接查看所有Docker命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker</span><br></pre></td></tr></table></figure>
<h2 id="镜像操作">镜像操作</h2>
<ul>
<li>拉取镜像：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker pull [images name]</span><br></pre></td></tr></table></figure>
<p>当我们在本地主机上使用（run）一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用 docker pull 命令来下载它</p>
<ul>
<li>列出docker中的所有镜像：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br></pre></td></tr></table></figure>
<p>同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本我们使用 REPOSITORY:TAG 来定义不同的镜像。如果你不指定一个镜像的版本标签docker 将默认使用 REPOSITORY:latest​ 镜像。</p>
<ul>
<li>搜索镜像：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$  docker search [images name]</span><br></pre></td></tr></table></figure>
<p>或者前往<a href="https://hub.docker.com/">官方网站</a>查找镜像.</p>
<ul>
<li>删除镜像：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker rmi [images name]</span><br></pre></td></tr></table></figure>
<ul>
<li>创建镜像：</li>
</ul>
<p>创建镜像常用的两种方式为：</p>
<ol type="1">
<li>从已经创建的容器中更新镜像，并且提交这个镜像</li>
<li>使用 Dockerfile 指令来创建一个新的镜像</li>
</ol>
<blockquote>
<p>更新镜像：</p>
<p>更新镜像之前先使用镜像创建一个容器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -it ubuntu:15.10 &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p>在运行的容器内使用 <code>apt-get update</code>命令进行更新。</p>
<p>操作完成后使用<code>exit</code>退出容器</p>
<p>使用<code>docker commit</code>来提交容器副本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker commit -m&#x3D;&quot;test&quot; -a&#x3D;&quot;wxb&quot; 4758145e9199 wxb&#x2F;ubuntu:test</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>-m:</strong> 提交的描述信息</li>
<li><strong>-a:</strong> 指定镜像作者</li>
<li><strong>4758145e9199：</strong>容器 ID</li>
<li><strong>wxb/ubuntu:test:</strong> 指定要创建的目标镜像名</li>
</ul>
</blockquote>
<blockquote>
<p>构建镜像：</p>
<p>使用<code>docker build</code>从零开始构建一个新的镜像。首先需要创建Dockerfile文件（文件中包含一组构建镜像的指令）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;$ cat &gt; Dockerfile (vim Dockerfile)</span><br><span class="line">&gt;FROM    ubuntu:15.10</span><br><span class="line">&gt;MAINTAINER      Fisher &quot;test@test.com&quot;</span><br><span class="line"></span><br><span class="line">&gt;RUN     &#x2F;bin&#x2F;echo &#39;root:123456&#39; |chpasswd</span><br><span class="line">&gt;RUN     useradd wxb</span><br><span class="line">&gt;RUN     &#x2F;bin&#x2F;echo &#39;wxb:123456&#39; |chpasswd</span><br><span class="line">&gt;RUN     &#x2F;bin&#x2F;echo -e &quot;LANG&#x3D;\&quot;en_US.UTF-8\&quot;&quot; &gt;&#x2F;etc&#x2F;default&#x2F;local</span><br><span class="line">&gt;EXPOSE  22</span><br><span class="line">&gt;EXPOSE  80</span><br><span class="line">&gt;CMD     &#x2F;usr&#x2F;sbin&#x2F;sshd -D</span><br></pre></td></tr></table></figure>
<p>从Dockerfile文件构建镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;docker build -t ubuntu:test .&#x2F;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>-t</strong> ：指定要创建的目标镜像名</li>
<li><strong>./</strong> ：Dockerfile 文件所在目录，可以指定Dockerfile 的绝对路径</li>
</ul>
</blockquote>
<ul>
<li>添加新的镜像标签</li>
</ul>
<p>可以使用 docker tag 命令，为镜像添加一个新的标签：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker tag [images id] ubuntu:newtag</span><br></pre></td></tr></table></figure>
<p>此时镜像id不变，只是多了一个指向它的标签。</p>
<hr />
<h2 id="容器操作">容器操作</h2>
<ul>
<li>创建并启动容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -it ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -i -t ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<p><strong>-i</strong>: 交互式操作;<strong>-t</strong>: 终端。另外<strong>-d</strong>表示容器在后台运行，使用<strong>-d</strong>参数后不会自动进入容器，需要手动进入，可以使用<strong>attach</strong>和<strong>exec</strong>命令，一般使用exec<strong>命令</strong>，退出容器后不会自动停止容器(<strong>--name</strong>参数可以命名容器)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker exec -it [id|name] &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<ul>
<li>启动、停止、重启容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker start|stop|restart [container id|name]</span><br><span class="line">stop容器时跟上-t参数代表延时多少时间停止(默认为10)</span><br></pre></td></tr></table></figure>
<ul>
<li>导出和导入容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker export [container id] &gt; name.tar</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat path&#x2F;name.tar | docker import - test&#x2F;ubuntu:v1</span><br></pre></td></tr></table></figure>
<p>从url地址导入容器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker import http:&#x2F;&#x2F;example.com&#x2F;exampleimage.tgz example&#x2F;imagerepo</span><br></pre></td></tr></table></figure>
<ul>
<li>删除容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker rm -f [container id]</span><br></pre></td></tr></table></figure>
<p>一次删除不在运行状态的所有容器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker container prune</span><br></pre></td></tr></table></figure>
<ul>
<li>批量删除容器：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker ps -a # 查看运行中容器状态</span><br><span class="line">$ docker ps -a # 查看所用容器状态</span><br><span class="line">$ docker ps -aq # 列出所有容器的id</span><br><span class="line">$ docker stop $(docker ps -aq) # 批量停止所用的容器</span><br><span class="line">$ docker rm $(docker ps -aq) # 批量停止所用的容器</span><br></pre></td></tr></table></figure>
<h2 id="容器端口映射">容器端口映射</h2>
<p>自动随机配置主机端口至容器开放端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -d -P [images Name] [Commend]</span><br></pre></td></tr></table></figure>
<p>自定义主机端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -d -p host_port [images Name] [Commend]</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -d -p host_port:container_port [images Name] [Commend]</span><br></pre></td></tr></table></figure>
<p>指定主机IP地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -d -p 127.0.0.1:host_port:container_port [images Name] [Commend]</span><br></pre></td></tr></table></figure>
<p>默认为tcp端口，可指定udp端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -d -p 127.0.0.1:host_port:container_port&#x2F;udp [images Name] [Commend]</span><br></pre></td></tr></table></figure>
<p>查看端口绑定情况：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker port [container id|name] container_port</span><br></pre></td></tr></table></figure>
<h2 id="docker容器互联">Docker容器互联</h2>
<p>docker除了通过端口映射连接外，还可以通过连接系统将多个容器连接到一起，共享连接信息。</p>
<ul>
<li>新建Docker网络：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker network create -d bridge test-net</span><br></pre></td></tr></table></figure>
<p><strong>-d</strong>：参数指定 Docker 网络类型，有 bridge、overlay。</p>
<ul>
<li>查看docker网络</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker network ls</span><br></pre></td></tr></table></figure>
<ul>
<li>容器连接到网络(<strong>--network参数</strong>)：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -itd --name test1 --network test-net ubuntu &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>
<ul>
<li>配置容器DNS:</li>
</ul>
<p>在宿主机的 /etc/docker/daemon.json 文件中增加以下内容来设置全部容器的 DNS（重启docker后生效）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;dns&quot; : [</span><br><span class="line">    &quot;114.114.114.114&quot;,</span><br><span class="line">    &quot;8.8.8.8&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看容器的DNS信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -it --rm  ubuntu  cat etc&#x2F;resolv.conf</span><br></pre></td></tr></table></figure>
<p>手动配置容器DNS信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker run -it --rm -h host_ubuntu  --dns&#x3D;114.114.114.114 --dns-search&#x3D;test.com ubuntu</span><br></pre></td></tr></table></figure>
<p><strong>--rm</strong>：容器退出时自动清理容器内部的文件系统。</p>
<p><strong>-h HOSTNAME 或者 --hostname=HOSTNAME</strong>： 设定容器的主机名，它会被写到容器内的 /etc/hostname 和 /etc/hosts。</p>
<p><strong>--dns=IP_ADDRESS</strong>： 添加 DNS 服务器到容器的 /etc/resolv.conf 中，让容器用这个服务器来解析所有不在 /etc/hosts 中的主机名。</p>
<p><strong>--dns-search=DOMAIN</strong>： 设定容器的搜索域，当设定搜索域为 .example.com 时，在搜索一个名为 host 的主机时，DNS 不仅搜索 host，还会搜索 host.example.com</p>
<hr />
<h1 id="docker-仓库管理">Docker 仓库管理</h1>
<p>目前 Docker 官方维护了一个公共仓库 <a href="https://hub.docker.com/">Docker Hub</a>。大部分需求都可以通过在 Docker Hub 中直接下载镜像来实现，其他运营商也提供相应的Docker仓库。</p>
<ul>
<li>登录，登出自己的docker账号，登录后可以拉取自己账号下的镜像：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker login</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker logout</span><br></pre></td></tr></table></figure>
<ul>
<li>登录后将自己的镜像推送到Docker Hub:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker tag ubuntu:15.10 username&#x2F;ubuntu:15.10</span><br><span class="line">$ docker push username&#x2F;ubuntu:15.10</span><br></pre></td></tr></table></figure>
<hr />
<h1 id="其他docker部件">其他Docker部件</h1>
<h2 id="docker-dockerfile">Docker Dockerfile</h2>
<p>Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。</p>
<ul>
<li><strong>FROM 和 RUN 指令</strong></li>
</ul>
<p><strong>FROM</strong>：定制的镜像都是基于 FROM 的镜像.</p>
<p><strong>RUN</strong>：用于执行后面跟着的命令行命令。有shell格式和exec格式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RUN &lt;命令行命令&gt;</span><br><span class="line"># &lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;]</span><br><span class="line"># 例如：</span><br><span class="line"># RUN [&quot;.&#x2F;test.php&quot;, &quot;dev&quot;, &quot;offline&quot;] 等价于 RUN .&#x2F;test.php dev offline</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM centos</span><br><span class="line">RUN yum install wget</span><br><span class="line">RUN wget -O redis.tar.gz &quot;http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-5.0.3.tar.gz&quot;</span><br><span class="line">RUN tar -xvf redis.tar.gz</span><br><span class="line">以上执行会创建 3 层镜像。可简化为以下格式：</span><br><span class="line">FROM centos</span><br><span class="line">RUN yum install wget \</span><br><span class="line">    &amp;&amp; wget -O redis.tar.gz &quot;http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-5.0.3.tar.gz&quot; \</span><br><span class="line">    &amp;&amp; tar -xvf redis.tar.gz</span><br></pre></td></tr></table></figure>
<ul>
<li>构建镜像</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker build -t ubuntu:test .</span><br></pre></td></tr></table></figure>
<p>此命令中的最后一个指令<code>.</code>为上下文路径，是指 docker 在构建镜像，有时候想要使用到本机的文件（比如复制），docker build 命令得知这个路径后，会将路径下的所有内容打包。</p>
<p>由于 docker 的运行模式是 C/S。我们本机是 C，docker 引擎是 S。实际的构建过程是在 docker 引擎下完成的，所以这个时候无法用到我们本机的文件。这就需要把我们本机的指定目录下的文件一起打包提供给 docker 引擎使用。因此Dockerfile文件路径下不要放过多的无关文件，否则会一起打包给容器。</p>
<p>如果未说明最后一个参数，那么默认上下文路径就是 Dockerfile 所在的位置。</p>
<ul>
<li>其他指令：</li>
</ul>
<blockquote>
<p>COPY:复制指令，从上下文目录中复制文件或者目录到容器里指定路径。</p>
<p>ADD:ADD 指令和 COPY 的使用格类似(同样需求下，官方推荐使用 COPY).</p>
<p>CMD:类似于 RUN 指令，用于运行程序，但二者运行的时间点不同,CMD 在docker run 时运行,RUN 是在 docker build时运行。</p>
<p>ENTRYPOINT：类似于 CMD 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序。</p>
<p>ENV:设置环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。</p>
<p>ARG:构建参数，与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量。</p>
<p>VOLUME:定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。</p>
<p>EXPOSE:仅仅只是声明端口。作用：帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。 在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。</p>
<p>WORKDIR:指定工作目录。用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在。</p>
<p>USER:用于指定执行后续命令的用户和用户组，这边只是切换后续命令执行的用户（用户和用户组必须提前已经存在）。</p>
<p>HEALTHCHECK:用于指定某个程序或者指令来监控 docker 容器服务的运行状态。</p>
<p>ONBUILD:用于延迟构建命令的执行。简单的说，就是 Dockerfile 里用 ONBUILD 指定的命令，在本次构建镜像的过程中不会执行.</p>
<p>LABEL:LABEL 指令用来给镜像添加一些元数据（metadata），以键值对的形式.</p>
</blockquote>
<h2 id="docker-compose">Docker Compose</h2>
<p>Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。</p>
<ul>
<li>yml 配置指令参考</li>
</ul>
<blockquote>
<p>version:指定本 yml 依从的 compose 哪个版本制定的。</p>
<p>build:指定为构建镜像上下文路径：</p>
<p>cap_add，cap_drop:添加或删除容器拥有的宿主机的内核功能。</p>
<p>cgroup_parent:为容器指定父 cgroup 组，意味着将继承该组的资源限制。</p>
<p>command:覆盖容器启动的默认命令。</p>
<p>container_name:指定自定义容器名称，而不是生成的默认名称。</p>
<p>depends_on:设置依赖关系。</p>
<p>deploy:指定与服务的部署和运行有关的配置。只在 swarm 模式下才会有用。</p>
<p>devices:指定设备映射列表。</p>
<p>dns:自定义 DNS 服务器，可以是单个值或列表的多个值。</p>
<p>dns_search:自定义 DNS 搜索域。可以是单个值或列表。</p>
<p>entrypoint:覆盖容器默认的 entrypoint。</p>
<p>env_file:从文件添加环境变量。可以是单个值或列表的多个值。</p>
<p>environment:添加环境变量。您可以使用数组或字典、任何布尔值，布尔值需要用引号引起来，以确保 YML 解析器不会将其转换为 True 或 False。</p>
<p>expose:暴露端口，但不映射到宿主机，只被连接的服务访问。</p>
<p>extra_hosts:添加主机名映射。类似 docker client --add-host。</p>
<p>healthcheck:用于检测 docker 服务是否健康运行。</p>
<p>image:指定容器运行的镜像。以下格式都可以.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">image: redis</span><br><span class="line">image: ubuntu:14.04</span><br><span class="line">image: tutum&#x2F;influxdb</span><br><span class="line">image: example-registry.com:4000&#x2F;postgresql</span><br><span class="line">image: a4bc65fd # 镜像id</span><br></pre></td></tr></table></figure>
<p>logging:服务的日志记录配置。</p>
<p>network_mode:设置网络模式。</p>
<p>restart:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">restart: &quot;no&quot; 				是默认的重启策略，在任何情况下都不会重启容器。</span><br><span class="line">restart: always				容器总是重新启动。</span><br><span class="line">restart: on-failure			在容器非正常退出时（退出状态非0），才会重启容器。</span><br><span class="line">restart: unless-stopped		在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器</span><br></pre></td></tr></table></figure>
<p>secrets:存储敏感数据，例如密码.</p>
<p>security_opt:修改容器默认的 schema 标签。</p>
<p>stop_grace_period:指定在容器无法处理 SIGTERM (或者任何 stop_signal 的信号)，等待多久后发送 SIGKILL 信号关闭容器。</p>
<p>stop_signal:设置停止容器的替代信号。默认情况下使用 SIGTERM 。</p>
<p>sysctls:设置容器中的内核参数，可以使用数组或字典格式。</p>
<p>tmpfs:在容器内安装一个临时文件系统。可以是单个值或列表的多个值。</p>
<p>ulimits:覆盖容器默认的 ulimit。</p>
<p>volumes:将主机的数据卷或着文件挂载到容器里。</p>
</blockquote>
<h2 id="docker-machine">Docker Machine</h2>
<p>Docker Machine 是一种可以让您在虚拟主机上安装 Docker 的工具，并可以使用 docker-machine 命令来管理主机。</p>
<p>Docker Machine 管理的虚拟主机可以是本地主机上的，也可以是云供应商，如阿里云，腾讯云，AWS，或 DigitalOcean。</p>
<p>使用 docker-machine 命令，您可以启动，检查，停止和重新启动托管主机，也可以升级 Docker 客户端和守护程序，以及配置 Docker 客户端与您的主机进行通信。</p>
<ul>
<li>安装Docker Machine:</li>
</ul>
<p>安装 Docker Machine 之前你需要先安装 Docker。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Linux安装</span><br><span class="line">$ base&#x3D;https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;machine&#x2F;releases&#x2F;download&#x2F;v0.16.0 &amp;&amp;</span><br><span class="line">  curl -L $base&#x2F;docker-machine-$(uname -s)-$(uname -m) &gt;&#x2F;tmp&#x2F;docker-machine &amp;&amp;</span><br><span class="line">  sudo mv &#x2F;tmp&#x2F;docker-machine &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-machine &amp;&amp;</span><br><span class="line">  chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-machine</span><br></pre></td></tr></table></figure>
<ul>
<li>基本命令：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine ls 列出可用的机器</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine create --driver virtualbox test 创建一台名为 test 的机器。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine ip test 查看机器的 ip</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine stop test 停止机器</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine start test 启动机器</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine ssh test 进入机器</span><br></pre></td></tr></table></figure>
<ul>
<li>命令参数说明：</li>
</ul>
<blockquote>
<ul>
<li><strong>config</strong>：查看当前激活状态 Docker 主机的连接信息。</li>
<li><strong>create</strong>：创建 Docker 主机</li>
<li><strong>env</strong>：显示连接到某个主机需要的环境变量</li>
<li><strong>inspect</strong>： 以 json 格式输出指定Docker的详细信息</li>
<li><strong>ip</strong>： 获取指定 Docker 主机的地址</li>
<li><strong>kill</strong>： 直接杀死指定的 Docker 主机</li>
<li><strong>ls</strong>： 列出所有的管理主机</li>
<li><strong>provision</strong>： 重新配置指定主机</li>
<li><strong>regenerate-certs</strong>： 为某个主机重新生成 TLS 信息</li>
<li><strong>restart</strong>： 重启指定的主机</li>
<li><strong>rm</strong>： 删除某台 Docker 主机，对应的虚拟机也会被删除</li>
<li><strong>ssh</strong>： 通过 SSH 连接到主机上，执行命令</li>
<li><strong>scp</strong>： 在 Docker 主机之间以及 Docker 主机和本地主机之间通过 scp 远程复制数据</li>
<li><strong>mount</strong>： 使用 SSHFS 从计算机装载或卸载目录</li>
<li><strong>start</strong>： 启动一个指定的 Docker 主机，如果对象是个虚拟机，该虚拟机将被启动</li>
<li><strong>status</strong>： 获取指定 Docker 主机的状态(包括：Running、Paused、Saved、Stopped、Stopping、Starting、Error)等</li>
<li><strong>stop</strong>： 停止一个指定的 Docker 主机</li>
<li><strong>upgrade</strong>： 将一个指定主机的 Docker 版本更新为最新</li>
<li><strong>url</strong>： 获取指定 Docker 主机的监听 URL</li>
<li><strong>version</strong>： 显示 Docker Machine 的版本或者主机 Docker 版本</li>
<li><strong>help</strong>： 显示帮助信息</li>
</ul>
</blockquote>
<h2 id="swarm-集群管理">Swarm 集群管理</h2>
<p>Docker Swarm 是 Docker 的集群管理工具。它将 Docker 主机池转变为单个虚拟 Docker 主机。 Docker Swarm 提供了标准的 Docker API，所有任何已经与 Docker 守护程序通信的工具都可以使用 Swarm 轻松地扩展到多个主机。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine create -d virtualbox swarm-manager # 创建 swarm 集群管理节点（manager）</span><br></pre></td></tr></table></figure>
<p>初始化 swarm 集群，进行初始化的这台机器，就是集群的管理节点。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-machine ssh swarm-manager</span><br><span class="line">$ docker swarm init --advertise-addr 192.168.99.107 #这里的 IP 为创建机器时分配的 ip。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker info 查看集群信息</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker@swarm-manager:~$ docker service create --replicas 1 --name helloworld alpine ping docker.com 部署服务到集群中</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker@swarm-manager:~$ docker service ps helloworld 查看服务部署情况</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker@swarm-manager:~$ docker service scale helloworld&#x3D;2 扩展集群服务</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker@swarm-manager:~$ docker service rm helloworld 删除服务</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker@swarm-manager:~$ docker service create --replicas 1 --name redis --update-delay 10s redis:3.0.6</span><br><span class="line">docker@swarm-manager:~$ docker service update --image redis:3.0.7 redis 滚动升级服务</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker@swarm-manager:~$ docker node ls </span><br><span class="line">docker@swarm-manager:~$  docker node update --availability active swarm-worker1 停止某个节点接收新的任务</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>数学基础</title>
    <url>/2021/09/14/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="第一章-数学基础">第一章 数学基础</h1>
<p>深度学习通常又需要哪些数学基础？深度学习里的数学到底难在哪里？通常初学者都会有这些问题，在网络推荐及书本推荐里，经常看到会列出一系列数学科目，比如微积分、线性代数、概率论、复变函数、数值计算、优化理论、信息论等等。这些数学知识有相关性，但实际上按照这样的知识范围来学习，学习成本会很久，而且会很枯燥，本章我们通过选举一些数学基础里容易混淆的一些概念做以介绍，帮助大家更好的理清这些易混淆概念之间的关系。</p>
<h2 id="向量和矩阵">1.1 向量和矩阵</h2>
<h3 id="标量向量矩阵张量之间的联系">1.1.1 标量、向量、矩阵、张量之间的联系</h3>
<p><strong>标量（scalar）</strong><br />
一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。</p>
<p><strong>向量（vector）</strong><br />
​一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量<span class="math inline">\(X\)</span>的第一个元素是<span class="math inline">\(X_1\)</span>，第二个元素是<span class="math inline">\(X_2\)</span>，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。</p>
<p><strong>矩阵（matrix）</strong><br />
​矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如<span class="math inline">\(A\)</span>。</p>
<p><strong>张量（tensor）</strong><br />
​在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 <span class="math inline">\(A\)</span> 来表示张量“A”。张量<span class="math inline">\(A\)</span>中坐标为<span class="math inline">\((i,j,k)\)</span>的元素记作<span class="math inline">\(A_{(i,j,k)}\)</span>。</p>
<p><strong>四者之间关系</strong></p>
<blockquote>
<p>标量是0阶张量，向量是一阶张量。举例：<br />
​标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。<br />
​向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。<br />
​张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上/下和左/右偏转了多少。</p>
</blockquote>
<h3 id="张量与矩阵的区别">1.1.2 张量与矩阵的区别</h3>
<ul>
<li>从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么<span class="math inline">\(n\)</span>阶张量就是所谓的<span class="math inline">\(n\)</span>维的“表格”。 张量的严格定义是利用线性映射来描述。</li>
<li>从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。</li>
<li>张量可以用3×3矩阵形式来表达。</li>
<li>表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。</li>
</ul>
<h3 id="矩阵和向量相乘结果">1.1.3 矩阵和向量相乘结果</h3>
<p>若使用爱因斯坦求和约定（Einstein summation convention），矩阵<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>相乘得到矩阵<span class="math inline">\(C\)</span>可以用下式表示： $ a_{ik}*b_{kj}=c_{ij}  $ 其中，<span class="math inline">\(a_{ik}\)</span>, <span class="math inline">\(b_{kj}\)</span>, <span class="math inline">\(c_{ij}\)</span>分别表示矩阵<span class="math inline">\(A, B, C\)</span>的元素，<span class="math inline">\(k\)</span>出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。 而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵<span class="math inline">\(B\)</span>是一个<span class="math inline">\(n \times 1\)</span>的矩阵。</p>
<h3 id="向量和矩阵的范数归纳">1.1.4 向量和矩阵的范数归纳</h3>
<p><strong>向量的范数(norm)</strong><br />
​ 定义一个向量为：<span class="math inline">\(\vec{a}=[-5, 6, 8, -10]\)</span>。任意一组向量设为<span class="math inline">\(\vec{x}=(x_1,x_2,...,x_N)\)</span>。其不同范数求解如下：</p>
<ul>
<li>向量的1范数：向量的各个元素的绝对值之和，上述向量<span class="math inline">\(\vec{a}\)</span>的1范数结果就是：29。</li>
</ul>
<p><span class="math inline">\(\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert\)</span></p>
<ul>
<li>向量的2范数：向量的每个元素的平方和再开平方根，上述<span class="math inline">\(\vec{a}\)</span>的2范数结果就是：15。</li>
</ul>
<p><span class="math inline">\(\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}\)</span></p>
<ul>
<li>向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量<span class="math inline">\(\vec{a}\)</span>的负无穷范数结果就是：5。</li>
</ul>
<p><span class="math inline">\(\Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|}\)</span></p>
<ul>
<li>向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量<span class="math inline">\(\vec{a}\)</span>的正无穷范数结果就是：10。</li>
</ul>
<p><span class="math inline">\(\Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|}\)</span></p>
<ul>
<li>向量的p范数：</li>
</ul>
<p><span class="math inline">\(L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}\)</span></p>
<p><strong>矩阵的范数</strong></p>
<p>定义一个矩阵<span class="math inline">\(A=[-1, 2, -3; 4, -6, 6]\)</span>。 任意矩阵定义为：<span class="math inline">\(A_{m\times n}\)</span>，其元素为 <span class="math inline">\(a_{ij}\)</span>。</p>
<p>矩阵的范数定义为</p>
<p><span class="math inline">\(\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}\)</span></p>
<p>当向量取不同范数时, 相应得到了不同的矩阵范数。</p>
<ul>
<li><p><strong>矩阵的1范数（列范数）</strong>：矩阵的每一列上的元</p>
<p>素绝对值先求和，再从中取个最大的,（列和最大），上述矩阵<span class="math inline">\(A\)</span>的1范数先得到<span class="math inline">\([5,8,9]\)</span>，再取最大的最终结果就是：9。</p></li>
</ul>
<p><span class="math inline">\(\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|\)</span></p>
<ul>
<li><strong>矩阵的2范数</strong>：矩阵<span class="math inline">\(A^TA\)</span>的最大特征值开平方根，上述矩阵<span class="math inline">\(A\)</span>的2范数得到的最终结果是：10.0623。</li>
</ul>
<p><span class="math inline">\(\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}\)</span></p>
<p>其中， <span class="math inline">\(\lambda_{max}(A^T A)\)</span> 为 <span class="math inline">\(A^T A\)</span> 的特征值绝对值的最大值。</p>
<ul>
<li><strong>矩阵的无穷范数（行范数）</strong>：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵<span class="math inline">\(A\)</span>的行范数先得到<span class="math inline">\([6；16]\)</span>，再取最大的最终结果就是：16。</li>
</ul>
<p><span class="math inline">\(\Vert A\Vert_{\infty}=\max_{1\le i \le m}\sum_{j=1}^n |{a_{ij}}|\)</span></p>
<ul>
<li><p><strong>矩阵的核范数</strong>：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。</p></li>
<li><strong>矩阵的L0范数</strong>：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵<span class="math inline">\(A\)</span>最终结果就是：6。</li>
<li><strong>矩阵的L1范数</strong>：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵<span class="math inline">\(A\)</span>最终结果就是：22。<br />
</li>
<li><p><strong>矩阵的F范数</strong>：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，它的优点在于它是一个凸函数，可以求导求解，易于计算，上述矩阵A最终结果就是：10.0995。</p></li>
</ul>
<p><span class="math inline">\(\Vert A\Vert_F=\sqrt{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^2)}\)</span></p>
<ul>
<li><strong>矩阵的L21范数</strong>：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵<span class="math inline">\(A\)</span>最终结果就是：17.1559。</li>
<li><strong>矩阵的 p范数</strong></li>
</ul>
<p><span class="math inline">\(\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}\)</span></p>
<h3 id="如何判断一个矩阵为正定">1.1.5 如何判断一个矩阵为正定</h3>
<p>判定一个矩阵是否为正定，通常有以下几个方面：</p>
<ul>
<li>顺序主子式全大于0；<br />
</li>
<li>存在可逆矩阵<span class="math inline">\(C\)</span>使<span class="math inline">\(C^TC\)</span>等于该矩阵；</li>
<li>正惯性指数等于<span class="math inline">\(n\)</span>；</li>
<li>合同于单位矩阵<span class="math inline">\(E\)</span>（即：规范形为<span class="math inline">\(E\)</span>）</li>
<li>标准形中主对角元素全为正；</li>
<li>特征值全为正；</li>
<li>是某基的度量矩阵。</li>
</ul>
<h2 id="导数和偏导数">1.2 导数和偏导数</h2>
<h3 id="导数偏导计算">1.2.1 导数偏导计算</h3>
<p><strong>导数定义</strong>:</p>
<p>导数(derivative)代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。几何意义是这个点的切线。物理意义是该时刻的（瞬时）变化率。 ​</p>
<p><em>注意</em>：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。在物理学中有平均速度和瞬时速度之说。平均速度有</p>
<p><span class="math inline">\(v=\frac{s}{t}\)</span></p>
<p>其中<span class="math inline">\(v\)</span>表示平均速度，<span class="math inline">\(s\)</span>表示路程，<span class="math inline">\(t\)</span>表示时间。这个公式可以改写为</p>
<p><span class="math inline">\(\bar{v}=\frac{\Delta s}{\Delta t}=\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}\)</span></p>
<p>其中<span class="math inline">\(\Delta s\)</span>表示两点之间的距离，而<span class="math inline">\(\Delta t\)</span>表示走过这段距离需要花费的时间。当<span class="math inline">\(\Delta t\)</span>趋向于0（<span class="math inline">\(\Delta t \to 0\)</span>）时，也就是时间变得很短时，平均速度也就变成了在<span class="math inline">\(t_0\)</span>时刻的瞬时速度，表示成如下形式：</p>
<p><span class="math inline">\(v(t_0)=\lim_{\Delta t \to 0}{\bar{v}}=\lim_{\Delta t \to 0}{\frac{\Delta s}{\Delta t}}=\lim_{\Delta t \to 0}{\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}}\)</span></p>
<p>实际上，上式表示的是路程<span class="math inline">\(s\)</span>关于时间<span class="math inline">\(t\)</span>的函数在<span class="math inline">\(t=t_0\)</span>处的导数。一般的，这样定义导数：如果平均变化率的极限存在，即有</p>
<p><span class="math inline">\(\lim_{\Delta x \to 0}{\frac{\Delta y}{\Delta x}}=\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}}\)</span></p>
<p>则称此极限为函数 <span class="math inline">\(y=f(x)\)</span> 在点 <span class="math inline">\(x_0\)</span> 处的导数。记作 <span class="math inline">\(f&#39;(x_0)\)</span> 或 <span class="math inline">\(y&#39;\vert_{x=x_0}\)</span> 或 <span class="math inline">\(\frac{dy}{dx}\vert_{x=x_0}\)</span> 或 <span class="math inline">\(\frac{df(x)}{dx}\vert_{x=x_0}\)</span>。</p>
<p>通俗地说，导数就是曲线在某一点切线的斜率。</p>
<p><strong>偏导数</strong>:</p>
<p>既然谈到偏导数(partial derivative)，那就至少涉及到两个自变量。以两个自变量为例，<span class="math inline">\(z=f(x,y)\)</span>，从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。</p>
<p><em>注意</em>：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。</p>
<p>设函数<span class="math inline">\(z=f(x,y)\)</span>在点<span class="math inline">\((x_0,y_0)\)</span>的领域内有定义，当<span class="math inline">\(y=y_0\)</span>时，<span class="math inline">\(z\)</span>可以看作关于<span class="math inline">\(x\)</span>的一元函数<span class="math inline">\(f(x,y_0)\)</span>，若该一元函数在<span class="math inline">\(x=x_0\)</span>处可导，即有</p>
<p><span class="math inline">\(\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}}=A\)</span></p>
<p>函数的极限<span class="math inline">\(A\)</span>存在。那么称<span class="math inline">\(A\)</span>为函数<span class="math inline">\(z=f(x,y)\)</span>在点<span class="math inline">\((x_0,y_0)\)</span>处关于自变量<span class="math inline">\(x\)</span>的偏导数，记作<span class="math inline">\(f_x(x_0,y_0)\)</span>或<span class="math inline">\(\frac{\partial z}{\partial x}\vert_{y=y_0}^{x=x_0}\)</span>或<span class="math inline">\(\frac{\partial f}{\partial x}\vert_{y=y_0}^{x=x_0}\)</span>或<span class="math inline">\(z_x\vert_{y=y_0}^{x=x_0}\)</span>。</p>
<p>偏导数在求解时可以将另外一个变量看做常数，利用普通的求导方式求解，比如<span class="math inline">\(z=3x^2+xy\)</span>关于<span class="math inline">\(x\)</span>的偏导数就为<span class="math inline">\(z_x=6x+y\)</span>，这个时候<span class="math inline">\(y\)</span>相当于<span class="math inline">\(x\)</span>的系数。</p>
<p>某点<span class="math inline">\((x_0,y_0)\)</span>处的偏导数的几何意义为曲面<span class="math inline">\(z=f(x,y)\)</span>与面<span class="math inline">\(x=x_0\)</span>或面<span class="math inline">\(y=y_0\)</span>交线在<span class="math inline">\(y=y_0\)</span>或<span class="math inline">\(x=x_0\)</span>处切线的斜率。</p>
<h3 id="导数和偏导数有什么区别">1.2.2 导数和偏导数有什么区别？</h3>
<p>导数和偏导没有本质区别，如果极限存在，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。</p>
<blockquote>
<ul>
<li>一元函数，一个<span class="math inline">\(y\)</span>对应一个<span class="math inline">\(x\)</span>，导数只有一个。<br />
</li>
<li>二元函数，一个<span class="math inline">\(z\)</span>对应一个<span class="math inline">\(x\)</span>和一个<span class="math inline">\(y\)</span>，有两个导数：一个是<span class="math inline">\(z\)</span>对<span class="math inline">\(x\)</span>的导数，一个是<span class="math inline">\(z\)</span>对<span class="math inline">\(y\)</span>的导数，称之为偏导。<br />
</li>
<li>求偏导时要注意，对一个变量求导，则视另一个变量为常数，只对改变量求导，从而将偏导的求解转化成了一元函数的求导。</li>
</ul>
</blockquote>
<h2 id="特征值和特征向量">1.3 特征值和特征向量</h2>
<h3 id="特征值分解与特征向量">1.3.1 特征值分解与特征向量</h3>
<ul>
<li><p>特征值分解可以得到特征值(eigenvalues)与特征向量(eigenvectors)；</p></li>
<li><p>特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。</p>
<p>如果说一个向量<span class="math inline">\(\vec{v}\)</span>是方阵<span class="math inline">\(A\)</span>的特征向量，将一定可以表示成下面的形式：</p></li>
</ul>
<p><span class="math inline">\(A\nu = \lambda \nu\)</span></p>
<p><span class="math inline">\(\lambda\)</span>为特征向量<span class="math inline">\(\vec{v}\)</span>对应的特征值。特征值分解是将一个矩阵分解为如下形式：</p>
<p><span class="math inline">\(A=Q\sum Q^{-1}\)</span></p>
<p>其中，<span class="math inline">\(Q\)</span>是这个矩阵<span class="math inline">\(A\)</span>的特征向量组成的矩阵，<span class="math inline">\(\sum\)</span>是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵<span class="math inline">\(A\)</span>的信息可以由其特征值和特征向量表示。</p>
<h3 id="奇异值与特征值有什么关系">1.3.2 奇异值与特征值有什么关系</h3>
<p>那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵<span class="math inline">\(A\)</span>的转置乘以<span class="math inline">\(A\)</span>，并对<span class="math inline">\(A^TA\)</span>求特征值，则有下面的形式：</p>
<p><span class="math inline">\((A^TA)V = \lambda V\)</span></p>
<p>这里<span class="math inline">\(V\)</span>就是上面的右奇异向量，另外还有：</p>
<p><span class="math inline">\(\sigma_i = \sqrt{\lambda_i}, u_i=\frac{1}{\sigma_i}AV\)</span></p>
<p>这里的<span class="math inline">\(\sigma\)</span>就是奇异值，<span class="math inline">\(u\)</span>就是上面说的左奇异向量。【证明那个哥们也没给】 ​奇异值<span class="math inline">\(\sigma\)</span>跟特征值类似，在矩阵<span class="math inline">\(\sum\)</span>中也是从大到小排列，而且<span class="math inline">\(\sigma\)</span>的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前<span class="math inline">\(r\)</span>（<span class="math inline">\(r\)</span>远小于<span class="math inline">\(m、n\)</span>）个的奇异值来近似描述矩阵，即部分奇异值分解： <span class="math inline">\(A_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^T\)</span></p>
<p>右边的三个矩阵相乘的结果将会是一个接近于<span class="math inline">\(A\)</span>的矩阵，在这儿，<span class="math inline">\(r\)</span>越接近于<span class="math inline">\(n\)</span>，则相乘的结果越接近于<span class="math inline">\(A\)</span>。</p>
<h2 id="概率分布与随机变量">1.4 概率分布与随机变量</h2>
<h3 id="机器学习为什么要使用概率">1.4.1 机器学习为什么要使用概率</h3>
<p>事件的概率是衡量该事件发生的可能性的量度。虽然在一次随机试验中某个事件的发生是带有偶然性的，但那些可在相同条件下大量重复的随机试验却往往呈现出明显的数量规律。<br />
​机器学习除了处理不确定量，也需处理随机量。不确定性和随机性可能来自多个方面，使用概率论来量化不确定性。<br />
​概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。</p>
<blockquote>
<p>​ 例如在机器学习（Andrew Ng）的课中，会有一个朴素贝叶斯假设就是条件独立的一个例子。该学习算法对内容做出假设，用来分辨电子邮件是否为垃圾邮件。假设无论邮件是否为垃圾邮件，单词x出现在邮件中的概率条件独立于单词y。很明显这个假设不是不失一般性的，因为某些单词几乎总是同时出现。然而，最终结果是，这个简单的假设对结果的影响并不大，且无论如何都可以让我们快速判别垃圾邮件。</p>
</blockquote>
<h3 id="变量与随机变量有什么区别">1.4.2 变量与随机变量有什么区别</h3>
<p><strong>随机变量</strong>（random variable）</p>
<p>表示随机现象（在一定条件下，并不总是出现相同结果的现象称为随机现象）中各种结果的实值函数（一切可能的样本点）。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数等，都是随机变量的实例。<br />
​随机变量与模糊变量的不确定性的本质差别在于，后者的测定结果仍具有不确定性，即模糊性。</p>
<p><strong>变量与随机变量的区别：</strong><br />
​当变量的取值的概率不是1时,变量就变成了随机变量；当随机变量取值的概率为1时,随机变量就变成了变量。</p>
<blockquote>
<p>比如：<br />
​ 当变量<span class="math inline">\(x\)</span>值为100的概率为1的话,那么<span class="math inline">\(x=100\)</span>就是确定了的,不会再有变化,除非有进一步运算. ​ 当变量<span class="math inline">\(x\)</span>的值为100的概率不为1,比如为50的概率是0.5,为100的概率是0.5,那么这个变量就是会随不同条件而变化的,是随机变量,取到50或者100的概率都是0.5,即50%。</p>
</blockquote>
<h3 id="随机变量与概率分布的联系">1.4.3 随机变量与概率分布的联系</h3>
<p>一个随机变量仅仅表示一个可能取得的状态，还必须给定与之相伴的概率分布来制定每个状态的可能性。用来描述随机变量或一簇随机变量的每一个可能的状态的可能性大小的方法，就是 <strong>概率分布(probability distribution)</strong>.</p>
<p>随机变量可以分为离散型随机变量和连续型随机变量。</p>
<p>相应的描述其概率分布的函数是</p>
<p>概率质量函数(Probability Mass Function, PMF):描述离散型随机变量的概率分布，通常用大写字母 <span class="math inline">\(P\)</span>表示。</p>
<p>概率密度函数(Probability Density Function, PDF):描述连续型随机变量的概率分布，通常用小写字母<span class="math inline">\(p\)</span>表示。</p>
<h3 id="离散型随机变量和概率质量函数">1.4.4 离散型随机变量和概率质量函数</h3>
<p>PMF 将随机变量能够取得的每个状态映射到随机变量取得该状态的概率。</p>
<ul>
<li>一般而言，<span class="math inline">\(P(x)\)</span> 表示时<span class="math inline">\(X=x\)</span>的概率.</li>
<li>有时候为了防止混淆，要明确写出随机变量的名称<span class="math inline">\(P(\)</span>x<span class="math inline">\(=x)\)</span></li>
<li>有时候需要先定义一个随机变量，然后制定它遵循的概率分布x服从<span class="math inline">\(P(\)</span>x<span class="math inline">\()\)</span></li>
</ul>
<p>PMF 可以同时作用于多个随机变量，即联合概率分布(joint probability distribution) <span class="math inline">\(P(X=x,Y=y)\)</span>*表示 <span class="math inline">\(X=x\)</span>和<span class="math inline">\(Y=y\)</span>同时发生的概率，也可以简写成 <span class="math inline">\(P(x,y)\)</span>.</p>
<p>如果一个函数<span class="math inline">\(P\)</span>是随机变量 <span class="math inline">\(X\)</span> 的 PMF， 那么它必须满足如下三个条件</p>
<ul>
<li><span class="math inline">\(P\)</span>的定义域必须是的所有可能状态的集合</li>
<li><span class="math inline">\(∀x∈\)</span>x, $0 P(x) 1 $.</li>
<li><span class="math inline">\(∑_{x∈X} P(x)=1\)</span>. 我们把这一条性质称之为 归一化的(normalized)</li>
</ul>
<h3 id="连续型随机变量和概率密度函数">1.4.5 连续型随机变量和概率密度函数</h3>
<p>如果一个函数<span class="math inline">\(p\)</span>是x的PDF，那么它必须满足如下几个条件</p>
<ul>
<li><span class="math inline">\(p\)</span>的定义域必须是x的所有可能状态的集合。</li>
<li><span class="math inline">\(∀x∈X,p(x)≥0\)</span>. 注意，我们并不要求$ p(x)≤1$，因为此处 <span class="math inline">\(p(x)\)</span>不是表示的对应此状态具体的概率，而是概率的一个相对大小(密度)。具体的概率，需要积分去求。</li>
<li><span class="math inline">\(∫p(x)dx=1\)</span>, 积分下来，总和还是1，概率之和还是1.</li>
</ul>
<p>注：PDF<span class="math inline">\(p(x)\)</span>并没有直接对特定的状态给出概率，给出的是密度，相对的，它给出了落在面积为 <span class="math inline">\(δx\)</span>的无线小的区域内的概率为$ p(x)δx$. 由此，我们无法求得具体某个状态的概率，我们可以求得的是 某个状态 <span class="math inline">\(x\)</span> 落在 某个区间<span class="math inline">\([a,b]\)</span>内的概率为$ _{a}^{b}p(x)dx$.</p>
<h3 id="举例理解条件概率">1.4.6 举例理解条件概率</h3>
<p>条件概率公式如下： <span class="math inline">\(P(A|B) = P(A\cap B) / P(B)\)</span> 说明：在同一个样本空间<span class="math inline">\(\Omega\)</span>中的事件或者子集<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>，如果随机从<span class="math inline">\(\Omega\)</span>中选出的一个元素属于<span class="math inline">\(B\)</span>，那么下一个随机选择的元素属于<span class="math inline">\(A\)</span> 的概率就定义为在<span class="math inline">\(B\)</span>的前提下<span class="math inline">\(A\)</span>的条件概率。条件概率文氏图示意如图1.1所示。<br />
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210914233223.jpg" alt="条件概率" /></p>
<p>图1.1 条件概率文氏图示意</p>
<p>根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是<span class="math inline">\(P(A\bigcap B)\)</span>除以<span class="math inline">\(P(B)\)</span>。<br />
​举例：一对夫妻有两个小孩，已知其中一个是女孩，则另一个是女孩子的概率是多少？（面试、笔试都碰到过）<br />
​<strong>穷举法</strong>：已知其中一个是女孩，那么样本空间为男女，女女，女男，则另外一个仍然是女生的概率就是1/3。<br />
​<strong>条件概率法</strong>：<span class="math inline">\(P(女|女)=P(女女)/P(女)\)</span>,夫妻有两个小孩，那么它的样本空间为女女，男女，女男，男男，则<span class="math inline">\(P(女女)\)</span>为1/4，<span class="math inline">\(P（女）= 1-P(男男)=3/4\)</span>,所以最后<span class="math inline">\(1/3\)</span>。<br />
这里大家可能会误解，男女和女男是同一种情况，但实际上类似姐弟和兄妹是不同情况。</p>
<h3 id="联合概率与边缘概率联系区别">1.4.7 联合概率与边缘概率联系区别</h3>
<p><strong>区别：</strong><br />
​联合概率：联合概率指类似于<span class="math inline">\(P(X=a,Y=b)\)</span>这样，包含多个条件，且所有条件同时成立的概率。联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。<br />
​边缘概率：边缘概率是某个事件发生的概率，而与其它事件无关。边缘概率指类似于<span class="math inline">\(P(X=a)\)</span>，<span class="math inline">\(P(Y=b)\)</span>这样，仅与单个随机变量有关的概率。</p>
<p><strong>联系：</strong><br />
​联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。</p>
<h3 id="条件概率的链式法则">1.4.8 条件概率的链式法则</h3>
<p>由条件概率的定义，可直接得出下面的乘法公式：<br />
​乘法公式 设<span class="math inline">\(A, B\)</span>是两个事件，并且<span class="math inline">\(P(A) &gt; 0\)</span>, 则有 <span class="math inline">\(P(AB) = P(B|A)P(A)\)</span> 推广 <span class="math inline">\(P(ABC)=P(C|AB)P(B|A)P(A)\)</span> 一般地，用归纳法可证：若<span class="math inline">\(P(A_1A_2...A_n)&gt;0\)</span>，则有 <span class="math inline">\(P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})P(A_{n-1}|A_1A_2...A_{n-2})...P(A_2|A_1)P(A_1) =P(A_1)\prod_{i=2}^{n}P(A_i|A_1A_2...A_{i-1})\)</span> 任何多维随机变量联合概率分布，都可以分解成只有一个变量的条件概率相乘形式。</p>
<h3 id="独立性和条件独立性">1.4.9 独立性和条件独立性</h3>
<p><strong>独立性</strong> ​两个随机变量<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>，概率分布表示成两个因子乘积形式，一个因子只包含<span class="math inline">\(x\)</span>，另一个因子只包含<span class="math inline">\(y\)</span>，两个随机变量相互独立(independent)。<br />
​条件有时为不独立的事件之间带来独立，有时也会把本来独立的事件，因为此条件的存在，而失去独立性。<br />
​举例：<span class="math inline">\(P(XY)=P(X)P(Y)\)</span>, 事件<span class="math inline">\(X\)</span>和事件<span class="math inline">\(Y\)</span>独立。此时给定<span class="math inline">\(Z\)</span>， <span class="math inline">\(P(X,Y|Z) \not = P(X|Z)P(Y|Z)\)</span> 事件独立时，联合概率等于概率的乘积。这是一个非常好的数学性质，然而不幸的是，无条件的独立是十分稀少的，因为大部分情况下，事件之间都是互相影响的。</p>
<p><strong>条件独立性</strong><br />
​给定<span class="math inline">\(Z\)</span>的情况下,<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>条件独立，当且仅当 <span class="math inline">\(X\bot Y|Z \iff P(X,Y|Z) = P(X|Z)P(Y|Z)\)</span> <span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的关系依赖于<span class="math inline">\(Z\)</span>，而不是直接产生。</p>
<blockquote>
<p><strong>举例</strong>定义如下事件：<br />
<span class="math inline">\(X\)</span>：明天下雨；<br />
<span class="math inline">\(Y\)</span>：今天的地面是湿的；<br />
<span class="math inline">\(Z\)</span>：今天是否下雨；<br />
<span class="math inline">\(Z\)</span>事件的成立，对<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>均有影响，然而，在<span class="math inline">\(Z\)</span>事件成立的前提下，今天的地面情况对明天是否下雨没有影响。</p>
</blockquote>
<h2 id="常见概率分布">1.5 常见概率分布</h2>
<h3 id="bernoulli分布">1.5.1 Bernoulli分布</h3>
<p><strong>Bernoulli分布</strong>(伯努利分布，0-1分布)是单个二值随机变量分布, 单参数<span class="math inline">\(\phi\)</span>∈[0,1]控制,<span class="math inline">\(\phi\)</span>给出随机变量等于1的概率. 主要性质有: <span class="math inline">\(\begin{align*} P(x=1) &amp;= \phi \\ P(x=0) &amp;= 1-\phi \\ 概率质量函数：P(x=x) &amp;= \phi^x(1-\phi)^{1-x} \\ \end{align*}\)</span> 其期望和方差为： <span class="math inline">\(\begin{align*} E_x[x] &amp;= \phi \\ Var_x(x) &amp;= \phi{(1-\phi)} \end{align*}\)</span> <strong>适用范围</strong>: <strong>伯努利分布</strong>适合对<strong>离散型</strong>随机变量建模.</p>
<p><strong>Multinoulli分布</strong>也叫<strong>范畴分布</strong>, 是单个<em>k</em>值随机分布,经常用来表示<strong>对象分类的分布</strong>. 其中<span class="math inline">\(k\)</span>是有限值.Multinoulli分布由向量<span class="math inline">\(\vec{p}\in[0,1]^{k-1}\)</span>参数化,每个分量<span class="math inline">\(p_i\)</span>表示第<span class="math inline">\(i\)</span>个状态的概率, 且<span class="math inline">\(p_k=1-1^Tp\)</span>.这里<span class="math inline">\(1^T\)</span>表示元素全为1的列向量的转置，其实就是对于向量p中除了k的概率之和。可以重写为<span class="math inline">\(p_k=1-\sum_{0}^{k-1}p_i\)</span> 。</p>
<p>补充二项分布、多项分布：</p>
<p>二项分布，通俗点硬币抛多次。二项分布(Binomial distribution)是<strong>n重伯努利试验</strong>成功次数的离散概率分布。</p>
<p>多项式分布(Multinomial Distribution)是二项式分布的推广。二项式做n次伯努利实验，规定了每次试验的结果只有两个，如果现在还是做n次试验，只不过每次试验的结果可以有多m个，且m个结果发生的概率互斥且和为1，则发生其中一个结果X次的概率就是多项式分布。</p>
<h3 id="高斯分布">1.5.2 高斯分布</h3>
<p>高斯也叫正态分布(Normal Distribution), 概率度函数如下:<br />
<span class="math inline">\(N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}exp\left ( -\frac{1}{2\sigma^2}(x-\mu)^2 \right )\)</span> 其中, <span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>分别是均值和标准差, 中心峰值x坐标由<span class="math inline">\(\mu\)</span>给出, 峰的宽度受<span class="math inline">\(\sigma\)</span>控制, 最大点在<span class="math inline">\(x=\mu\)</span>处取得, 拐点为<span class="math inline">\(x=\mu\pm\sigma\)</span></p>
<p>正态分布中，±1<span class="math inline">\(\sigma\)</span>、±2<span class="math inline">\(\sigma\)</span>、±3<span class="math inline">\(\sigma\)</span>下的概率分别是68.3%、95.5%、99.73%，这3个数最好记住。</p>
<p>此外, 令<span class="math inline">\(\mu=0,\sigma=1\)</span>高斯分布即简化为标准正态分布: <span class="math inline">\(N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi}}exp\left ( -\frac{1}{2}x^2 \right )\)</span> 对概率密度函数高效求值: <span class="math inline">\(N(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2\pi}}exp\left(-\frac{1}{2}\beta(x-\mu)^2\right)\)</span></p>
<p>其中，<span class="math inline">\(\beta=\frac{1}{\sigma^2}\)</span>通过参数<span class="math inline">\(\beta∈（0，\infty）\)</span>来控制分布精度。</p>
<h3 id="何时采用正态分布">1.5.3 何时采用正态分布</h3>
<p>问: 何时采用正态分布? 答: 缺乏实数上分布的先验知识, 不知选择何种形式时, 默认选择正态分布总是不会错的, 理由如下:</p>
<ol type="1">
<li>中心极限定理告诉我们, 很多独立随机变量均近似服从正态分布, 现实中很多复杂系统都可以被建模成正态分布的噪声, 即使该系统可以被结构化分解.</li>
<li>正态分布是具有相同方差的所有概率分布中, 不确定性最大的分布, 换句话说, 正态分布是对模型加入先验知识最少的分布.</li>
</ol>
<p>正态分布的推广: 正态分布可以推广到<span class="math inline">\(R^n\)</span>空间, 此时称为<strong>多位正态分布</strong>, 其参数是一个正定对称矩阵<span class="math inline">\(\Sigma\)</span>: <span class="math inline">\(N(x;\vec\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right)\)</span> 对多为正态分布概率密度高效求值: <span class="math inline">\(N(x;\vec{\mu},\vec\beta^{-1}) = \sqrt{det(\vec\beta)}{(2\pi)^n}exp\left(-\frac{1}{2}(\vec{x}-\vec\mu)^T\beta(\vec{x}-\vec\mu)\right)\)</span> 此处，<span class="math inline">\(\vec\beta\)</span>是一个精度矩阵。</p>
<h3 id="指数分布">1.5.4 指数分布</h3>
<p>深度学习中, 指数分布用来描述在<span class="math inline">\(x=0\)</span>点处取得边界点的分布, 指数分布定义如下: <span class="math inline">\(p(x;\lambda)=\lambda I_{x\geq 0}exp(-\lambda{x})\)</span> 指数分布用指示函数<span class="math inline">\(I_{x\geq 0}\)</span>来使<span class="math inline">\(x\)</span>取负值时的概率为零。</p>
<h3 id="laplace-分布拉普拉斯分布">1.5.5 Laplace 分布（拉普拉斯分布）</h3>
<p>一个联系紧密的概率分布是 Laplace 分布（Laplace distribution），它允许我们在任意一点 <span class="math inline">\(\mu\)</span>处设置概率质量的峰值</p>
<p><span class="math inline">\(Laplace(x;\mu;\gamma)=\frac{1}{2\gamma}exp\left(-\frac{|x-\mu|}{\gamma}\right)\)</span></p>
<h3 id="dirac分布和经验分布">1.5.6 Dirac分布和经验分布</h3>
<p>Dirac分布可保证概率分布中所有质量都集中在一个点上. Diract分布的狄拉克<span class="math inline">\(\delta\)</span>函数(也称为<strong>单位脉冲函数</strong>)定义如下:</p>
<p><span class="math inline">\(p(x)=\delta(x-\mu), x\neq \mu\)</span></p>
<p><span class="math inline">\(\int_{a}^{b}\delta(x-\mu)dx = 1, a &lt; \mu &lt; b\)</span></p>
<p>Dirac 分布经常作为 经验分布（empirical distribution）的一个组成部分出现</p>
<p><span class="math inline">\(\hat{p}(\vec{x})=\frac{1}{m}\sum_{i=1}^{m}\delta(\vec{x}-{\vec{x}}^{(i)})\)</span></p>
<p>, 其中, m个点<span class="math inline">\(x^{1},...,x^{m}\)</span>是给定的数据集, <strong>经验分布</strong>将概率密度<span class="math inline">\(\frac{1}{m}\)</span>赋给了这些点.</p>
<p>当我们在训练集上训练模型时, 可以认为从这个训练集上得到的经验分布指明了<strong>采样来源</strong>.</p>
<p><strong>适用范围</strong>: 狄拉克δ函数适合对<strong>连续型</strong>随机变量的经验分布.</p>
<blockquote>

</blockquote>
<h2 id="期望方差协方差相关系数">1.6 期望、方差、协方差、相关系数</h2>
<h3 id="期望">1.6.1 期望</h3>
<p>在概率论和统计学中，数学期望（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和。它反映随机变量平均取值的大小。</p>
<ul>
<li>线性运算： <span class="math inline">\(E(ax+by+c) = aE(x)+bE(y)+c\)</span><br />
</li>
<li>推广形式： <span class="math inline">\(E(\sum_{k=1}^{n}{a_ix_i+c}) = \sum_{k=1}^{n}{a_iE(x_i)+c}\)</span></li>
<li>函数期望：设<span class="math inline">\(f(x)\)</span>为<span class="math inline">\(x\)</span>的函数，则<span class="math inline">\(f(x)\)</span>的期望为
<ul>
<li>离散函数： <span class="math inline">\(E(f(x))=\sum_{k=1}^{n}{f(x_k)P(x_k)}\)</span></li>
<li>连续函数： <span class="math inline">\(E(f(x))=\int_{-\infty}^{+\infty}{f(x)p(x)dx}\)</span></li>
</ul></li>
</ul>
<blockquote>
<p>注意：</p>
<ul>
<li>函数的期望大于等于期望的函数（Jensen（詹森）不等式，即<span class="math inline">\(E(f(x))\geqslant f(E(x))\)</span><br />
</li>
<li>一般情况下，乘积的期望不等于期望的乘积。<br />
</li>
<li>如果<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>相互独立，则<span class="math inline">\(E(xy)=E(x)E(y)\)</span>。</li>
</ul>
</blockquote>
<h3 id="方差">1.6.2 方差</h3>
<p>概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。方差是一种特殊的期望。定义为：</p>
<p><span class="math inline">\(Var(x) = E((x-E(x))^2)\)</span></p>
<blockquote>
<p>方差性质：</p>
<p>1）<span class="math inline">\(Var(x) = E(x^2) -E(x)^2\)</span><br />
2）常数的方差为0;<br />
3）方差不满足线性性质;<br />
4）如果<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>相互独立, <span class="math inline">\(Var(ax+by)=a^2Var(x)+b^2Var(y)\)</span></p>
</blockquote>
<h3 id="协方差">1.6.3 协方差</h3>
<p>协方差是衡量两个变量线性相关性强度及变量尺度。 两个随机变量的协方差定义为：</p>
<p><span class="math inline">\(Cov(x,y)=E((x-E(x))(y-E(y)))\)</span></p>
<p>方差是一种特殊的协方差。当<span class="math inline">\(X=Y\)</span>时，<span class="math inline">\(Cov(x,y)=Var(x)=Var(y)\)</span>。</p>
<blockquote>
<p>协方差性质：</p>
<p>1）独立变量的协方差为0。<br />
2）协方差计算公式：</p>
</blockquote>
<p><span class="math inline">\(Cov(\sum_{i=1}^{m}{a_ix_i}, \sum_{j=1}^{m}{b_jy_j}) = \sum_{i=1}^{m} \sum_{j=1}^{m}{a_ib_jCov(x_iy_i)}\)</span></p>
<blockquote>
<p>3）特殊情况：</p>
</blockquote>
<p><span class="math inline">\(Cov(a+bx, c+dy) = bdCov(x, y)\)</span></p>
<h3 id="相关系数">1.6.4 相关系数</h3>
<p>相关系数是研究变量之间线性相关程度的量。两个随机变量的相关系数定义为：</p>
<p><span class="math inline">\(Corr(x,y) = \frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}\)</span></p>
<blockquote>
<p>相关系数的性质：<br />
1）有界性。相关系数的取值范围是 [-1,1]，可以看成无量纲的协方差。<br />
2）值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性。</p>
</blockquote>
<h2 id="参考文献">参考文献</h2>
<p>[0] github.com/scutan90/DeepLearning-500-questions</p>
<p>[1]Ian，Goodfellow，Yoshua，Bengio，Aaron...深度学习[M]，人民邮电出版，2017</p>
<p>[2]周志华.机器学习[M].清华大学出版社，2016.</p>
<p>[3]同济大学数学系.高等数学（第七版）[M]，高等教育出版社，2014.</p>
<p>[4]盛骤，试式千，潘承毅等编. 概率论与数理统计（第4版）[M]，高等教育出版社，2008</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>DL基础</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>经典网络简介</title>
    <url>/2021/12/14/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h1 id="lenet-5">1 LeNet-5</h1>
<h2 id="模型介绍">1.1 模型介绍</h2>
<p>​ LeNet-5是由<span class="math inline">\(LeCun\)</span> 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，CNN）<span class="math inline">\(^{[1]}\)</span>，其命名来源于作者<span class="math inline">\(LeCun\)</span>的名字，5则是其研究成果的代号，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。</p>
<h2 id="模型结构">1.2 模型结构</h2>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet1.png" alt="LeNet-5网络结构图" /><figcaption>LeNet-5网络结构图</figcaption>
</figure>
<p>LeNet-5一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成，网络的参数配置如表4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的<span class="math inline">\(“5\times5\times1/1,6”\)</span>表示核大小为<span class="math inline">\(5\times5\times1\)</span>、步长为<span class="math inline">\(1\)</span>且核个数为6的卷积核）。</p>
<p>表4.1 LeNet-5网络参数配置</p>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 18%" />
<col style="width: 21%" />
<col style="width: 18%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">网络层</th>
<th style="text-align: center;">输入尺寸</th>
<th style="text-align: center;">核尺寸</th>
<th style="text-align: center;">输出尺寸</th>
<th style="text-align: center;">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(32\times32\times1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times1/1,6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times6\)</span></td>
<td style="text-align: center;"><span class="math inline">\((5\times5\times1+1)\times6\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times6\)</span></td>
<td style="text-align: center;"><span class="math inline">\((1+1)\times6\)</span> <span class="math inline">\(^*\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times6/1,16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(10\times10\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1516^*\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_4\)</span></td>
<td style="text-align: center;"><span class="math inline">\(10\times10\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\((1+1)\times16\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_5\)</span><span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times16/1,120\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times120\)</span></td>
<td style="text-align: center;"><span class="math inline">\((5\times5\times16+1)\times120\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(F_6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times120\)</span></td>
<td style="text-align: center;"><span class="math inline">\(120\times84\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times84\)</span></td>
<td style="text-align: center;"><span class="math inline">\((120+1)\times84\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">输出层</td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times84\)</span></td>
<td style="text-align: center;"><span class="math inline">\(84\times10\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times10\)</span></td>
<td style="text-align: center;"><span class="math inline">\((84+1)\times10\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>​ <span class="math inline">\(^*\)</span> 在LeNet中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是<span class="math inline">\((1+1)\times6\)</span>而不是零。</p>
<p>​ <span class="math inline">\(^*\)</span> <span class="math inline">\(C_3\)</span>卷积层可训练参数并未直接连接<span class="math inline">\(S_2\)</span>中所有的特征图（Feature Map），而是采用如图4.2所示的采样特征方式进行连接（稀疏连接），生成的16个通道特征图中分别按照相邻3个特征图、相邻4个特征图、非相邻4个特征图和全部6个特征图进行映射，得到的参数个数计算公式为<span class="math inline">\(6\times(25\times3+1)+6\times(25\times4+1)+3\times(25\times4+1)+1\times(25\times6+1)=1516\)</span>，在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）;强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。 <span class="math inline">\(^*\)</span> <span class="math inline">\(C_5\)</span>卷积层在图4.1中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在<span class="math inline">\(5\times5\)</span>卷积后尺寸被压缩为<span class="math inline">\(1\times1\)</span>，输出结果看起来和全连接很相似。</p>
</blockquote>
<h2 id="模型特性">1.3 模型特性</h2>
<ul>
<li>卷积网络使用一个3层的序列组合：卷积、下采样（池化）、非线性映射（LeNet-5最重要的特性，奠定了目前深层卷积网络的基础）</li>
<li>使用卷积提取空间特征</li>
<li>使用映射的空间均值进行下采样</li>
<li>使用<span class="math inline">\(tanh\)</span>或<span class="math inline">\(sigmoid\)</span>进行非线性映射</li>
<li>多层神经网络（MLP）作为最终的分类器</li>
<li>层间的稀疏连接矩阵以避免巨大的计算开销</li>
</ul>
<hr />
<h1 id="alexnet">2 AlexNet</h1>
<h2 id="模型介绍-1">2.1 模型介绍</h2>
<p>​ AlexNet是由<span class="math inline">\(Alex\)</span> $Krizhevsky <span class="math inline">\(提出的首个应用于图像分类的深层卷积神经网络，该网络在2012年ILSVRC（ImageNet Large Scale Visual Recognition Competition）图像分类竞赛中以15.3%的top-5测试错误率赢得第一名\)</span>^{[2]}$。AlexNet使用GPU代替CPU进行运算，使得在可接受的时间范围内模型结构能够更加复杂，它的出现证明了深层卷积神经网络在复杂模型下的有效性，使CNN在计算机视觉中流行开来，直接或间接地引发了深度学习的热潮。</p>
<h2 id="模型结构-1">2.2 模型结构</h2>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet2.png" alt="AlexNet网络结构图" /><figcaption>AlexNet网络结构图</figcaption>
</figure>
<p>如图4.3所示，除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, LRN），AlexNet一共包含8层，前5层由卷积层组成，而剩下的3层为全连接层。网络结构分为上下两层，分别对应两个GPU的操作过程，除了中间某些层（<span class="math inline">\(C_3\)</span>卷积层和<span class="math inline">\(F_{6-8}\)</span>全连接层会有GPU间的交互），其他层两个GPU分别计算结 果。最后一层全连接层的输出作为<span class="math inline">\(softmax\)</span>的输入，得到1000个图像分类标签对应的概率值。除去GPU并行结构的设计，AlexNet网络结构与LeNet十分相似，其网络的参数配置如表4.2所示。</p>
<p>表4.2 AlexNet网络参数配置</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 20%" />
<col style="width: 23%" />
<col style="width: 20%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">网络层</th>
<th style="text-align: center;">输入尺寸</th>
<th style="text-align: center;">核尺寸</th>
<th style="text-align: center;">输出尺寸</th>
<th style="text-align: center;">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_1\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(11\times11\times3/4,48(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(55\times55\times48(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((11\times11\times3+1)\times48\times2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max}\)</span><span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(55\times55\times48(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3/2(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(27\times27\times48(\times2_{GPU})\)</span></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(27\times27\times48(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times48/1,128(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(27\times27\times128(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((5\times5\times48+1)\times128\times2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(27\times27\times128(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3/2(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times128(\times2_{GPU})\)</span></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_3\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times128\times2_{GPU}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times256/1,192(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times256+1)\times192\times2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_4\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times192/1,192(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times192+1)\times192\times2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_5\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times192/1,128(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times128(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times192+1)\times128\times2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times128(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3/2(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(6\times6\times128(\times2_{GPU})\)</span></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层<span class="math inline">\(F_6\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(6\times6\times128\times2_{GPU}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(9216\times2048(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times2048(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((9216+1)\times2048\times2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(F_7\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times2048\times2_{GPU}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4096\times2048(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times2048(\times2_{GPU})\)</span></td>
<td style="text-align: center;"><span class="math inline">\((4096+1)\times2048\times2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层<span class="math inline">\(F_8\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times2048\times2_{GPU}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4096\times1000\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times1000\)</span></td>
<td style="text-align: center;"><span class="math inline">\((4096+1)\times1000\times2\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>卷积层<span class="math inline">\(C_1\)</span>输入为<span class="math inline">\(224\times224\times3\)</span>的图片数据，分别在两个GPU中经过核为<span class="math inline">\(11\times11\times3\)</span>、步长（stride）为4的卷积卷积后，分别得到两条独立的<span class="math inline">\(55\times55\times48\)</span>的输出数据。</p>
<p>下采样层<span class="math inline">\(S_{max}\)</span>实际上是嵌套在卷积中的最大池化操作，但是为了区分没有采用最大池化的卷积层单独列出来。在<span class="math inline">\(C_{1-2}\)</span>卷积层中的池化操作之后（ReLU激活操作之前），还有一个LRN操作，用作对相邻特征点的归一化处理。</p>
<p>卷积层<span class="math inline">\(C_3\)</span> 的输入与其他卷积层不同，<span class="math inline">\(13\times13\times192\times2_{GPU}\)</span>表示汇聚了上一层网络在两个GPU上的输出结果作为输入，所以在进行卷积操作时通道上的卷积核维度为384。</p>
<p>全连接层<span class="math inline">\(F_{6-8}\)</span>中输入数据尺寸也和<span class="math inline">\(C_3\)</span>类似，都是融合了两个GPU流向的输出结果作为输入。</p>
</blockquote>
<h2 id="模型特性-1">2.3 模型特性</h2>
<ul>
<li>所有卷积层都使用ReLU作为非线性映射函数，使模型收敛速度更快</li>
<li>在多个GPU上进行模型的训练，不但可以提高模型的训练速度，还能提升数据的使用规模</li>
<li>使用LRN对局部的特征进行归一化，结果作为ReLU激活函数的输入能有效降低错误率</li>
<li>重叠最大池化（overlapping max pooling），即池化范围z与步长s存在关系<span class="math inline">\(z&gt;s\)</span>（如<span class="math inline">\(S_{max}\)</span>中核尺度为<span class="math inline">\(3\times3/2\)</span>），避免平均池化（average pooling）的平均效应</li>
<li>使用<strong>随机丢弃技术（dropout）</strong>选择性地忽略训练中的单个神经元，避免模型的过拟合</li>
</ul>
<hr />
<h1 id="zfnet">3 ZFNet</h1>
<h2 id="模型介绍-2">3.1 模型介绍</h2>
<p>ZFNet是由<span class="math inline">\(Matthew\)</span> <span class="math inline">\(D. Zeiler\)</span>和<span class="math inline">\(Rob\)</span> <span class="math inline">\(Fergus\)</span>在AlexNet基础上提出的大型卷积网络，在2013年ILSVRC图像分类竞赛中以11.19%的错误率获得冠军（实际上原ZFNet所在的队伍并不是真正的冠军，原ZFNet以13.51%错误率排在第8，真正的冠军是<span class="math inline">\(Clarifai\)</span>这个队伍，而<span class="math inline">\(Clarifai\)</span>这个队伍所对应的一家初创公司的CEO又是<span class="math inline">\(Zeiler\)</span>，而且<span class="math inline">\(Clarifai\)</span>对ZFNet的改动比较小，所以通常认为是ZFNet获得了冠军）<span class="math inline">\(^{[3-4]}\)</span>。ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过<strong>反卷积</strong>（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。</p>
<h2 id="模型结构-2">3.2 模型结构</h2>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet3.png" alt="ZFNet网络结构图（原始结构图与AlexNet风格结构图）" /><figcaption>ZFNet网络结构图（原始结构图与AlexNet风格结构图）</figcaption>
</figure>
<p>如图4.4所示，ZFNet与AlexNet类似，都是由8层网络组成的卷积神经网络，其中包含5层卷积层和3层全连接层。两个网络结构最大的不同在于，ZFNet第一层卷积采用了<span class="math inline">\(7\times7\times3/2\)</span>的卷积核替代了AlexNet中第一层卷积核<span class="math inline">\(11\times11\times3/4\)</span>的卷积核。图4.5中ZFNet相比于AlexNet在第一层输出的特征图中包含更多中间频率的信息，而AlexNet第一层输出的特征图大多是低频或高频的信息，对中间频率特征的缺失导致后续网络层次如图4.5（c）能够学习到的特征不够细致，而导致这个问题的根本原因在于AlexNet在第一层中采用的卷积核和步长过大。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet4.png" alt="ZFNet" /><figcaption>ZFNet</figcaption>
</figure>
<p>（a）ZFNet第一层输出的特征图（b）AlexNet第一层输出的特征图（c）AlexNet第二层输出的特征图（d）ZFNet第二层输出的特征图</p>
<p>表4.3 ZFNet网络参数配置</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 18%" />
<col style="width: 21%" />
<col style="width: 18%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">网络层</th>
<th style="text-align: center;">输入尺寸</th>
<th style="text-align: center;">核尺寸</th>
<th style="text-align: center;">输出尺寸</th>
<th style="text-align: center;">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_1\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(7\times7\times3/2,96\)</span></td>
<td style="text-align: center;"><span class="math inline">\(110\times110\times96\)</span></td>
<td style="text-align: center;"><span class="math inline">\((7\times7\times3+1)\times96\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(110\times110\times96\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(55\times55\times96\)</span></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_2\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(55\times55\times96\)</span></td>
<td style="text-align: center;"><span class="math inline">\(5\times5\times96/2,256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\((5\times5\times96+1)\times256\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times256\)</span></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times256/1,384\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times384\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times256+1)\times384\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_4\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times384\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times384/1,384\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times384\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times384+1)\times384\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_5\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times384\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times384/1,256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times384+1)\times256\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(13\times13\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(6\times6\times256\)</span></td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层<span class="math inline">\(F_6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(6\times6\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(9216\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\((9216+1)\times4096\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(F_7\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4096\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\((4096+1)\times4096\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层<span class="math inline">\(F_8\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4096\times1000\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times1000\)</span></td>
<td style="text-align: center;"><span class="math inline">\((4096+1)\times1000\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>卷积层<span class="math inline">\(C_1\)</span>与AlexNet中的<span class="math inline">\(C_1\)</span>有所不同，采用<span class="math inline">\(7\times7\times3/2\)</span>的卷积核代替<span class="math inline">\(11\times11\times3/4\)</span>，使第一层卷积输出的结果可以包含更多的中频率特征，对后续网络层中多样化的特征组合提供更多选择，有利于捕捉更细致的特征。</p>
<p>卷积层<span class="math inline">\(C_2\)</span>采用了步长2的卷积核，区别于AlexNet中<span class="math inline">\(C_2\)</span>的卷积核步长，所以输出的维度有所差异。</p>
</blockquote>
<h2 id="模型特性-2">3.3 模型特性</h2>
<p>​ ZFNet与AlexNet在结构上几乎相同，此部分虽属于模型特性，但准确地说应该是ZFNet原论文中可视化技术的贡献。</p>
<ul>
<li>可视化技术揭露了激发模型中每层单独的特征图。</li>
<li>可视化技术允许观察在训练阶段特征的演变过程且诊断出模型的潜在问题。</li>
<li>可视化技术用到了多层解卷积网络，即由特征激活返回到输入像素空间。</li>
<li>可视化技术进行了分类器输出的敏感性分析，即通过阻止部分输入图像来揭示那部分对于分类是重要的。</li>
<li>可视化技术提供了一个非参数的不变性来展示来自训练集的哪一块激活哪个特征图，不仅需要裁剪输入图片，而且自上而下的投影来揭露来自每块的结构激活一个特征图。</li>
<li>可视化技术依赖于解卷积操作，即卷积操作的逆过程，将特征映射到像素上。</li>
</ul>
<hr />
<h1 id="network-in-network">4 Network in Network</h1>
<h2 id="模型介绍-3">4.1 模型介绍</h2>
<p>Network In Network (NIN)是由<span class="math inline">\(Min Lin\)</span>等人提出，在CIFAR-10和CIFAR-100分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为NIN<span class="math inline">\(^{[5]}\)</span>。NIN以一种全新的角度审视了卷积神经网络中的卷积核设计，通过引入子网络结构代替纯卷积中的线性映射部分，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中下一节中介绍的GoogLeNet的Inception结构就是来源于这个思想。</p>
<h2 id="模型结构-3">4.2 模型结构</h2>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet5.png" alt="NIN网络结构图" /><figcaption>NIN网络结构图</figcaption>
</figure>
<p>NIN由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，最大的不同在于多层感知器对局部特征进行了非线性的映射，而传统卷积的方式是线性的。NIN的网络参数配置表4.4所示（原论文并未给出网络参数，表中参数为编者结合网络结构图和CIFAR-100数据集以<span class="math inline">\(3\times3\)</span>卷积为例给出）。</p>
<p>表4.4 NIN网络参数配置（结合原论文NIN结构和CIFAR-100数据给出）</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 17%" />
<col style="width: 19%" />
<col style="width: 17%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">网络层</th>
<th style="text-align: center;">输入尺寸</th>
<th style="text-align: center;">核尺寸</th>
<th style="text-align: center;">输出尺寸</th>
<th style="text-align: center;">参数个数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">局部全连接层<span class="math inline">\(L_{11}\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(32\times32\times3\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3)\times16/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(30\times30\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times3+1)\times16\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(L_{12}\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(30\times30\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(16\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(30\times30\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\(((16+1)\times16)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">局部全连接层<span class="math inline">\(L_{21}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(30\times30\times16\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3)\times64/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times16+1)\times64\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(L_{22}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(64\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(((64+1)\times64)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">局部全连接层<span class="math inline">\(L_{31}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3)\times100/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times100\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times64+1)\times100\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(L_{32}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times100\)</span></td>
<td style="text-align: center;"><span class="math inline">\(100\times100\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times100\)</span></td>
<td style="text-align: center;"><span class="math inline">\(((100+1)\times100)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">全局平均采样<span class="math inline">\(GAP\)</span> <span class="math inline">\(^*\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times100\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26\times26\times100/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1\times100\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>局部全连接层<span class="math inline">\(L_{11}\)</span>实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为<span class="math inline">\(30\times30\)</span>（<span class="math inline">\(\frac{32-3_k+1}{1_{stride}}=30\)</span>） 全连接层<span class="math inline">\(L_{12}\)</span>是紧跟<span class="math inline">\(L_{11}\)</span>后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接<span class="math inline">\(L_{11}\)</span>和<span class="math inline">\(L_{12}\)</span>的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。 全局平均采样层或全局平均池化层<span class="math inline">\(GAP\)</span>（Global Average Pooling）将<span class="math inline">\(L_{32}\)</span>输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。</p>
</blockquote>
<h2 id="模型特点">4.3 模型特点</h2>
<ul>
<li>使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。</li>
<li>使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。</li>
</ul>
<hr />
<h1 id="vggnet">4.5 VGGNet</h1>
<h2 id="模型介绍-4">4.5.1 模型介绍</h2>
<p>​ VGGNet是由牛津大学视觉几何小组（Visual Geometry Group, VGG）提出的一种深层卷积网络结构，他们以7.32%的错误率赢得了2014年ILSVRC分类任务的亚军（冠军由GoogLeNet以6.65%的错误率夺得）和25.32%的错误率夺得定位任务（Localization）的第一名（GoogLeNet错误率为26.44%）<span class="math inline">\(^{[5]}\)</span>，网络名称VGGNet取自该小组名缩写。VGGNet是首批把图像分类的错误率降低到10%以内模型，同时该网络所采用的<span class="math inline">\(3\times3\)</span>卷积核的思想是后来许多模型的基础，该模型发表在2015年国际学习表征会议（International Conference On Learning Representations, ICLR）后至今被引用的次数已经超过1万4千余次。</p>
<h2 id="模型结构-4">5.2 模型结构</h2>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet6.png" alt="VGG16网络结构图" /><figcaption>VGG16网络结构图</figcaption>
</figure>
<p>在原论文中的VGGNet包含了6个版本的演进，分别对应VGG11、VGG11-LRN、VGG13、VGG16-1、VGG16-3和VGG19，不同的后缀数值表示不同的网络层数（VGG11-LRN表示在第一层中采用了LRN的VGG11，VGG16-1表示后三组卷积块中最后一层卷积采用卷积核尺寸为<span class="math inline">\(1\times1\)</span>，相应的VGG16-3表示卷积核尺寸为<span class="math inline">\(3\times3\)</span>），本节介绍的VGG16为VGG16-3。图4.7中的VGG16体现了VGGNet的核心思路，使用<span class="math inline">\(3\times3\)</span>的卷积组合代替大尺寸的卷积（2个<span class="math inline">\(3\times3卷积即可与\)</span><span class="math inline">\(5\times5\)</span>卷积拥有相同的感受视野），网络参数设置如表4.5所示。</p>
<p>​ 表4.5 VGG16网络参数配置</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 17%" />
<col style="width: 24%" />
<col style="width: 17%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">网络层</th>
<th style="text-align: center;">输入尺寸</th>
<th style="text-align: center;">核尺寸</th>
<th style="text-align: center;">输出尺寸</th>
<th style="text-align: center;">参数个数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{11}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times64/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times3+1)\times64\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{12}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times64/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times64+1)\times64\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(224\times224\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(112\times112\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{21}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(112\times112\times64\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times128/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(112\times112\times128\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times64+1)\times128\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{22}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(112\times112\times128\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times128/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(112\times112\times128\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times128+1)\times128\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(112\times112\times128\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times128\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{31}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times128\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times256/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times128+1)\times256\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{32}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times256/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times256+1)\times256\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{33}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times256/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times256+1)\times256\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max3}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(56\times56\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{41}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times256\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times512/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times256+1)\times512\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{42}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times512/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{43}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times512/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max4}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(28\times28\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{51}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times512/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{52}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times512/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">卷积层<span class="math inline">\(C_{53}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(3\times3\times512/1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">下采样层<span class="math inline">\(S_{max5}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(14\times14\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2\times2/2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(7\times7\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层<span class="math inline">\(FC_{1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(7\times7\times512\)</span></td>
<td style="text-align: center;"><span class="math inline">\((7\times7\times512)\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\((7\times7\times512+1)\times4096\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">全连接层<span class="math inline">\(FC_{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4096\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\((4096+1)\times4096\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层<span class="math inline">\(FC_{3}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times4096\)</span></td>
<td style="text-align: center;"><span class="math inline">\(4096\times1000\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\times1000\)</span></td>
<td style="text-align: center;"><span class="math inline">\((4096+1)\times1000\)</span></td>
</tr>
</tbody>
</table>
<h2 id="模型特性-3">5.3 模型特性</h2>
<ul>
<li>整个网络都使用了同样大小的卷积核尺寸<span class="math inline">\(3\times3\)</span>和最大池化尺寸<span class="math inline">\(2\times2\)</span>。</li>
<li><span class="math inline">\(1\times1\)</span>卷积的意义主要在于线性变换，而输入通道数和输出通道数不变，没有发生降维。</li>
<li>两个<span class="math inline">\(3\times3\)</span>的卷积层串联相当于1个<span class="math inline">\(5\times5\)</span>的卷积层，感受野大小为<span class="math inline">\(5\times5\)</span>。同样地，3个<span class="math inline">\(3\times3\)</span>的卷积层串联的效果则相当于1个<span class="math inline">\(7\times7\)</span>的卷积层。这样的连接方式使得网络参数量更小，而且多层的激活函数令网络对特征的学习能力更强。</li>
<li>VGGNet在训练时有一个小技巧，先训练浅层的的简单网络VGG11，再复用VGG11的权重来初始化VGG13，如此反复训练并初始化VGG19，能够使训练时收敛的速度更快。</li>
<li>在训练过程中使用多尺度的变换对原始数据做数据增强，使得模型不易过拟合。</li>
</ul>
<hr />
<h1 id="googlenet">6 GoogLeNet</h1>
<h2 id="模型介绍-5">6.1 模型介绍</h2>
<p>GoogLeNet作为2014年ILSVRC在分类任务上的冠军，以6.65%的错误率力压VGGNet等模型，在分类的准确率上面相比过去两届冠军ZFNet和AlexNet都有很大的提升。从名字<strong>GoogLe</strong>Net可以知道这是来自谷歌工程师所设计的网络结构，而名字中Goog<strong>LeNet</strong>更是致敬了LeNet<span class="math inline">\(^{[0]}\)</span>。GoogLeNet中最核心的部分是其内部子网络结构Inception，该结构灵感来源于NIN，至今已经经历了四次版本迭代（Inception<span class="math inline">\(_{v1-4}\)</span>）。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet7.png" alt="Inception性能比较图" /><figcaption>Inception性能比较图</figcaption>
</figure>
<h2 id="模型结构-5">6.2 模型结构</h2>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214classNet8.png" alt="GoogLeNet网络结构图" /><figcaption>GoogLeNet网络结构图</figcaption>
</figure>
<p>GoogLeNet相比于以前的卷积神经网络结构，除了在深度上进行了延伸，还对网络的宽度进行了扩展，整个网络由许多块状子网络的堆叠而成，这个子网络构成了Inception结构。图为Inception的四个版本：<span class="math inline">\(Inception_{v1}\)</span>在同一层中采用不同的卷积核，并对卷积结果进行合并;<span class="math inline">\(Inception_{v2}\)</span>组合不同卷积核的堆叠形式，并对卷积结果进行合并;<span class="math inline">\(Inception_{v3}\)</span>则在<span class="math inline">\(v_2\)</span>基础上进行深度组合的尝试;<span class="math inline">\(Inception_{v4}\)</span>结构相比于前面的版本更加复杂，子网络中嵌套着子网络。</p>
<h2 id="模型特性-4">6.3 模型特性</h2>
<ul>
<li><p>采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；</p></li>
<li><p>之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；</p></li>
<li><p>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2，采用1x1卷积核来进行降维。</p></li>
</ul>
<hr />
<p><strong>为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？</strong></p>
<ul>
<li>评测对比：为了让自己的结果更有说服力，在发表自己成果的时候会同一个标准的baseline及在baseline上改进而进行比较，常见的比如各种检测分割的问题都会基于VGG或者Resnet101这样的基础网络。</li>
<li>时间和精力有限：在科研压力和工作压力中，时间和精力只允许大家在有限的范围探索。</li>
<li>模型创新难度大：进行基本模型的改进需要大量的实验和尝试，并且需要大量的实验积累和强大灵感，很有可能投入产出比比较小。</li>
<li>资源限制：创造一个新的模型需要大量的时间和计算资源，往往在学校和小型商业团队不可行。</li>
<li>在实际的应用场景中，其实是有大量的非标准模型的配置。</li>
</ul>
<hr />
<h2 id="参考文献">参考文献</h2>
<p>[0] github.com/scutan90/DeepLearning-500-questions</p>
<p>[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE</em>, november 1998.</p>
<p>[2] A. Krizhevsky, I. Sutskever and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. <em>Advances in Neural Information Processing Systems 25</em>. Curran Associates, Inc. 1097–1105.</p>
<p>[3] LSVRC-2013. http://www.image-net.org/challenges/LSVRC/2013/results.php</p>
<p>[4] M. D. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. <em>European Conference on Computer Vision</em>.</p>
<p>[5] M. Lin, Q. Chen, and S. Yan. Network in network. <em>Computing Research Repository</em>, abs/1312.4400, 2013.</p>
<p>[6] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>International Conference on Machine Learning</em>, 2015.</p>
<p>[7] Bharath Raj. <a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">a-simple-guide-to-the-versions-of-the-inception-network</a>, 2018.</p>
<p>[8] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. <a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a>, 2016.</p>
<p>[9] Sik-Ho Tsang. <a href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification</a>, 2018.</p>
<p>[10] Zbigniew Wojna, Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens. <a href="https://arxiv.org/pdf/1512.00567v3.pdf">Rethinking the Inception Architecture for Computer Vision</a>, 2015.</p>
<p>[11] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. <a href="https://arxiv.org/pdf/1409.4842v1.pdf">Going deeper with convolutions</a>, 2014.</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>DL基础</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积网络</title>
    <url>/2021/12/14/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>卷积神经网络是一种用来处理局部和整体相关性的计算网络结构，被应用在图像识别、自然语言处理甚至是语音识别领域，因为图像数据具有显著的局部与整体关系，其在图像识别领域的应用获得了巨大的成功。</p>
<h1 id="卷积神经网络的组成层">1 卷积神经网络的组成层</h1>
<p>​ 以图像分类任务为例，在表1所示卷积神经网络中，一般包含5种类型的网络层次结构：</p>
<p>​ 表1 卷积神经网络的组成</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">CNN层次结构</th>
<th style="text-align: center;">输出尺寸</th>
<th style="text-align: left;">作用</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">输入层</td>
<td style="text-align: center;"><span class="math inline">\(W_1\times H_1\times 3\)</span></td>
<td style="text-align: left;">卷积网络的原始输入，可以是原始或预处理后的像素矩阵</td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积层</td>
<td style="text-align: center;"><span class="math inline">\(W_1\times H_1\times K\)</span></td>
<td style="text-align: left;">参数共享、局部连接，利用平移不变性从全局特征图提取局部特征</td>
</tr>
<tr class="odd">
<td style="text-align: center;">激活层</td>
<td style="text-align: center;"><span class="math inline">\(W_1\times H_1\times K\)</span></td>
<td style="text-align: left;">将卷积层的输出结果进行非线性映射</td>
</tr>
<tr class="even">
<td style="text-align: center;">池化层</td>
<td style="text-align: center;"><span class="math inline">\(W_2\times H_2\times K\)</span></td>
<td style="text-align: left;">进一步筛选特征，可以有效减少后续网络层次所需的参数量</td>
</tr>
<tr class="odd">
<td style="text-align: center;">全连接层</td>
<td style="text-align: center;"><span class="math inline">\((W_2 \cdot H_2 \cdot K)\times C\)</span></td>
<td style="text-align: left;">将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值）</td>
</tr>
</tbody>
</table>
<blockquote>
<p><span class="math inline">\(W_1\times H_1\times 3\)</span>对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;<span class="math inline">\(K\)</span>表示卷积层中卷积核（滤波器）的个数;<span class="math inline">\(W_2\times H_2\)</span> 为池化后特征图的尺度，在全局池化中尺度对应<span class="math inline">\(1\times 1\)</span>;<span class="math inline">\((W_2 \cdot H_2 \cdot K)\)</span>是将多维特征压缩到1维之后的大小，<span class="math inline">\(C\)</span>对应的则是图像类别个数。</p>
</blockquote>
<h2 id="输入层">1.1 输入层</h2>
<p>​ 输入层(Input Layer)通常是输入卷积神经网络的原始数据或经过预处理的数据，可以是图像识别领域中原始三维的多彩图像，也可以是音频识别领域中经过傅利叶变换的二维波形数据，甚至是自然语言处理中一维表示的句子向量。以图像分类任务为例，输入层输入的图像一般包含RGB三个通道，是一个由长宽分别为<span class="math inline">\(H\)</span>和<span class="math inline">\(W\)</span>组成的3维像素值矩阵<span class="math inline">\(H\times W \times 3\)</span>，卷积网络会将输入层的数据传递到一系列卷积、池化等操作进行特征提取和转化，最终由全连接层对特征进行汇总和结果输出。根据计算能力、存储大小和模型结构的不同，卷积神经网络每次可以批量处理的图像个数不尽相同，若指定输入层接收到的图像个数为<span class="math inline">\(N\)</span>，则输入层的输出数据为<span class="math inline">\(N\times H\times W\times 3\)</span>。</p>
<h2 id="卷积层">1.2 卷积层</h2>
<p>卷积层(Convolution Layer)通常用作对输入层输入数据进行特征提取，通过卷积核矩阵对原始数据中隐含关联性的一种抽象。卷积操作原理上其实是对两张像素矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。图5.1表示卷积操作过程中的不同填充策略，上半部分采用零填充，下半部分采用有效卷积（舍弃不能完整运算的边缘部分）。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN1.png" alt="卷积操作示意图" /><figcaption>卷积操作示意图</figcaption>
</figure>
<h2 id="激活层">1.3 激活层</h2>
<p>​ 激活层(Activation Layer)负责对卷积层抽取的特征进行激活，由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。激活层主要由激活函数组成，即在卷积层输出结果的基础上嵌套一个非线性函数，让输出的特征图具有非线性关系。卷积网络中通常采用ReLU来充当激活函数（还包括tanh和sigmoid等）ReLU的函数形式如公式（5-1）所示，能够限制小于0的值为0,同时大于等于0的值保持不变。 <span class="math display">\[
f(x)=\begin{cases}
   0 &amp;\text{if } x&lt;0 \\
   x &amp;\text{if } x\ge 0
\end{cases}
\]</span></p>
<h2 id="池化层">1.4 池化层</h2>
<p>​ 池化层又称为降采样层(Downsampling Layer)，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化。</p>
<h2 id="全连接层">1.5 全连接层</h2>
<p>​ 全连接层(Full Connected Layer)负责对卷积神经网络学习提取到的特征进行汇总，将多维的特征输入映射为二维的特征输出，高维表示样本批次，低位常常对应任务目标。</p>
<hr />
<h1 id="卷积在图像中有什么直观作用">2 卷积在图像中有什么直观作用</h1>
<p>​ 在卷积神经网络中，卷积常用来提取图像的特征，但不同层次的卷积操作提取到的特征类型是不相同的，特征类型粗分如表所示。 ​ 卷积提取的特征类型</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">卷积层次</th>
<th style="text-align: center;">特征类型</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">浅层卷积</td>
<td style="text-align: center;">边缘特征</td>
</tr>
<tr class="even">
<td style="text-align: center;">中层卷积</td>
<td style="text-align: center;">局部特征</td>
</tr>
<tr class="odd">
<td style="text-align: center;">深层卷积</td>
<td style="text-align: center;">全局特征</td>
</tr>
</tbody>
</table>
<p>图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。表5.3显示了应用不同类型的卷积核（滤波器）后的各种卷积图像。</p>
<p>表5.3 一些常见卷积核的作用</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 41%" />
<col style="width: 41%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">卷积作用</th>
<th style="text-align: center;">卷积核</th>
<th style="text-align: center;">卷积后图像</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">输出原图</td>
<td style="text-align: center;"><span class="math inline">\(\begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span></td>
<td style="text-align: center;"><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN2.png" /></td>
</tr>
<tr class="even">
<td style="text-align: center;">边缘检测（突出边缘差异）</td>
<td style="text-align: center;"><span class="math inline">\(\begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; 0 &amp; 1 \end{bmatrix}\)</span></td>
<td style="text-align: center;"><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN3.png" /></td>
</tr>
<tr class="odd">
<td style="text-align: center;">边缘检测（突出中间值）</td>
<td style="text-align: center;"><span class="math inline">\(\begin{bmatrix} -1 &amp; -1 &amp; -1 \\ -1 &amp; 8 &amp; -1 \\ -1 &amp; -1 &amp; -1 \end{bmatrix}\)</span></td>
<td style="text-align: center;"><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN4.png" /></td>
</tr>
<tr class="even">
<td style="text-align: center;">图像锐化</td>
<td style="text-align: center;"><span class="math inline">\(\begin{bmatrix} 0 &amp; -1 &amp; 0 \\ -1 &amp; 5 &amp; -1 \\ 0 &amp; -1 &amp; 0 \end{bmatrix}\)</span></td>
<td style="text-align: center;"><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN5.png" /></td>
</tr>
<tr class="odd">
<td style="text-align: center;">方块模糊</td>
<td style="text-align: center;"><span class="math inline">\(\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 \end{bmatrix} \times \frac{1}{9}\)</span></td>
<td style="text-align: center;"><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN6.png" /></td>
</tr>
<tr class="even">
<td style="text-align: center;">高斯模糊</td>
<td style="text-align: center;"><span class="math inline">\(\begin{bmatrix} 1 &amp; 2 &amp; 1 \\ 2 &amp; 4 &amp; 2 \\ 1 &amp; 2 &amp; 1 \end{bmatrix} \times \frac{1}{16}\)</span></td>
<td style="text-align: center;"><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN7.png" /></td>
</tr>
</tbody>
</table>
<hr />
<h1 id="卷积层有哪些基本参数">3 卷积层有哪些基本参数？</h1>
<p>卷积层中需要用到卷积核（滤波器或特征检测器）与图像特征矩阵进行点乘运算，利用卷积核与对应的特征感受域进行划窗式运算时，需要设定卷积核对应的大小、步长、个数以及填充的方式，如表5.4所示。</p>
<p>​ 表5.4 卷积层的基本参数</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 41%" />
<col style="width: 41%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">参数名</th>
<th style="text-align: left;">作用</th>
<th style="text-align: left;">常见设置</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">卷积核大小 (Kernel Size)</td>
<td style="text-align: left;">卷积核的大小定义了卷积的感受野</td>
<td style="text-align: left;">在过去常设为5，如LeNet-5；现在多设为3，通过堆叠<span class="math inline">\(3\times3\)</span>的卷积核来达到更大的感受域</td>
</tr>
<tr class="even">
<td style="text-align: center;">卷积核步长 (Stride)</td>
<td style="text-align: left;">定义了卷积核在卷积过程中的步长</td>
<td style="text-align: left;">常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样</td>
</tr>
<tr class="odd">
<td style="text-align: center;">填充方式 (Padding)</td>
<td style="text-align: left;">在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略</td>
<td style="text-align: left;">设置为'SAME'表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；当设置为'VALID'时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致</td>
</tr>
<tr class="even">
<td style="text-align: center;">输入通道数 (In Channels)</td>
<td style="text-align: left;">指定卷积操作时卷积核的深度</td>
<td style="text-align: left;">默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式</td>
</tr>
<tr class="odd">
<td style="text-align: center;">输出通道数 (Out Channels)</td>
<td style="text-align: left;">指定卷积核的个数</td>
<td style="text-align: left;">若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量</td>
</tr>
</tbody>
</table>
<blockquote>
<p>卷积操作维度变换公式：</p>
<p><span class="math inline">\(O_d =\begin{cases} \lceil \frac{(I_d - k_{size})+ 1)}{s}\rceil ,&amp; \text{padding=VALID}\\ \lceil \frac{I_d}{s}\rceil,&amp;\text{padding=SAME} \end{cases}\)</span></p>
<p>其中，<span class="math inline">\(I_d\)</span>为输入维度，<span class="math inline">\(O_d\)</span>为输出维度，<span class="math inline">\(k_{size}\)</span>为卷积核大小，<span class="math inline">\(s\)</span>为步长</p>
</blockquote>
<hr />
<h1 id="卷积核有什么类型">4 卷积核有什么类型？</h1>
<p>​ 常见的卷积主要是由连续紧密的卷积核对输入的图像特征进行滑窗式点乘求和操作，除此之外还有其他类型的卷积核在不同的任务中会用到，具体分类如表5.5所示。</p>
<p>表5.5 卷积核分类</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN8.png" /></p>
<hr />
<h1 id="二维卷积与三维卷积有什么区别">5 二维卷积与三维卷积有什么区别？</h1>
<ul>
<li><strong>二维卷积</strong> 二维卷积操作如图5.3所示，为了更直观的说明，分别展示在单通道和多通道输入中，对单个通道输出的卷积操作。在单通道输入的情况下，若输入卷积核尺寸为 <span class="math inline">\((k_h, k_w, 1)\)</span>，卷积核在输入图像的空间维度上进行滑窗操作，每次滑窗和 <span class="math inline">\((k_h, k_w)\)</span>窗口内的值进行卷积操作，得到输出图像中的一个值。在多通道输入的情况下，假定输入图像特征通道数为3，卷积核尺寸则为<span class="math inline">\((k_h, k_w, 3)\)</span>，每次滑窗与3个通道上的<span class="math inline">\((k_h, k_w)\)</span>窗口内的所有值进行卷积操作，得到输出图像中的一个值。</li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN9.png" /></p>
<ul>
<li><strong>三维卷积</strong> 3D卷积操作如图所示，同样分为单通道和多通道，且假定只使用1个卷积核，即输出图像仅有一个通道。对于单通道输入，与2D卷积不同之处在于，输入图像多了一个深度(depth)维度，卷积核也多了一个<span class="math inline">\(k_d\)</span>维度，因此3D卷积核的尺寸为<span class="math inline">\((k_h, k_w, k_d)\)</span>，每次滑窗与<span class="math inline">\((k_h, k_w, k_d)\)</span>窗口内的值进行相关操作，得到输出3D图像中的一个值。对于多通道输入，则与2D卷积的操作一样，每次滑窗与3个channels上的<span class="math inline">\((k_h, k_w, k_d)\)</span>窗口内的所有值进行相关操作，得到输出3D图像中的一个值。</li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN10.png" /></p>
<hr />
<h1 id="有哪些池化方法">6 有哪些池化方法？</h1>
<p>池化操作通常也叫做子采样(Subsampling)或降采样(Downsampling)，在构建卷积神经网络时，往往会用在卷积层之后，通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。 表5.6 池化分类</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN11.png" /></p>
<blockquote>
<p>SPPNet<span class="math inline">\(^{[3]}\)</span>就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同<span class="math inline">\((win_{size}=\lceil \frac{in}{out}\rceil; stride=\lfloor \frac{in}{out}\rfloor; )\)</span>，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取</p>
</blockquote>
<hr />
<h1 id="times1卷积作用">7 <span class="math inline">\(1\times1\)</span>卷积作用？</h1>
<p>NIN(Network in Network)<span class="math inline">\(^{[4]}\)</span>是第一篇探索<span class="math inline">\(1\times1\)</span>卷积核的论文，这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套MLP子网络而得名NIN。NIN对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用<span class="math inline">\(1\times1\)</span>的卷积进行代替。</p>
<p>​ GoogLeNet<span class="math inline">\(^{[5]}\)</span>则采用<span class="math inline">\(1\times1\)</span>卷积核来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入<span class="math inline">\(1\times1\)</span>卷积，可以通过分离通道与宽高卷积来减少模型参数量。以图5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为<span class="math inline">\(C_1=16\)</span>，则左半边网络模块所需的参数为<span class="math inline">\((1\times1+3\times3+5\times5+0)\times C_1\times C_1=8960\)</span>；假定右半边网络模块采用的<span class="math inline">\(1\times1\)</span>卷积通道数为<span class="math inline">\(C_2=8\)</span><span class="math inline">\((满足C_1&gt;C_2)\)</span>，则右半部分的网络结构所需参数量为<span class="math inline">\((1\times1\times (3C_1+C_2)+3\times3\times C_2 +5\times5\times C_2)\times C_1=5248\)</span> ，可以在不改变模型表达能力的前提下大大减少所使用的参数量。</p>
<p>综上所述，<span class="math inline">\(1\times 1\)</span>卷积的作用主要为以下两点：</p>
<ul>
<li>实现信息的跨通道交互和整合。</li>
<li>对卷积核通道数进行降维和升维，减小参数量。</li>
</ul>
<hr />
<h1 id="卷积层和池化层有什么区别">8 卷积层和池化层有什么区别？</h1>
<p>卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出，但是其内在操作是有本质区别的，如表5.7所示。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">卷积层</th>
<th style="text-align: center;">池化层</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>结构</strong></td>
<td style="text-align: center;">零填充时输出维度不变，而通道数改变</td>
<td style="text-align: center;">通常特征维度会降低，通道数不变</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>稳定性</strong></td>
<td style="text-align: center;">输入特征发生细微改变时，输出结果会改变</td>
<td style="text-align: center;">感受域内的细微变化不影响输出结果</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>作用</strong></td>
<td style="text-align: center;">感受域内提取局部关联特征</td>
<td style="text-align: center;">感受域内提取泛化特征，降低维度</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>参数量</strong></td>
<td style="text-align: center;">与卷积核尺寸、卷积核个数相关</td>
<td style="text-align: center;">不引入额外参数</td>
</tr>
</tbody>
</table>
<hr />
<h1 id="卷积核是否一定越大越好">9 卷积核是否一定越大越好？</h1>
<p>在早期的卷积神经网络中（如LeNet-5、AlexNet），用到了一些较大的卷积核（<span class="math inline">\(11\times11\)</span>和<span class="math inline">\(5\times 5\)</span>），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠2个<span class="math inline">\(3\times 3\)</span>卷积核可以获得与<span class="math inline">\(5\times 5\)</span>卷积核相同的感受视野，同时参数量会更少（<span class="math inline">\(3×3×2+1\)</span> &lt; $ 5×5×1+1$），<span class="math inline">\(3\times 3\)</span>卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。</p>
<p>​ 但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。</p>
<p>​ 综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的<span class="math inline">\(1\times 1\)</span>极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。</p>
<hr />
<h1 id="每层卷积是否只能用一种尺寸的卷积核">10 每层卷积是否只能用一种尺寸的卷积核？</h1>
<p>经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如VGG结构中使用了大量的<span class="math inline">\(3×3\)</span>卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如GoogLeNet、Inception系列的网络，均是每层使用了多个卷积核结构。如图5.3所示，输入的特征在同一层分别经过<span class="math inline">\(1×1\)</span>、<span class="math inline">\(3×3\)</span>和<span class="math inline">\(5×5\)</span>三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214cnn12.png" /></p>
<hr />
<h1 id="怎样才能减少卷积层参数量">11 怎样才能减少卷积层参数量？</h1>
<p>减少卷积层参数量的方法可以简要地归为以下几点：</p>
<ul>
<li>使用堆叠小卷积核代替大卷积核：VGG网络中2个<span class="math inline">\(3\times 3\)</span>的卷积核可以代替1个<span class="math inline">\(5\times 5\)</span>的卷积核</li>
<li>使用分离卷积操作：将原本<span class="math inline">\(K\times K\times C\)</span>的卷积操作分离为<span class="math inline">\(K\times K\times 1\)</span>和<span class="math inline">\(1\times1\times C\)</span>的两部分操作</li>
<li>添加<span class="math inline">\(1\times 1\)</span>的卷积操作：与分离卷积类似，但是通道数可变，在<span class="math inline">\(K\times K\times C_1\)</span>卷积前添加<span class="math inline">\(1\times1\times C_2\)</span>的卷积核（满足<span class="math inline">\(C_2 &lt;C_1\)</span>）</li>
<li>在卷积层前使用池化操作：池化可以降低卷积层的输入特征维度</li>
</ul>
<hr />
<h1 id="在进行卷积操作时必须同时考虑通道和区域吗">12 在进行卷积操作时，必须同时考虑通道和区域吗？</h1>
<p>标准卷积中，采用区域与通道同时处理的操作，如下图所示：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN13.png" /></p>
<p>这样做可以简化卷积层内部的结构，每一个输出的特征像素都由所有通道的同一个区域提取而来。</p>
<p>​ 但是这种方式缺乏灵活性，并且在深层的网络结构中使得运算变得相对低效，更为灵活的方式是使区域和通道的卷积分离开来，通道分离（深度分离）卷积网络由此诞生。如下图所示，Xception网络可解决上述问题。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN14.png" /></p>
<p>我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道特征矩阵之后，再对这批新通道特征进行标准的<span class="math inline">\(1×1\)</span>跨通道卷积操作。</p>
<hr />
<h1 id="采用宽卷积的好处有什么">13 采用宽卷积的好处有什么？</h1>
<p>宽卷积对应的是窄卷积，实际上并不是卷积操作的类型，指的是卷积过程中的填充方法，对应的是'SAME'填充和'VALID'填充。'SAME'填充通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致；'VALID'填充的方式则相反，实际并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。此外，由于前一种方式通过补零来进行完整的卷积操作，可以有效地保留原始的输入特征信息。</p>
<p>​ 比如下图左部分为窄卷积。注意到越在边缘的位置被卷积的次数越少。宽卷积可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，如下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一一部分0值，使得输出和输入的维度一致。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN15.png" /></p>
<hr />
<h1 id="理解转置卷积与棋盘效应">14 理解转置卷积与棋盘效应</h1>
<h2 id="标准卷积">14.1 标准卷积</h2>
<p>在理解转置卷积之前，需要先理解标准卷积的运算方式。</p>
<p>首先给出一个输入输出结果</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN16.png" /></p>
<p>那是怎样计算的呢？</p>
<p>卷积的时候需要对卷积核进行180的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值，计算例子如下：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN17.png" /></p>
<p>这样计算出左上角(即第一行第一列)像素的卷积后像素值。</p>
<p>给出一个更直观的例子，从左到右看，原像素经过卷积由1变成-8。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN18.png" /></p>
<p>通过滑动卷积核，就可以得到整张图片的卷积结果。</p>
<h2 id="转置卷积">14.2 转置卷积</h2>
<p>图像的deconvolution过程如下：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN19.png" /></p>
<p>输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7</p>
<p>过程如下：</p>
<ol type="1">
<li><p>输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图</p></li>
<li><p>将4个特征图进行步长为3的相加； 输出的位置和输入的位置相同。步长为3是指每隔3个像素进行相加，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。</p>
<p>可以看出翻卷积的大小是由卷积核大小与滑动步长决定， in是输入大小， k是卷积核大小， s是滑动步长， out是输出大小 得到 out = (in - 1) * s + k 上图过程就是， (2 - 1) * 3 + 4 = 7。</p></li>
</ol>
<hr />
<h1 id="卷积神经网络的参数设置">15 卷积神经网络的参数设置</h1>
<p>卷积神经网络中常见的参数在其他类型的神经网络中也是类似的，但是参数的设置还得结合具体的任务才能设置在合理的范围，具体的参数列表如表XX所示。 ​ 表XX 卷积神经网络常见参数</p>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 9%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">参数名</th>
<th style="text-align: center;">常见设置</th>
<th style="text-align: left;">参数说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">学习率(Learning Rate)</td>
<td style="text-align: center;"><span class="math inline">\(0-1\)</span></td>
<td style="text-align: left;">反向传播网络中更新权值矩阵的步长，在一些常见的网络中会在固定迭代次数或模型不再收敛后对学习率进行指数下降(如<span class="math inline">\(lr=lr\times 0.1\)</span>)。当学习率越大计算误差对权值矩阵的影响越大，容易在某个局部最优解附近震荡；越小的学习率对网络权值的更新越精细，但是需要花费更多的时间去迭代</td>
</tr>
<tr class="even">
<td style="text-align: center;">批次大小(Batch Size)</td>
<td style="text-align: center;"><span class="math inline">\(1-N\)</span></td>
<td style="text-align: left;">批次大小指定一次性流入模型的数据样本个数，根据任务和计算性能限制判断实际取值，在一些图像任务中往往由于计算性能和存储容量限制只能选取较小的值。在相同迭代次数的前提下，数值越大模型越稳定，泛化能力越强，损失值曲线越平滑，模型也更快地收敛，但是每次迭代需要花费更多的时间</td>
</tr>
<tr class="odd">
<td style="text-align: center;">数据轮次(Epoch)</td>
<td style="text-align: center;"><span class="math inline">\(1-N\)</span></td>
<td style="text-align: left;">数据轮次指定所有训练数据在模型中训练的次数，根据数据集规模和分布情况会设置为不同的值。当模型较为简单或训练数据规模较小时，通常轮次不宜过高，否则模型容易过拟合；模型较为复杂或训练数据规模足够大时，可适当提高数据的训练轮次。</td>
</tr>
<tr class="even">
<td style="text-align: center;">权重衰减系数(Weight Decay)</td>
<td style="text-align: center;"><span class="math inline">\(0-0.001\)</span></td>
<td style="text-align: left;">模型训练过程中反向传播权值更新的权重衰减值</td>
</tr>
</tbody>
</table>
<hr />
<h1 id="提高卷积神经网络的泛化能力">16 提高卷积神经网络的泛化能力</h1>
<p>卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，表XX记录了提高卷积网络泛化能力的方法。 ​ 表XX 提高卷积网络化能力的方法</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">方法</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">使用更多数据</td>
<td style="text-align: left;">在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力</td>
</tr>
<tr class="even">
<td style="text-align: center;">使用更大批次</td>
<td style="text-align: left;">在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定</td>
</tr>
<tr class="odd">
<td style="text-align: center;">调整数据分布</td>
<td style="text-align: left;">大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力</td>
</tr>
<tr class="even">
<td style="text-align: center;">调整目标函数</td>
<td style="text-align: left;">在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数<span class="math inline">\(f(y,y&#39;)=|y-y&#39;|\)</span>在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成<span class="math inline">\(f(y,y&#39;)=(y-y&#39;)^2\)</span>则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力</td>
</tr>
<tr class="odd">
<td style="text-align: center;">调整网络结构</td>
<td style="text-align: left;">在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用</td>
</tr>
<tr class="even">
<td style="text-align: center;">数据增强</td>
<td style="text-align: left;">数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。</td>
</tr>
<tr class="odd">
<td style="text-align: center;">权值正则化</td>
<td style="text-align: left;">权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如<span class="math inline">\(Loss=f(WX+b,y&#39;)+\frac{\lambda}{\eta}\sum{|W|}\)</span>)。</td>
</tr>
<tr class="even">
<td style="text-align: center;">屏蔽网络节点</td>
<td style="text-align: left;">该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>对大多数神经网络模型同样通用</p>
</blockquote>
<hr />
<h1 id="卷积神经网络在不同领域的应用">17 卷积神经网络在不同领域的应用</h1>
<p>卷积神经网络中的卷积操作是其关键组成，而卷积操作只是一种数学运算方式，实际上对不同类型的数值表示数据都是通用的，尽管这些数值可能表示的是图像像素值、文本序列中单个字符或是语音片段中单字的音频。只要使原始数据能够得到有效地数值化表示，卷积神经网络能够在不同的领域中得到应用，要关注的是如何将卷积的特性更好地在不同领域中应用.</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN20.png" /></p>
<h2 id="联系">17.1 联系</h2>
<p>​ 自然语言处理是对一维信号（词序列）做操作。 ​ 计算机视觉是对二维（图像）或三维（视频流）信号做操作。</p>
<h2 id="区别">17.2 区别</h2>
<p>​ 自然语言处理的输入数据通常是离散取值（例如表示一个单词或字母通常表示为词典中的one hot向量），计算机视觉则是连续取值（比如归一化到0，1之间的灰度值）。CNN有两个主要特点，区域不变性(location invariance)和组合性(Compositionality)。</p>
<ol type="1">
<li>区域不变性：滤波器在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。</li>
<li>局部组合性：CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在语言里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。</li>
</ol>
<hr />
<h1 id="卷积神经网络凸显共性的方法">18 卷积神经网络凸显共性的方法？</h1>
<h2 id="局部连接">18.1 局部连接</h2>
<p>我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。 在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征； 下图是一个很经典的图示，左边是全连接，右边是局部连接。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN21.png" /></p>
<p>对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。</p>
<h2 id="权值共享">18.2 权值共享</h2>
<p>权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。 需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。 权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN22.png" /></p>
<p>这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。</p>
<h2 id="池化操作">18.3 池化操作</h2>
<p>池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN23.png" /></p>
<hr />
<h1 id="全连接局部连接全卷积与局部卷积">19 全连接、局部连接、全卷积与局部卷积</h1>
<p>大多数神经网络中高层网络通常会采用全连接层(Global Connected Layer)，通过多对多的连接方式对特征进行全局汇总，可以有效地提取全局信息。但是全连接的方式需要大量的参数，是神经网络中最占资源的部分之一，因此就需要由局部连接(Local Connected Layer)，仅在局部区域范围内产生神经元连接，能够有效地减少参数量。根据卷积操作的作用范围可以分为全卷积(Global Convolution)和局部卷积(Local Convolution)。实际上这里所说的全卷积就是标准卷积，即在整个输入特征维度范围内采用相同的卷积核参数进行运算，全局共享参数的连接方式可以使神经元之间的连接参数大大减少;局部卷积又叫平铺卷积(Tiled Convolution)或非共享卷积(Unshared Convolution)，是局部连接与全卷积的折中。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN24.png" /></p>
<hr />
<h1 id="局部卷积的应用">20 局部卷积的应用</h1>
<p>并不是所有的卷积都会进行权重共享，在某些特定任务中，会使用不权重共享的卷积。下面通过人脸这一任务来进行讲解。在读人脸方向的一些paper时，会发现很多都会在最后加入一个Local Connected Conv，也就是不进行权重共享的卷积层。总的来说，这一步的作用就是使用3D模型来将人脸对齐，从而使CNN发挥最大的效果。</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN25.png" /></p>
<p>截取论文中的一部分图，经过3D对齐以后，形成的图像均是152×152，输入到上述的网络结构中。该结构的参数如下：</p>
<p>Conv：32个11×11×3的卷积核，</p>
<p>Max-pooling: 3×3，stride=2，</p>
<p>Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个7×7的卷积核，</p>
<p>Local-Conv: 16个5×5的卷积核，</p>
<p>Fully-connected: 4096维，</p>
<p>Softmax: 4030维。</p>
<p>前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中Max-pooling层使得卷积的输出对微小的偏移情况更加鲁棒。但不能使用更多的Max-pooling层，因为太多的Max-pooling层会使得网络损失图像信息。全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置特征之间的相关性。最后使用softmax层用于人脸分类。 中间三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因：</p>
<p>（1）对齐的人脸图片中，不同的区域会有不同的统计特征，因此并不存在特征的局部稳定性，所以使用相同的卷积核会导致信息的丢失。</p>
<p>（2）不共享的卷积核并不增加inference时特征的计算量，仅会增加训练时的计算量。 使用不共享的卷积核，由于需要训练的参数量大大增加，因此往往需要通过其他方法增加数据量。</p>
<hr />
<h1 id="netvlad池化">21 NetVLAD池化</h1>
<p>NetVLAD是论文[15]提出的一个局部特征聚合的方法。</p>
<p>在传统的网络里面，例如VGG啊，最后一层卷积层输出的特征都是类似于Batchsize x 3 x 3 x 512的这种东西，然后会经过FC聚合，或者进行一个Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行Softmax or 其他的Loss。</p>
<p>这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。</p>
<p>那么NetVLAD考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的W x H x D，设计一个新的池化，去聚合一个“局部特征“，这即是NetVLAD的作用。</p>
<p>NetVLAD的一个输入是一个W x H x D的图像特征，例如VGG-Net最后的3 x 3 x 512这样的矩阵，在网络中还需加一个维度为Batchsize。</p>
<p>NetVLAD还需要另输入一个标量K即表示VLAD的聚类中心数量，它主要是来构成一个矩阵C，是通过原数据算出来的每一个<span class="math inline">\(W \times H\)</span>特征的聚类中心，C的shape即<span class="math inline">\(C: K \times D\)</span>，然后根据三个输入，VLAD是计算下式的V:</p>
<p><span class="math display">\[V(j, k) = \sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}\]</span></p>
<p>其中j表示维度，从1到D，可以看到V的j是和输入与c对应的，对每个类别k，都对所有的x进行了计算，如果<span class="math inline">\(x_i\)</span>属于当前类别k，<span class="math inline">\(a_k=1\)</span>，否则<span class="math inline">\(a_k=0\)</span>，计算每一个x和它聚类中心的残差，然后把残差加起来，即是每个类别k的结果，最后分别L2正则后拉成一个长向量后再做L2正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。</p>
<p>输入与输出如下图所示：</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN27.png" /></p>
<p>中间得到的K个D维向量即是对D个x都进行了与聚类中心计算残差和的过程，最终把K个D维向量合起来后进行即得到最终输出的<span class="math inline">\(K \times D\)</span>长度的一维向量。</p>
<p>而VLAD本身是不可微的，因为上面的a要么是0要么是1，表示要么当前描述x是当前聚类，要么不是，是个离散的，NetVLAD为了能够在深度卷积网络里使用反向传播进行训练，对a进行了修正。</p>
<p>那么问题就是如何重构一个a，使其能够评估当前的这个x和各个聚类的关联程度？用softmax来得到：</p>
<p><span class="math display">\[a_k = \frac{e^{W_k^T x_i + b_k}}{e^{W_{k&#39;}^T x_i + b_{k&#39;}}}\]</span></p>
<p>将这个把上面的a替换后，即是NetVLAD的公式，可以进行反向传播更新参数。</p>
<p>所以一共有三个可训练参数，上式a中的<span class="math inline">\(W: K \times D\)</span>，上式a中的<span class="math inline">\(b: K \times 1\)</span>，聚类中心<span class="math inline">\(c: K \times D\)</span>，而原始VLAD只有一个参数c。</p>
<p>最终池化得到的输出是一个恒定的K x D的一维向量（经过了L2正则），如果带Batchsize，输出即为Batchsize x (K x D)的二维矩阵。</p>
<p>NetVLAD作为池化层嵌入CNN网络即如下图所示，</p>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211214CNN28.png" /></p>
<p>原论文中采用将传统图像检索方法VLAD进行改进后应用在CNN的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。</p>
<p>后续相继又提出了ActionVLAD、ghostVLAD等改进。</p>
<hr />
<h1 id="参考文献">参考文献</h1>
<p>[0] github.com/scutan90/DeepLearning-500-questions</p>
<p>[1] 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6):1229-1251.</p>
<p>[2] 常亮, 邓小明, 周明全,等. 图像理解中的卷积神经网络[J]. 自动化学报, 2016, 42(9):1300-1312.</p>
<p>[3] Chua L O. CNN: A Paradigm for Complexity[M]// CNN a paradigm for complexity /. 1998.</p>
<p>[4] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, PP(99):1-1.</p>
<p>[5] Hoochang S, Roth H R, Gao M, et al. Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning[J]. IEEE Transactions on Medical Imaging, 2016, 35(5):1285-1298.</p>
<p>[6] 许可. 卷积神经网络在图像识别上的应用的研究[D]. 浙江大学, 2012.</p>
<p>[7] 陈先昌. 基于卷积神经网络的深度学习算法与应用研究[D]. 浙江工商大学, 2014.</p>
<p>[8] <a href="http://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual Recognition, Stanford</a></p>
<p>[9] <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.2gfx5zcw3">Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks</a></p>
<p>[10] cs231n 动态卷积图：<a href="http://cs231n.github.io/assets/conv-demo/index.html" class="uri">http://cs231n.github.io/assets/conv-demo/index.html</a></p>
<p>[11] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</p>
<p>[12] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898.</p>
<p>[13] 魏秀参.解析深度学习——卷积神经网络原理与视觉实践[M].电子工业出版社, 2018</p>
<p>[14] Jianxin W U , Gao B B , Wei X S , et al. Resource-constrained deep learning: challenges and practices[J]. Scientia Sinica(Informationis), 2018.</p>
<p>[15] Arandjelovic R , Gronat P , Torii A , et al. [IEEE 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - Las Vegas, NV, USA (2016.6.27-2016.6.30)] 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - NetVLAD: CNN Architecture for Weakly Supervised Place Recognition[C]// 2016:5297-5307.</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>DL基础</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习基础</title>
    <url>/2021/11/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="第三章-深度学习基础">第三章 深度学习基础</h1>
<h2 id="基本概念">3.1 基本概念</h2>
<h3 id="神经网络组成">3.1.1 神经网络组成？</h3>
<p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。</p>
<p><strong>感知机</strong></p>
<p>多层感知机中的特征神经元模型称为感知机，由<em>Frank Rosenblatt</em>于1957年发明。</p>
<p>简单的感知机如下图所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119DL1.png" alt="image-20211119221055777" /><figcaption>image-20211119221055777</figcaption>
</figure>
<p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为$ 4 $分、<span class="math inline">\(1\)</span> 分、$-3 <span class="math inline">\(分，这\)</span> 3$ 位评分的权重分别是 <span class="math inline">\(1、3、2\)</span>，则该歌手最终得分为 <span class="math inline">\(4 \times 1 + 1 \times 3 + (-3) \times 2 = 1\)</span> 。按照比赛规则，选取的 <span class="math inline">\(threshold\)</span> 为 <span class="math inline">\(3\)</span>，说明只有歌手的综合评分大于$ 3$ 时，才可顺利晋级。</p>
<p><strong>多层感知机</strong></p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119DL2.png" alt="3.1.1.5" /><figcaption>3.1.1.5</figcaption>
</figure>
<p>输出层可以不止有$ 1$ 个神经元。隐藏层可以只有$ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221411DL.png" alt="image-20211119221411589" /><figcaption>image-20211119221411589</figcaption>
</figure>
<h3 id="神经网络有哪些常用模型结构">3.1.2 神经网络有哪些常用模型结构？</h3>
<p>下图包含了大部分常用的模型：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221448DL.jpg" alt="3-7" /><figcaption>3-7</figcaption>
</figure>
<h3 id="如何选择深度学习开发平台">3.1.3 如何选择深度学习开发平台？</h3>
<p>​ 现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。</p>
<p><strong>参考1：与现有编程平台、技能整合的难易程度</strong></p>
<p>​ 主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。</p>
<p><strong>参考2: 与相关机器学习、数据处理生态整合的紧密程度</strong></p>
<p>​ 深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。</p>
<p><strong>参考3：对数据量及硬件的要求和支持</strong></p>
<p>​ 深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。</p>
<p><strong>参考4：深度学习平台的成熟程度</strong></p>
<p>​ 成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。</p>
<p><strong>参考5：平台利用是否多样性？</strong></p>
<p>​ 有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。</p>
<h3 id="为什么使用深层表示">3.1.4 为什么使用深层表示?</h3>
<ol type="1">
<li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li>
</ol>
<h3 id="为什么深层神经网络难以训练">3.1.5 为什么深层神经网络难以训练？</h3>
<ol type="1">
<li><p>梯度消失 梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。</p>
<p>​ 梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221616DL.png" alt="image-20211119221616944" /><figcaption>image-20211119221616944</figcaption>
</figure></li>
<li><p>梯度爆炸 在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为<span class="math inline">\(NaN\)</span>值，再也无法更新。</p></li>
<li><p>权重矩阵的退化导致模型的有效自由度减少。</p>
<p>​ 参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221642DL.png" alt="image-20211119221642527" /><figcaption>image-20211119221642527</figcaption>
</figure></li>
</ol>
<p>随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p>
<h3 id="深度学习和机器学习有什么不同">3.1.6 深度学习和机器学习有什么不同？</h3>
<p>​ <strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p>
<p>​ <strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p>
<p>​ 传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p>
<h2 id="网络操作与计算">3.2 网络操作与计算</h2>
<h3 id="前向传播与反向传播">3.2.1 前向传播与反向传播？</h3>
<p>神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。</p>
<p><strong>前向传播</strong></p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221742DL.png" alt="image-20211119221742601" /><figcaption>image-20211119221742601</figcaption>
</figure>
<p>假设上一层结点 $ i,j,k,... $ 等一些结点与本层的结点 $ w $ 有连接，那么结点 $ w $ 的值怎么算呢？就是通过上一层的 $ i,j,k,... $ 等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如 <span class="math inline">\(ReLu\)</span>，<span class="math inline">\(sigmoid\)</span> 等函数，最后得到的结果就是本层结点 $ w $ 的输出。</p>
<p>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
<p><strong>反向传播</strong></p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221809DL.png" alt="image-20211119221809673" /><figcaption>image-20211119221809673</figcaption>
</figure>
<p>由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下：</p>
<p>设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$  $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $  =   $。同理，下一层也是这么计算，只不过 $  $ 计算方法变了，一直反向传播到输入层，最后有 $  =   $，且 $  = w_i j $。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。</p>
<h3 id="如何计算神经网络的输出">3.2.2 如何计算神经网络的输出？</h3>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221858DL.png" alt="image-20211119221858020" /><figcaption>image-20211119221858020</figcaption>
</figure>
<p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w_{41}, w_{42}, w_{43} $。</p>
<p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。</p>
<p><span class="math display">\[
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})
\]</span></p>
<p>其中 $ w_{4b} $ 是节点 4 的偏置项。</p>
<p>同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。</p>
<p>计算输出层的节点 8 的输出值 $ y_1 $：</p>
<p><span class="math display">\[
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})
\]</span></p>
<p>其中 $ w_{8b} $ 是节点 8 的偏置项。</p>
<p>同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h3 id="如何计算卷积神经网络输出值">3.2.3 如何计算卷积神经网络输出值？</h3>
<p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119221934DL.png" alt="image-20211119221934319" /><figcaption>image-20211119221934319</figcaption>
</figure>
<p>$ x_{i,j} $ 表示图像第 $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 <span class="math inline">\(filter\)</span> 的偏置项。 表<span class="math inline">\(a_i,_j\)</span>示 feature map 第 $ i$ 行第 $ j $ 列元素。 <span class="math inline">\(f\)</span> 表示激活函数，这里以$ ReLU$ 函数为例。卷积计算公式如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222016DL.png" alt="image-20211119222016911" /><figcaption>image-20211119222016911</figcaption>
</figure>
<p>当步长为 <span class="math inline">\(1\)</span> 时，计算 feature map 元素 $ a_{0,0} $ 如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222035DL.png" alt="image-20211119222035758" /><figcaption>image-20211119222035758</figcaption>
</figure>
<p>其计算过程图示如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222056DL.png" alt="image-20211119222056016" /><figcaption>image-20211119222056016</figcaption>
</figure>
<p>以此类推，计算出全部的Feature Map。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222121DL.png" alt="image-20211119222121913" /><figcaption>image-20211119222121913</figcaption>
</figure>
<p>当步幅为 2 时，Feature Map计算如下</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222146DL.png" alt="image-20211119222146904" /><figcaption>image-20211119222146904</figcaption>
</figure>
<p>注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：</p>
<p><span class="math inline">\(W_2 = (W_1 - F + 2P)/S + 1\\ H_2 = (H_1 - F + 2P)/S + 1\)</span></p>
<p>​ 其中 $ W_2 <span class="math inline">\(， 是卷积后 Feature Map 的宽度；\)</span> W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 <span class="math inline">\(0\)</span>，如果 <span class="math inline">\(P\)</span> 的值是 <span class="math inline">\(1\)</span>，那么就补 <span class="math inline">\(1\)</span> 圈 <span class="math inline">\(0\)</span>；<span class="math inline">\(S\)</span> 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。</p>
<p>​ 举例：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 <span class="math inline">\(，\)</span> Z $ 则</p>
<p><span class="math inline">\(a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)\)</span></p>
<p>其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。</p>
<p>​ 每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。<span class="math inline">\(7*7*3\)</span> 输入，经过两个 <span class="math inline">\(3*3*3\)</span> filter 的卷积(步幅为 <span class="math inline">\(2\)</span>)，得到了 <span class="math inline">\(3*3*2\)</span> 的输出。图中的 Zero padding 是 <span class="math inline">\(1\)</span>，也就是在输入元素的周围补了一圈 <span class="math inline">\(0\)</span>。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222327DL.png" alt="image-20211119222327625" /><figcaption>image-20211119222327625</figcaption>
</figure>
<p>以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 * 3 * 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 * 3 * 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h3 id="如何计算-pooling-层输出值输出值">3.2.4 如何计算 Pooling 层输出值输出值？</h3>
<p>​ Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222350DL.png" alt="image-20211119222350801" /><figcaption>image-20211119222350801</figcaption>
</figure>
<p>除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。 ​ 对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。</p>
<h3 id="实例理解反向传播">3.2.5 实例理解反向传播</h3>
<p>​ 一个典型的三层神经网络如下所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222412DL.png" alt="image-20211119222412418" /><figcaption>image-20211119222412418</figcaption>
</figure>
<p>其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。</p>
<p>​ 假设输入数据集为 $ D={x_1, x_2, ..., x_n} $，输出数据集为 $ y_1, y_2, ..., y_n $。</p>
<p>​ 如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。</p>
<p>假设有如下的网络层：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222432DL.png" alt="image-20211119222432634" /><figcaption>image-20211119222432634</figcaption>
</figure>
<p>输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为 $ o_1, o_2 <span class="math inline">\(，\)</span> w_i $ 为层与层之间连接的权重，激活函数为 <span class="math inline">\(sigmoid\)</span> 函数。对以上参数取初始值，如下图所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222451DL.png" alt="image-20211119222451562" /><figcaption>image-20211119222451562</figcaption>
</figure>
<p>其中：</p>
<ul>
<li>输入数据 $ i1=0.05, i2 = 0.10 $</li>
<li>输出数据 $ o1=0.01, o2=0.99 $;</li>
<li>初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $</li>
<li>目标：给出输入数据 $ i1,i2 $ ( <span class="math inline">\(0.05\)</span>和<span class="math inline">\(0.10\)</span> )，使输出尽可能与原始输出 $ o1,o2 $，( <span class="math inline">\(0.01\)</span>和<span class="math inline">\(0.99\)</span>)接近。</li>
</ul>
<p><strong>前向传播</strong></p>
<ol type="1">
<li>输入层 --&gt; 输出层</li>
</ol>
<p>计算神经元 $ h1 $ 的输入加权和：</p>
<p><span class="math inline">\(net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\\net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775\)</span></p>
<p>神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）：</p>
<p><span class="math inline">\(out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992\)</span></p>
<p>同理，可计算出神经元 $ h2 $ 的输出 $ o1 $：</p>
<p><span class="math inline">\(out_{h2} = 0.596884378\)</span></p>
<ol start="2" type="1">
<li>隐含层--&gt;输出层： 　　</li>
</ol>
<p>计算输出层神经元 $ o1 $ 和 $ o2 $ 的值：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222642DL.png" alt="image-20211119222642435" /><figcaption>image-20211119222642435</figcaption>
</figure>
<p>这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 , 0.772928465] $，与实际值 $ [0.01 , 0.99] $ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p><strong>反向传播 </strong></p>
<p>​ 1.计算总误差</p>
<p>总误差：(这里使用Square Error)</p>
<p><span class="math inline">\(E_{total} = \sum \frac{1}{2}(target - output)^2\)</span></p>
<p>但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和：</p>
<p><span class="math inline">\(E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2 = \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083\)</span>.</p>
<p><span class="math inline">\(E_{o2} = 0.023560026\)</span>.</p>
<p><span class="math inline">\(E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109\)</span>.</p>
<p>​ 2.隐含层 --&gt; 输出层的权值更新：</p>
<p>以权重参数 $ w5 $ 为例，如果我们想知道 $ w5 $ 对整体误差产生了多少影响，可以用整体误差对 $ w5 $ 求偏导求出：（链式法则）</p>
<p><span class="math inline">\(\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}\)</span></p>
<p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222731DL.png" alt="image-20211119222731869" /><figcaption>image-20211119222731869</figcaption>
</figure>
<h3 id="神经网络更深有什么意义">3.2.6 神经网络更“深”有什么意义？</h3>
<p>前提：在一定范围内。</p>
<ul>
<li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li>
<li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li>
</ul>
<h2 id="超参数">3.3 超参数</h2>
<h3 id="什么是超参数">3.3.1 什么是超参数？</h3>
<p>​ <strong>超参数</strong> : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>​ 超参数通常存在于：</p>
<pre><code>1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定</code></pre>
<p>​ 超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h3 id="如何寻找超参数的最优值">3.3.2 如何寻找超参数的最优值？</h3>
<p>​ 在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ol type="1">
<li>猜测和检查：根据经验或直觉，选择参数，一直迭代。</li>
<li>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</li>
<li>随机搜索：让计算机随机挑选一组值。</li>
<li>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</li>
<li>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</li>
<li>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</li>
</ol>
<h3 id="超参数搜索一般过程">3.3.3 超参数搜索一般过程？</h3>
<p>超参数搜索一般过程：</p>
<ol type="1">
<li>将数据集划分成训练集、验证集及测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li>
<li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li>
</ol>
<p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p>
<h2 id="激活函数">3.4 激活函数</h2>
<h3 id="为什么需要非线性激活函数">3.4.1 为什么需要非线性激活函数？</h3>
<p><strong>为什么需要激活函数？</strong></p>
<ol type="1">
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<p><strong>为什么激活函数需要非线性函数？</strong></p>
<ol type="1">
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h3 id="常见的激活函数及图像">3.4.2 常见的激活函数及图像</h3>
<ol type="1">
<li><p>sigmoid 激活函数</p>
<p>函数的定义为：$ f(x) =  $，其值域为 $ (0,1) $。</p>
<p>函数图像如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222832DL.png" alt="image-20211119222832128" /><figcaption>image-20211119222832128</figcaption>
</figure></li>
</ol>
<p>tanh激活函数</p>
<p>函数的定义为：$ f(x) = tanh(x) =  $，值域为 $ (-1,1) $。</p>
<p>函数图像如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222850DL.png" alt="image-20211119222850464" /><figcaption>image-20211119222850464</figcaption>
</figure>
<p>Relu激活函数</p>
<p>函数的定义为：$ f(x) = max(0, x) $ ，值域为 $ [0,+∞) $；</p>
<p>函数图像如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222908DL.png" alt="image-20211119222908298" /><figcaption>image-20211119222908298</figcaption>
</figure>
<p>Leak Relu 激活函数</p>
函数定义为： $ f(x) = {
<span class="math display">\[\begin{aligned}
ax, \quad x&lt;0 \\
x, \quad x&gt;0
\end{aligned}\]</span>
<p>. $，值域为 $ (-∞,+∞) $。</p>
<p>图像如下（$ a = 0.5 $）：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222924DL.png" alt="image-20211119222924814" /><figcaption>image-20211119222924814</figcaption>
</figure>
<p>SoftPlus 激活函数</p>
<p>函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。</p>
<p>函数图像如下:</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119222939DL.png" alt="image-20211119222939320" /><figcaption>image-20211119222939320</figcaption>
</figure>
<p>softmax 函数</p>
<p>函数定义为： $ (z)_j =  $。</p>
<p>Softmax 多用于多分类神经网络输出。</p>
<h3 id="常见激活函数的导数计算">3.4.3 常见激活函数的导数计算？</h3>
<p>对常见激活函数，导数计算如下：</p>
<table style="width:100%;">
<colgroup>
<col style="width: 8%" />
<col style="width: 24%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>原函数</th>
<th>函数表达式</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sigmoid激活函数</td>
<td><span class="math inline">\(f(x)=\frac{1}{1+e^{-x}}\)</span></td>
<td><span class="math inline">\(f^{&#39;}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))\)</span></td>
<td>当<span class="math inline">\(x=10\)</span>,或<span class="math inline">\(x=-10\)</span>，<span class="math inline">\(f^{&#39;}(x) \approx0\)</span>,当<span class="math inline">\(x=0\)</span><span class="math inline">\(f^{&#39;}(x) =0.25\)</span></td>
</tr>
<tr class="even">
<td>Tanh激活函数</td>
<td><span class="math inline">\(f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span></td>
<td><span class="math inline">\(f^{&#39;}(x)=-(tanh(x))^2\)</span></td>
<td>当<span class="math inline">\(x=10\)</span>,或<span class="math inline">\(x=-10\)</span>，<span class="math inline">\(f^{&#39;}(x) \approx0\)</span>,当<span class="math inline">\(x=0\)</span><span class="math inline">\(f^{`}(x) =1\)</span></td>
</tr>
<tr class="odd">
<td>Relu激活函数</td>
<td><span class="math inline">\(f(x)=max(0,x)\)</span></td>
<td><span class="math inline">\(c(u)=\begin{cases} 0,x&lt;0 \\ 1,x&gt;0 \\ undefined,x=0\end{cases}\)</span></td>
<td>通常<span class="math inline">\(x=0\)</span>时，给定其导数为1和0</td>
</tr>
</tbody>
</table>
<h3 id="激活函数有哪些性质">3.4.4 激活函数有哪些性质？</h3>
<ol type="1">
<li>非线性： 当激活函数是非线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>$ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的 Learning Rate。</li>
</ol>
<h3 id="如何选择激活函数">3.4.5 如何选择激活函数？</h3>
<p>​ 选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>以下是常见的选择情况：</p>
<ol type="1">
<li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li>
</ol>
<h3 id="使用-relu-激活函数的优点">3.4.6 使用 ReLu 激活函数的优点？</h3>
<ol type="1">
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h3 id="什么时候可以用线性激活函数">3.4.7 什么时候可以用线性激活函数？</h3>
<ol type="1">
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h3 id="怎样理解-relu-0-时是非线性激活函数">3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？</h3>
<p>根据图像可看出具有如下特点：</p>
<ol type="1">
<li><p>单侧抑制；</p></li>
<li><p>相对宽阔的兴奋边界；</p></li>
<li><p>稀疏激活性；</p>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x&lt;0 $ 时，ReLU 硬饱和，而当 $ x&gt;0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。</p>
<h3 id="softmax-定义及作用">3.4.9 Softmax 定义及作用</h3>
<p>Softmax 是一种形如下式的函数：</p>
<p><span class="math inline">\(P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}\)</span></p>
<p>​ 其中，$ _i $ 和 $ x $ 是列向量，$ _i^T x $ 可能被换成函数关于 $ x $ 的函数 $ f_i(x) $</p>
<p>​ 通过 softmax 函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ _i $ 作为最佳参数。</p>
<p>​ 但是，使得范围在 $ [0,1] $ 之间的方法有很多，为啥要在前面加上以 $ e $ 的幂函数的形式呢？参考 logistic 函数：</p>
<p><span class="math inline">\(P(i) = \frac{1}{1+exp(-\theta_i^T x)}\)</span></p>
<p>​ 这个函数的作用就是使得 $ P(i) $ 在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。</p>
<p>​ softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。</p></li>
</ol>
<h3 id="softmax-函数如何应用于多分类">3.4.10 Softmax 函数如何应用于多分类？</h3>
<p>​ softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！</p>
<p>​ 假设我们有一个数组，$ V_i $ 表示 $ V $ 中的第 $ i $ 个元素，那么这个元素的 softmax 值就是</p>
<p><span class="math display">\[
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}
\]</span></p>
<p>​ 从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119223352DL.png" alt="image-20211119223352094" /><figcaption>image-20211119223352094</figcaption>
</figure>
<p>继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] $，这就是 soft 的功能。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119223413DL.png" alt="image-20211119223413173" /><figcaption>image-20211119223413173</figcaption>
</figure>
<p>更形象的映射过程如下图所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119223431DL.png" alt="image-20211119223430968" /><figcaption>image-20211119223430968</figcaption>
</figure>
<p>softmax 直白来说就是将原来输出是 $ 3,1,-3 $ 通过 softmax 函数一作用，就映射成为 $ (0,1) $ 的值，而这些值的累和为 $ 1 $（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="交叉熵代价函数定义及其求导推导">3.4.11 交叉熵代价函数定义及其求导推导</h3>
<p>神经元的输出就是 a = σ(z)，其中<span class="math inline">\(z=\sum w_{j}i_{j}+b\)</span>是输⼊的带权和。</p>
<p><span class="math inline">\(C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]\)</span></p>
<p>​ 其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p>
<p>​ 表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。</p>
<p>​ 将交叉熵看做是代价函数有两点原因。</p>
<p>​ 第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。</p>
<p>​ 第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。</p>
<p>​ 假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的）</p>
<p>​ 综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将<span class="math inline">\(a={\varsigma}(z)\)</span>代⼊到 公式中应⽤两次链式法则，得到：</p>
<p><span class="math inline">\(\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}&#39;(z)x_{j}\end{eqnarray}\)</span></p>
<p>​ 根据<span class="math inline">\(\varsigma(z)=\frac{1}{1+e^{-z}}\)</span> 的定义，和⼀些运算，我们可以得到 <span class="math inline">\({\varsigma}&#39;(z)=\varsigma(z)(1-\varsigma(z))\)</span>。化简后可得：</p>
<p><span class="math inline">\(\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)\)</span></p>
<p>​ 这是⼀个优美的公式。它告诉我们权重学习的速度受到<span class="math inline">\(\varsigma(z)-y\)</span>，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中<span class="math inline">\({\varsigma}&#39;(z)\)</span>导致的学习缓慢。当我们使⽤交叉熵的时候，<span class="math inline">\({\varsigma}&#39;(z)\)</span>被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。</p>
<p>​ 根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：</p>
<p><span class="math inline">\(\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)\)</span></p>
<p>​ 再⼀次, 这避免了⼆次代价函数中类似<span class="math inline">\({\varsigma}&#39;(z)\)</span>项导致的学习缓慢。</p>
<h3 id="为什么tanh收敛速度比sigmoid快">3.4.12 为什么Tanh收敛速度比Sigmoid快？</h3>
<p>首先看如下两个函数的求导：</p>
<p><span class="math inline">\(tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)\)</span></p>
<p><span class="math inline">\(s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]\)</span></p>
<p>由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<p>注：梯度消失（gradient vanishing）或者爆炸（gradient explosion）是激活函数<strong>以及当前权重</strong>耦合产生的综合结果： 设任意激活函数为<span class="math inline">\(\sigma(\cdot)\)</span>，k+1层网络输出为<span class="math inline">\(f_{k+1}=\sigma(Wf_k)\)</span>，求导得到<span class="math inline">\(\frac {\partial h_{t+1}}{\partial h_t}=diag(\sigma&#39;(Wh_t))W\)</span>。可见求导结果同时会受到权重<span class="math inline">\(W\)</span>和激活函数的导数<span class="math inline">\(\sigma&#39;(\cdot)\)</span>的影响，以sigmoid函数<span class="math inline">\(\sigma(X)=\frac {1}{1+e^{-x}}\)</span>为例，其导数为<span class="math inline">\(\sigma&#39;(x)=\frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})\)</span>，其值恒大于零小于1，用链式法则求梯度回传时连续相乘使得结果趋于0，但是如果权重<span class="math inline">\(W\)</span>是较大的数值，使得<span class="math inline">\(\frac {\partial f_{t+1}}{\partial f_t}\)</span>相乘结果大于1，则梯度回传时连续相乘则不会发生梯度消失。 综上，在讨论激活函数收敛速度或与梯度消失或者爆炸相关时，应同时考虑当前权重<span class="math inline">\(W\)</span>数值的影响。</p>
<h3 id="内聚外斥---center-loss">3.4.12 内聚外斥 - Center Loss</h3>
<p>在计算机视觉任务中, 由于其简易性, 良好的表现, 与对分类任务的概率性理解, Cross Entropy Loss (交叉熵代价) + Softmax 组合被广泛应用于以分类任务为代表的任务中. 在此应用下, 我们可将其学习过程进一步理解为: 更相似(同类/同物体)的图像在特征域中拥有“更近的距离”, 相反则”距离更远“. 换而言之, 我们可以进一步理解为其学习了一种低类内距离(Intra-class Distance)与高类间距离(Inter-class Distance)的特征判别模型. 在此Center Loss则可以高效的计算出这种具判别性的特征. 不同于传统的Softmax Loss, Center Loss通过学习“特征中心”从而最小化其类内距离. 其表达形式如下:</p>
<p><span class="math inline">\(L_{C} = \frac{1}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}\)</span></p>
<p>其中<span class="math inline">\(x_{i}\)</span>表示FCN(全连接层)之前的特征, <span class="math inline">\(c_{y_{i}}\)</span>表示第$y_{i} $个类别的特征中心, <span class="math inline">\(m\)</span>表示mini-batch的大小. 我们很清楚的看到<span class="math inline">\(L_{C}\)</span>的终极目标为最小化每个特征与其特征中心的方差, 即最小化类内距离. 其迭代公式为:</p>
<p><span class="math inline">\(\frac{\partial L_{C}}{\partial x_{i}}=x_{i}-c_{y_{i}}\)</span></p>
<p><span class="math inline">\(\Delta{c_{j}} = \frac{\sum^{m}_{i=1}\delta(y_{i}=j)\cdot(c_{j}-x_{i})}{1+\sum^{m}_{i=1}\delta(y_{i}=j)}\)</span></p>
其中$ (condition)={
<span class="math display">\[\begin{array}{rcl}
1       &amp;      &amp; {condition\ is\ True}\\
0     &amp;      &amp; {otherwise}\\ \end{array}\]</span>
<p>.$</p>
<p>结合Softmax, 我们可以搭配二者使用, 适当平衡这两种监督信号. 在Softmax拉开类间距离的同时, 利用Center Loss最小化类内距离. 例如:</p>
<p><span class="math inline">\(\begin{eqnarray}L &amp; = &amp; L_{S} + \lambda L_{C} \\ &amp;=&amp; -\sum^{m}_{i=1}log\frac{e^{W_{y}^{T}x_{i}+b_{y_{i}}}}{\sum^{m}_{i=1}e^{W^{T}_{j}x_{i}+b_{j}}} + \frac{\lambda}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}\\ \end{eqnarray}\)</span></p>
<p>即便如此, Center Loss仍有它的不足之处: 其特征中心为存储在网络模型之外的额外参数, 不能与模型参数一同优化. 这些额外参数将与记录每一步特征变化的自动回归均值估计(autoregressive mean estimator)进行更迭. 当需要学习的类别数量较大时, mini-batch可能无力提供足够的样本进行均值估计. 若此Center Loss将需要平衡两种监督损失来以确定更迭, 其过程需要一个对平衡超参数的搜索过程, 使得其择值消耗昂贵.</p>
<h2 id="batch_size">3.5 Batch_Size</h2>
<h3 id="为什么需要-batch_size">3.5.1 为什么需要 Batch_Size？</h3>
<p>Batch的选择，首先决定的是下降的方向。</p>
<p>如果数据集比较小，可采用全数据集的形式，好处是：</p>
<ol type="1">
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。</li>
</ol>
<p>对于更大的数据集，假如采用全数据集的形式，坏处是：</p>
<ol type="1">
<li>随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。</li>
<li>以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。</li>
</ol>
<h3 id="batch_size-值的选择">3.5.2 Batch_Size 值的选择</h3>
<p>​ 假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p>
<p>​ 既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？</p>
<p>​ 此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。</p>
<h3 id="在合理范围内增大batch_size有何好处">3.5.3 在合理范围内，增大Batch_Size有何好处？</h3>
<ol type="1">
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
</ol>
<h3 id="盲目增大-batch_size-有何坏处">3.5.4 盲目增大 Batch_Size 有何坏处？</h3>
<ol type="1">
<li>内存利用率提高了，但是内存容量可能撑不住了。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h3 id="调节-batch_size-对训练效果影响到底如何">3.5.5 调节 Batch_Size 对训练效果影响到底如何？</h3>
<ol type="1">
<li>Batch_Size 太小，模型表现效果极其糟糕(error飙升)。</li>
<li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li>
<li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。</li>
</ol>
<h2 id="归一化">3.6 归一化</h2>
<h3 id="归一化含义">3.6.1 归一化含义？</h3>
<ol type="1">
<li><p>归纳统一样本的统计分布性。归一化在 $ 0-1$ 之间是统计的概率分布，归一化在$ -1--+1$ 之间是统计的坐标分布。</p></li>
<li><p>无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。</p></li>
<li><p>归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。</p></li>
<li><p>另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。</p></li>
</ol>
<h3 id="为什么要归一化">3.6.2 为什么要归一化？</h3>
<ol type="1">
<li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li>
<li>为了程序运行时收敛加快。</li>
<li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li>
<li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li>
<li>保证输出数据中数值小的不被吞食。</li>
</ol>
<h3 id="为什么归一化能提高求解最优解速度">3.6.3 为什么归一化能提高求解最优解速度？</h3>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119223638DL.png" alt="image-20211119223638602" /><figcaption>image-20211119223638602</figcaption>
</figure>
<p>上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。</p>
<p>​ 当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</p>
<p>​ 因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p>
<h3 id="d-图解未归一化">3.6.4 3D 图解未归一化</h3>
<p>例子：</p>
<p>​ 假设 $ w1 $ 的范围在 $ [-10, 10] $，而 $ w2 $ 的范围在 $ [-100, 100] $，梯度每次都前进 1 单位，那么在 $ w1 $ 方向上每次相当于前进了 $ 1/20 $，而在 $ w2 $ 上只相当于 $ 1/200 $！某种意义上来说，在 $ w2 $ 上前进的步长更小一些,而 $ w1 $ 在搜索过程中会比 $ w2 $ “走”得更快。</p>
<p>​ 这样会导致，在搜索过程中更偏向于 $ w1 $ 的方向。走出了“L”形状，或者成为“之”字形。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119223717DL.png" alt="image-20211119223717379" /><figcaption>image-20211119223717379</figcaption>
</figure>
<h3 id="归一化有哪些类型">3.6.5 归一化有哪些类型？</h3>
<ol type="1">
<li>线性归一化</li>
</ol>
<p><span class="math inline">\(x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}\)</span> 适用范围：比较适用在数值比较集中的情况。 缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。</p>
<ol start="2" type="1">
<li>标准差标准化</li>
</ol>
<p><span class="math inline">\(x^{\prime} = \frac{x-\mu}{\sigma}\)</span></p>
<p>含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中 $ $ 为所有样本数据的均值，$ $ 为所有样本数据的标准差。</p>
<ol start="3" type="1">
<li>非线性归一化</li>
</ol>
<p>适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。</p>
<h3 id="局部响应归一化作用">3.6.6 局部响应归一化作用</h3>
<p>​ LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。</p>
<p>​ 在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p>
<h3 id="理解局部响应归一化">3.6.7 理解局部响应归一化</h3>
<p>​ 局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下：</p>
<p><span class="math inline">\(b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta\)</span></p>
<p>其中， 1) $ a $：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。</p>
<ul>
<li>batch：批次数(每一批为一张图片)。</li>
<li>height：图片高度。</li>
<li>width：图片宽度。</li>
<li>channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。</li>
</ul>
<ol start="2" type="1">
<li><p>$ a_{x,y}^i $ 表示在这个输出结构中的一个位置 $ [a,b,c,d] $，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第 $ a $ 张图的第 $ d $ 个通道下的高度为b宽度为c的点。</p></li>
<li><p>$ N $：论文公式中的 $ N $ 表示通道数 (channel)。</p></li>
<li><p>$ a <span class="math inline">\(，\)</span> n/2 $， $ k $ 分别表示函数中的 input,depth_radius,bias。参数 $ k, n, , $ 都是超参数，一般设置 $ k=2, n=5, =1*e-4, =0.75 $</p></li>
<li><p>$ <span class="math inline">\(：\)</span> $ 叠加的方向是沿着通道方向的，即每个点值的平方和是沿着 $ a $ 中的第 3 维 channel 方向的，也就是一个点同方向的前面 $ n/2 $ 个通道（最小为第 $ 0 $ 个通道）和后 $ n/2 $ 个通道（最大为第 $ d-1 $ 个通道）的点的平方和(共 $ n+1 $ 个点)。而函数的英文注解中也说明了把 input 当成是 $ d $ 个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。</p></li>
</ol>
<p>简单的示意图如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119223925DL.png" alt="image-20211119223925026" /><figcaption>image-20211119223925026</figcaption>
</figure>
<h3 id="什么是批归一化batch-normalization">3.6.8 什么是批归一化（Batch Normalization）</h3>
<p>​ 以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ (WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。</p>
<p>​ 这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。</p>
<h3 id="批归一化bn算法的优点">3.6.9 批归一化（BN）算法的优点</h3>
<p>下面我们来说一下BN算法的优点：</p>
<ol type="1">
<li>减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数；</li>
<li>减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛；</li>
<li>可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在)</li>
<li>破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。</li>
<li>减少梯度消失，加快收敛速度，提高训练精度。</li>
</ol>
<h3 id="批归一化bn算法流程">3.6.10 批归一化（BN）算法流程</h3>
<p>下面给出 BN 算法在训练时的过程</p>
<p>输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ , $</p>
<p>算法流程：</p>
<ol type="1">
<li><p>计算上一层输出数据的均值</p>
<p><span class="math inline">\(\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)\)</span></p></li>
</ol>
<p>其中，$ m $ 是此次训练样本 batch 的大小。</p>
<ol start="2" type="1">
<li><p>计算上一层输出数据的标准差</p>
<p><span class="math inline">\(\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2\)</span></p></li>
<li><p>归一化处理，得到</p></li>
</ol>
<p><span class="math inline">\(\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}\)</span></p>
<p>其中 $ $ 是为了避免分母为 0 而加进去的接近于 0 的很小值</p>
<ol start="4" type="1">
<li>重构，对经过上面归一化处理得到的数据进行重构，得到</li>
</ol>
<p><span class="math inline">\(y_i = \gamma \hat x_i + \beta\)</span></p>
<p>其中，$ , $ 为可学习参数。</p>
<p>注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ <em>{} $ 和标准差 $ </em>{}^2 $。此时，均值 $ <em>{} $ 是计算所有 batch $ </em>{} $ 值的平均值得到，标准差 $ <em>{}^2 $ 采用每个batch $ </em>{}^2 $ 的无偏估计得到。</p>
<h3 id="批归一化和群组归一化比较">3.6.11 批归一化和群组归一化比较</h3>
<table>
<colgroup>
<col style="width: 43%" />
<col style="width: 56%" />
</colgroup>
<thead>
<tr class="header">
<th>名称</th>
<th style="text-align: left;">特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>批量归一化（Batch Normalization，以下简称 BN）</td>
<td style="text-align: left;">可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。</td>
</tr>
<tr class="even">
<td>群组归一化 Group Normalization (简称 GN)</td>
<td style="text-align: left;">GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。</td>
</tr>
<tr class="odd">
<td>比较</td>
<td style="text-align: left;">在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。</td>
</tr>
</tbody>
</table>
<h3 id="weight-normalization和batch-normalization比较">3.6.12 Weight Normalization和Batch Normalization比较</h3>
<p>​ Weight Normalization 和 Batch Normalization 都属于参数重写（Reparameterization）的方法，只是采用的方式不同。</p>
<p>​ Weight Normalization 是对网络权值$ W $ 进行 normalization，因此也称为 Weight Normalization；</p>
<p>​ Batch Normalization 是对网络某一层输入数据进行 normalization。</p>
<p>​ Weight Normalization相比Batch Normalization有以下三点优势：</p>
<ol type="1">
<li><p>Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（Batch Normalization 不能直接用于RNN，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) RNN 是基于 time step 计算，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。</p></li>
<li><p>Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。相反，Weight Normalization 对通过标量 $ g $ 和向量 $ v $ 对权重 $ W $ 进行重写，重写向量 $ v $ 是固定的，因此，基于 Weight Normalization 的 Normalization 可以看做比 Batch Normalization 引入更少的噪声。</p></li>
<li><p>不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 Weight Normalization 时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小。因此，要比采用 Batch Normalization 进行 normalization 操作时，速度快。 但是 Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用。因此，采用 Weight Normalization 进行 Normalization 时需要特别注意参数初始值的选择。</p></li>
</ol>
<h3 id="batch-normalization在什么时候用比较合适">3.6.13 Batch Normalization在什么时候用比较合适？</h3>
<p>在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>
<p>​ BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。</p>
<h2 id="预训练与微调fine-tuning">3.7 预训练与微调(fine tuning)</h2>
<h3 id="为什么无监督预训练可以帮助深度学习">3.7.1 为什么无监督预训练可以帮助深度学习？</h3>
<p>深度网络存在问题:</p>
<ol type="1">
<li><p>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</p></li>
<li><p>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</p></li>
<li><p>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</p></li>
</ol>
<p><strong>解决方法：</strong></p>
<p>​ 逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p>
<p>经过预训练最终能得到比较好的局部最优解。</p>
<h3 id="什么是模型微调fine-tuning">3.7.2 什么是模型微调fine tuning</h3>
<p>​ 用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning).</p>
<p><strong>模型的微调举例说明：</strong></p>
<p>​ 我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p>
<h3 id="微调时候网络参数是否更新">3.7.3 微调时候网络参数是否更新？</h3>
<p>答案：会更新。</p>
<ol type="1">
<li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。</li>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ol>
<h3 id="fine-tuning-模型的三种状态">3.7.4 fine-tuning 模型的三种状态</h3>
<ol type="1">
<li><p>状态一：只预测，不训练。 特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</p></li>
<li><p>状态二：训练，但只训练最后分类层。 特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</p></li>
<li><p>状态三：完全训练，分类层+之前卷积层都训练 特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</p></li>
</ol>
<h2 id="权重偏差初始化">3.8 权重偏差初始化</h2>
<h3 id="全都初始化为-0">3.8.1 全都初始化为 0</h3>
<p><strong>偏差初始化陷阱</strong>： 都初始化为 0。</p>
<p><strong>产生陷阱原因</strong>：因为并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为 0，如果神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。更一般地说，如果权重初始化为同一个值，网络就是对称的。</p>
<p><strong>形象化理解</strong>：在神经网络中考虑梯度下降的时候，设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。</p>
<h3 id="全都初始化为同样的值">3.8.2 全都初始化为同样的值</h3>
<p>​ 偏差初始化陷阱： 都初始化为一样的值。 ​ 以一个三层网络为例： 首先看下结构</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224231DL.png" alt="image-20211119224231715" /><figcaption>image-20211119224231715</figcaption>
</figure>
<p>它的表达式为：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224303DL.png" alt="image-20211119224303548" /><figcaption>image-20211119224303548</figcaption>
</figure>
<p>如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是$ a1=a2=a3=.... $，既然都一样，就相当于一个输入了，为啥呢？？</p>
<p>如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下</p>
<p><span class="math inline">\(\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b;x,y) = a_j^{(l)} \delta_i^{(l+1)}\frac{\partial}{\partial b_{i}^{(l)}} J(W,b;x,y) = \delta_i^{(l+1)}\)</span></p>
<p>$ $ 的计算公式</p>
<p><span class="math inline">\(\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})\)</span></p>
<p>如果用的是 sigmoid 函数</p>
<p><span class="math inline">\(f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})\)</span></p>
<p>把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同......，最后就得到了相同的值（权重和截距）。</p>
<h3 id="初始化为小的随机数">3.8.3 初始化为小的随机数</h3>
<p>​ 将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。一个权重矩阵的实现可能看起来像 $ W=0.01∗np.random.randn(D,H) $，其中 randn 是从均值为 0 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space). 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。</p>
<p>​ 备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。</p>
<h3 id="用-1n-校准方差">3.8.4 用 $ 1/n $ 校准方差</h3>
<p>​ 上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 1。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化: $ w=np.random.randn(n)/n $，其中 n 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。</p>
<h3 id="稀疏初始化sparse-initialazation">3.8.5 稀疏初始化(Sparse Initialazation)</h3>
<p>​ 另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。</p>
<h3 id="初始化偏差">3.8.6 初始化偏差</h3>
<p>​ 将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 0.</p>
<h2 id="学习率">3.9 学习率</h2>
<h3 id="学习率的作用">3.9.1 学习率的作用</h3>
<p>​ 在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。</p>
<p>​ 在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。 ​ 在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减.</p>
<h3 id="学习率衰减常用参数有哪些">3.9.2 学习率衰减常用参数有哪些</h3>
<table>
<thead>
<tr class="header">
<th>参数名称</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>learning_rate</td>
<td>初始学习率</td>
</tr>
<tr class="even">
<td>global_step</td>
<td>用于衰减计算的全局步数，非负，用于逐步计算衰减指数</td>
</tr>
<tr class="odd">
<td>decay_steps</td>
<td>衰减步数，必须是正值，决定衰减周期</td>
</tr>
<tr class="even">
<td>decay_rate</td>
<td>衰减率</td>
</tr>
<tr class="odd">
<td>end_learning_rate</td>
<td>最低的最终学习率</td>
</tr>
<tr class="even">
<td>cycle</td>
<td>学习率下降后是否重新上升</td>
</tr>
<tr class="odd">
<td>alpha</td>
<td>最小学习率</td>
</tr>
<tr class="even">
<td>num_periods</td>
<td>衰减余弦部分的周期数</td>
</tr>
<tr class="odd">
<td>initial_variance</td>
<td>噪声的初始方差</td>
</tr>
<tr class="even">
<td>variance_decay</td>
<td>衰减噪声的方差</td>
</tr>
</tbody>
</table>
<h3 id="分段常数衰减">3.9.3 分段常数衰减</h3>
<p>​ 分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224538DL.png" alt="image-20211119224538817" /><figcaption>image-20211119224538817</figcaption>
</figure>
<h3 id="指数衰减">3.9.4 指数衰减</h3>
<p>​ 以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：</p>
<p><span class="math inline">\(decayed{\_}learning{\_}rate =learning{\_}rate*decay{\_}rate^{\frac{global{\_step}}{decay{\_}steps}}\)</span></p>
<p>​ 这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随 训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224616DL.png" alt="image-20211119224615979" /><figcaption>image-20211119224615979</figcaption>
</figure>
<h3 id="自然指数衰减">3.9.5 自然指数衰减</h3>
<p>​ 它与指数衰减方式相似，不同的在于它的衰减底数是<span class="math inline">\(e\)</span>，故而其收敛的速度更快，一般用于相对比较 容易训练的网络，便于较快的收敛，其更新规则如下</p>
<p><span class="math inline">\(decayed{\_}learning{\_}rate =learning{\_}rate*e^{\frac{-decay{\_rate}}{global{\_}step}}\)</span></p>
<p>​ 下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224656DL.png" alt="image-20211119224656426" /><figcaption>image-20211119224656426</figcaption>
</figure>
<h3 id="多项式衰减">3.9.6 多项式衰减</h3>
<p>​ 应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照 给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224724DL.png" alt="image-20211119224723977" /><figcaption>image-20211119224723977</figcaption>
</figure>
<p>需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。</p>
<p><span class="math inline">\(decay{\_}steps = decay{\_}steps*ceil \left( \frac{global{\_}step}{decay{\_}steps}\right)\)</span></p>
<p>如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224754DL.png" alt="image-20211119224754190" /><figcaption>image-20211119224754190</figcaption>
</figure>
<h3 id="余弦衰减">3.9.7 余弦衰减</h3>
<p>​ 余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224824DL.png" alt="image-20211119224824885" /><figcaption>image-20211119224824885</figcaption>
</figure>
<p>如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224843DL.png" alt="image-20211119224843117" /><figcaption>image-20211119224843117</figcaption>
</figure>
<h2 id="dropout-系列问题">3.12 Dropout 系列问题</h2>
<h3 id="为什么要正则化">3.12.1 为什么要正则化？</h3>
<ol type="1">
<li>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。<br />
</li>
<li>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。</li>
</ol>
<h3 id="为什么正则化有利于预防过拟合">3.12.2 为什么正则化有利于预防过拟合？</h3>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20211119224905DL.png" alt="image-20211119224905408" /><figcaption>image-20211119224905408</figcaption>
</figure>
<p>左图是高偏差，右图是高方差，中间是Just Right，这几张图我们在前面课程中看到过。</p>
<h3 id="理解dropout正则化">3.12.3 理解dropout正则化</h3>
<p>​ Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？</p>
<p>​ 直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。</p>
<h3 id="dropout率的选择">3.12.4 dropout率的选择</h3>
<ol type="1">
<li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）</li>
<li>对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li>
<li>球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。</li>
<li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。</li>
</ol>
<h3 id="dropout有什么缺点">3.12.5 dropout有什么缺点？</h3>
<p>​ dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。</p>
<h2 id="深度学习中常用的数据增强方法">3.13 深度学习中常用的数据增强方法？</h2>
<ul>
<li>Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；</li>
<li>PCA Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；</li>
<li>Random Scale：尺度变换；</li>
<li>Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；</li>
<li>Horizontal/Vertical Flip：水平/垂直翻转；</li>
<li>Shift：平移变换；</li>
<li>Rotation/Reflection：旋转/仿射变换；</li>
<li>Noise：高斯噪声、模糊处理；</li>
<li>Label Shuffle：类别不平衡数据的增广；</li>
</ul>
<h2 id="如何理解-internal-covariate-shift">3.14 如何理解 Internal Covariate Shift？</h2>
<p>​ 深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p>
<p>​ Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？</p>
<p>​ 大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。</p>
<p>​ 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p>
<p><strong>那么ICS会导致什么问题？</strong></p>
<p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
<p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p>
<p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p>
<p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p>
<h2 id="参考文献">参考文献</h2>
<p>[0] github.com/scutan90/DeepLearning-500-questions</p>
<p>[1] Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain.[J]. Psychological Review, 1958, 65(6):386-408.</p>
<p>[2] Duvenaud D , Rippel O , Adams R P , et al. Avoiding pathologies in very deep networks[J]. Eprint Arxiv, 2014:202-210.</p>
<p>[3] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors[J]. Cognitive modeling, 1988, 5(3): 1.</p>
<p>[4] Hecht-Nielsen R. Theory of the backpropagation neural network[M]//Neural networks for perception. Academic Press, 1992: 65-93.</p>
<p>[5] Felice M. Which deep learning network is best for you?| CIO[J]. 2017.</p>
<p>[6] Conneau A, Schwenk H, Barrault L, et al. Very deep convolutional networks for natural language processing[J]. arXiv preprint arXiv:1606.01781, 2016, 2.</p>
<p>[7] Ba J, Caruana R. Do deep nets really need to be deep?[C]//Advances in neural information processing systems. 2014: 2654-2662.</p>
<p>[8] Nielsen M A. Neural networks and deep learning[M]. USA: Determination press, 2015.</p>
<p>[9] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.</p>
<p>[10] 周志华. 机器学习[M].清华大学出版社, 2016.</p>
<p>[11] Kim J, Kwon Lee J, Mu Lee K. Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1646-1654.</p>
<p>[12] Chen Y, Lin Z, Zhao X, et al. Deep learning-based classification of hyperspectral data[J]. IEEE Journal of Selected topics in applied earth observations and remote sensing, 2014, 7(6): 2094-2107.</p>
<p>[13] Domhan T, Springenberg J T, Hutter F. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves[C]//Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.</p>
<p>[14] Maclaurin D, Duvenaud D, Adams R. Gradient-based hyperparameter optimization through reversible learning[C]//International Conference on Machine Learning. 2015: 2113-2122.</p>
<p>[15] Srivastava R K, Greff K, Schmidhuber J. Training very deep networks[C]//Advances in neural information processing systems. 2015: 2377-2385.</p>
<p>[16] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.</p>
<p>[17] Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning (ICML-11). 2011: 689-696.</p>
<p>[18] Deng L, Yu D. Deep learning: methods and applications[J]. Foundations and Trends® in Signal Processing, 2014, 7(3–4): 197-387.</p>
<p>[19] Erhan D, Bengio Y, Courville A, et al. Why does unsupervised pre-training help deep learning?[J]. Journal of Machine Learning Research, 2010, 11(Feb): 625-660.</p>
<p>[20] Dong C, Loy C C, He K, et al. Learning a deep convolutional network for image super resolution[C]//European conference on computer vision. Springer, Cham, 2014: 184-199.</p>
<p>[21] 郑泽宇，梁博文，顾思宇.TensorFlow：实战Google深度学习框架（第2版）[M].电子工业出版社,2018.</p>
<p>[22] 焦李成. 深度学习优化与识别[M].清华大学出版社,2017.</p>
<p>[23] 吴岸城. 神经网络与深度学习[M].电子工业出版社,2016.</p>
<p>[24] Wei, W.G.H., Liu, T., Song, A., et al. (2018) An Adaptive Natural Gradient Method with Adaptive Step Size in Multilayer Perceptrons. Chinese Automation Congress, 1593-1597.</p>
<p>[25] Y Feng, Y Li.An Overview of Deep Learning Optimization Methods and Learning Rate Attenuation Methods[J].Hans Journal of Data Mining,2018,8(4),186-200.</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>DL基础</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="第二章-机器学习基础">第二章 机器学习基础</h1>
<p>​ 机器学习起源于上世纪50年代，1959年在IBM工作的Arthur Samuel设计了一个下棋程序，这个程序具有学习的能力，它可以在不断的对弈中提高自己。由此提出了“机器学习”这个概念，它是一个结合了多个学科如概率论，优化理论，统计等，最终在计算机上实现自我获取新知识，学习改善自己的这样一个研究领域。机器学习是人工智能的一个子集，目前已经发展出许多有用的方法，比如支持向量机，回归，决策树，随机森林，强化方法，集成学习，深度学习等等，一定程度上可以帮助人们完成一些数据预测，自动化，自动决策，最优化等初步替代脑力的任务。本章我们主要介绍下机器学习的基本概念、监督学习、分类算法、逻辑回归、代价函数、损失函数、LDA、PCA、决策树、支持向量机、EM算法、聚类和降维以及模型评估有哪些方法、指标等等。</p>
<h2 id="基本概念">2.1 基本概念</h2>
<h3 id="大话理解机器学习本质">2.1.1 大话理解机器学习本质</h3>
<p>​ 机器学习(Machine Learning, ML)，顾名思义，让机器去学习。这里，机器指的是计算机，是算法运行的物理载体，你也可以把各种算法本身当做一个有输入和输出的机器。那么到底让计算机去学习什么呢？对于一个任务及其表现的度量方法，设计一种算法，让算法能够提取中数据所蕴含的规律，这就叫机器学习。如果输入机器的数据是带有标签的，就称作有监督学习。如果数据是无标签的，就是无监督学习。</p>
<h3 id="什么是神经网络">2.1.2 什么是神经网络</h3>
<p>​ 神经网络就是按照一定规则将多个神经元连接起来的网络。不同的神经网络，具有不同的连接规则。例如全连接(Full Connected, FC)神经网络，它的规则包括：</p>
<p>（1）有三种层：输入层，输出层，隐藏层。</p>
<p>（2）同一层的神经元之间没有连接。</p>
<p>（3）fully connected的含义：第 N 层的每个神经元和第 N-1 层的所有神经元相连，第 N-1 层神经元的输出就是第 N 层神经元的输入。</p>
<p>（4）每个连接都有一个权值。</p>
<p><strong>神经网络架构</strong> ​ 图2-1就是一个神经网络系统，它由很多层组成。输入层负责接收信息，比如一只猫的图片。输出层是计算机对这个输入信息的判断结果，它是不是猫。隐藏层就是对输入信息的传递和加工处理。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915230618.png" alt="image-20210915230618843" /><figcaption>image-20210915230618843</figcaption>
</figure>
<p>​ 图2-1 神经网络系统</p>
<h3 id="各种常见算法图示">2.1.3 各种常见算法图示</h3>
<p>​ 日常使用机器学习的任务中，我们经常会遇见各种算法，图2-2是各种常见算法的图示。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915230747.png" alt="image-20210915230747809" /><figcaption>image-20210915230747809</figcaption>
</figure>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915230805.png" alt="image-20210915230805001" /><figcaption>image-20210915230805001</figcaption>
</figure>
<p>​ 图2-2 各种常见算法图示</p>
<h3 id="计算图的导数计算">2.1.4 计算图的导数计算</h3>
<p>​ 计算图导数计算是反向传播，利用链式法则和隐式函数求导。</p>
<p>​ 假设 <span class="math inline">\(z = f(u,v)\)</span> 在点 <span class="math inline">\((u,v)\)</span> 处偏导连续，<span class="math inline">\((u,v)\)</span>是关于 <span class="math inline">\(t\)</span> 的函数，在 <span class="math inline">\(t\)</span> 点可导，求 <span class="math inline">\(z\)</span> 在 <span class="math inline">\(t\)</span> 点的导数。</p>
<p>根据链式法则有 <span class="math inline">\(\frac{dz}{dt}=\frac{\partial z}{\partial u}.\frac{du}{dt}+\frac{\partial z}{\partial v}  .\frac{dv}{dt}\)</span> ​ 链式法则用文字描述:“由两个函数凑起来的复合函数，其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数。<br />
​ 为了便于理解，下面举例说明： <span class="math inline">\(f(x)=x^2,g(x)=2x+1\)</span> ​ 则: <span class="math inline">\({f[g(x)]}&#39;=2[g(x)] \times g&#39;(x)=2[2x+1] \times 2=8x+4\)</span></p>
<h3 id="理解局部最优与全局最优">2.1.5 理解局部最优与全局最优</h3>
<p>​ 笑谈局部最优和全局最优</p>
<blockquote>
<p>​ 柏拉图有一天问老师苏格拉底什么是爱情？苏格拉底叫他到麦田走一次，摘一颗最大的麦穗回来，不许回头，只可摘一次。柏拉图空着手出来了，他的理由是，看见不错的，却不知道是不是最好的，一次次侥幸，走到尽头时，才发现还不如前面的，于是放弃。苏格拉底告诉他：“这就是爱情。”这故事让我们明白了一个道理，因为生命的一些不确定性，所以全局最优解是很难寻找到的，或者说根本就不存在，我们应该设置一些限定条件，然后在这个范围内寻找最优解，也就是局部最优解——有所斩获总比空手而归强，哪怕这种斩获只是一次有趣的经历。 ​ 柏拉图有一天又问什么是婚姻？苏格拉底叫他到树林走一次,选一棵最好的树做圣诞树，也是不许回头，只许选一次。这次他一身疲惫地拖了一棵看起来直挺、翠绿，却有点稀疏的杉树回来，他的理由是，有了上回的教训，好不容易看见一棵看似不错的，又发现时间、体力已经快不够用了，也不管是不是最好的，就拿回来了。苏格拉底告诉他：“这就是婚姻。”</p>
</blockquote>
<p>​ 优化问题一般分为局部最优和全局最优。其中，</p>
<p>（1）局部最优，就是在函数值空间的一个有限区域内寻找最小值；而全局最优，是在函数值空间整个区域寻找最小值问题。</p>
<p>（2）函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。</p>
<p>（3）全局最小点是那种它的函数值小于或等于所有的可行点。</p>
<h3 id="大数据与深度学习之间的关系">2.1.5 大数据与深度学习之间的关系</h3>
<p>首先来看大数据、机器学习及数据挖掘三者简单的定义：</p>
<p><strong>大数据</strong>通常被定义为“超出常用软件工具捕获，管理和处理能力”的数据集。 <strong>机器学习</strong>关心的问题是如何构建计算机程序使用经验自动改进。 <strong>数据挖掘</strong>是从数据中提取模式的特定算法的应用，在数据挖掘中，重点在于算法的应用，而不是算法本身。</p>
<p><strong>机器学习和数据挖掘</strong>之间的关系如下： 数据挖掘是一个过程，在此过程中机器学习算法被用作提取数据集中的潜在有价值模式的工具。 大数据与深度学习关系总结如下：</p>
<p>（1）深度学习是一种模拟大脑的行为。可以从所学习对象的机制以及行为等等很多相关联的方面进行学习，模仿类型行为以及思维。</p>
<p>（2）深度学习对于大数据的发展有帮助。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。</p>
<p>（3）深度学习转变了解决问题的思维。很多时候发现问题到解决问题，走一步看一步不是一个主要的解决问题的方式了，在深度学习的基础上，要求我们从开始到最后都要基于一个目标，为了需要优化的那个最终目标去进行处理数据以及将数据放入到数据应用平台上去，这就是端到端（End to End）。</p>
<p>（4）大数据的深度学习需要一个框架。在大数据方面的深度学习都是从基础的角度出发的，深度学习需要一个框架或者一个系统。总而言之，将你的大数据通过深度分析变为现实，这就是深度学习和大数据的最直接关系。</p>
<h2 id="机器学习学习方式">2.2 机器学习学习方式</h2>
<p>​ 根据数据类型的不同，对一个问题的建模有不同的方式。依据不同的学习方式和输入数据，机器学习主要分为以下四种学习方式。</p>
<h3 id="监督学习">2.2.1 监督学习</h3>
<p>​ 特点：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。</p>
<p>​ 常见应用场景：监督式学习的常见应用场景如分类问题和回归问题。</p>
<p>​ 算法举例：常见的有监督机器学习算法包括支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。深度学习(Deep Learning)也是大多数以监督学习的方式呈现。</p>
<h3 id="非监督式学习">2.2.2 非监督式学习</h3>
<p>​ 定义：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构。</p>
<p>​ 常见应用场景：常见的应用场景包括关联规则的学习以及聚类等。</p>
<p>​ 算法举例：常见算法包括Apriori算法以及k-Means算法。</p>
<h3 id="半监督式学习">2.2.3 半监督式学习</h3>
<p>​ 特点：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。</p>
<p>​ 常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。</p>
<p>​ 算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。</p>
<h3 id="弱监督学习">2.2.4 弱监督学习</h3>
<p>​ 特点：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。</p>
<p>​ 算法举例：举例，给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线，这就是已知弱标签学习强标签的问题。</p>
<p>​ 在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。</p>
<h3 id="监督学习有哪些步骤">2.2.5 监督学习有哪些步骤</h3>
<p>​ 监督学习是使用已知正确答案的示例来训练网络，每组训练数据有一个明确的标识或结果。想象一下，我们可以训练一个网络，让其从照片库中（其中包含气球的照片）识别出气球的照片。以下就是我们在这个假设场景中所要采取的步骤。</p>
<p><strong>步骤1：数据集的创建和分类</strong> ​ 首先，浏览你的照片（数据集），确定所有包含气球的照片，并对其进行标注。然后，将所有照片分为训练集和验证集。目标就是在深度网络中找一函数，这个函数输入是任意一张照片，当照片中包含气球时，输出1，否则输出0。</p>
<p><strong>步骤2：数据增强（Data Augmentation）</strong> ​ 当原始数据搜集和标注完毕，一般搜集的数据并不一定包含目标在各种扰动下的信息。数据的好坏对于机器学习模型的预测能力至关重要，因此一般会进行数据增强。对于图像数据来说，数据增强一般包括，图像旋转，平移，颜色变换，裁剪，仿射变换等。</p>
<p><strong>步骤3：特征工程（Feature Engineering）</strong> ​ 一般来讲，特征工程包含特征提取和特征选择。常见的手工特征(Hand-Crafted Feature)有尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)，方向梯度直方图(Histogram of Oriented Gradient, HOG)等。由于手工特征是启发式的，其算法设计背后的出发点不同，将这些特征组合在一起的时候有可能会产生冲突，如何将组合特征的效能发挥出来，使原始数据在特征空间中的判别性最大化，就需要用到特征选择的方法。在深度学习方法大获成功之后，人们很大一部分不再关注特征工程本身。因为，最常用到的卷积神经网络(Convolutional Neural Networks, CNNs)本身就是一种特征提取和选择的引擎。研究者提出的不同的网络结构、正则化、归一化方法实际上就是深度学习背景下的特征工程。</p>
<p><strong>步骤4：构建预测模型和损失</strong> ​ 将原始数据映射到特征空间之后，也就意味着我们得到了比较合理的输入。下一步就是构建合适的预测模型得到对应输入的输出。而如何保证模型的输出和输入标签的一致性，就需要构建模型预测和标签之间的损失函数，常见的损失函数(Loss Function)有交叉熵、均方差等。通过优化方法不断迭代，使模型从最初的初始化状态一步步变化为有预测能力的模型的过程，实际上就是学习的过程。</p>
<p><strong>步骤5：训练</strong> ​ 选择合适的模型和超参数进行初始化，其中超参数比如支持向量机中核函数、误差项惩罚权重等。当模型初始化参数设定好后，将制作好的特征数据输入到模型，通过合适的优化方法不断缩小输出与标签之间的差距，当迭代过程到了截止条件，就可以得到训练好的模型。优化方法最常见的就是梯度下降法及其变种，使用梯度下降法的前提是优化目标函数对于模型是可导的。</p>
<p><strong>步骤6：验证和模型选择</strong> ​ 训练完训练集图片后，需要进行模型测试。利用验证集来验证模型是否可以准确地挑选出含有气球在内的照片。 ​ 在此过程中，通常会通过调整和模型相关的各种事物（超参数）来重复步骤2和3，诸如里面有多少个节点，有多少层，使用怎样的激活函数和损失函数，如何在反向传播阶段积极有效地训练权值等等。</p>
<p><strong>步骤7：测试及应用</strong> ​ 当有了一个准确的模型，就可以将该模型部署到你的应用程序中。你可以将预测功能发布为API（Application Programming Interface, 应用程序编程接口）调用，并且你可以从软件中调用该API，从而进行推理并给出相应的结果。</p>
<h2 id="分类算法">2.8 分类算法</h2>
<p>​ 分类算法和回归算法是对真实世界不同建模的方法。分类模型是认为模型的输出是离散的，例如大自然的生物被划分为不同的种类，是离散的。回归模型的输出是连续的，例如人的身高变化过程是一个连续过程，而不是离散的。</p>
<p>​ 因此，在实际建模过程时，采用分类模型还是回归模型，取决于你对任务（真实世界）的分析和理解。</p>
<h3 id="常用分类算法的优缺点">2.8.1 常用分类算法的优缺点？</h3>
<p>​ 接下来我们介绍常用分类算法的优缺点，如表2-1所示。</p>
<p>​ 表2-1 常用分类算法的优缺点</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">算法</th>
<th style="text-align: left;">优点</th>
<th style="text-align: left;">缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bayes 贝叶斯分类法</td>
<td style="text-align: left;">1）所需估计的参数少，对于缺失数据不敏感。<br />2）有着坚实的数学基础，以及稳定的分类效率。</td>
<td style="text-align: left;">1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br />2）需要知道先验概率。<br />3）分类决策存在错误率。</td>
</tr>
<tr class="even">
<td style="text-align: left;">Decision Tree决策树</td>
<td style="text-align: left;">1）不需要任何领域知识或参数假设。<br />2）适合高维数据。<br />3）简单易于理解。<br />4）短时间内处理大量数据，得到可行且效果较好的结果。<br />5）能够同时处理数据型和常规性属性。</td>
<td style="text-align: left;">1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br />2）易于过拟合。<br />3）忽略属性之间的相关性。<br />4）不支持在线学习。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SVM支持向量机</td>
<td style="text-align: left;">1）可以解决小样本下机器学习的问题。<br />2）提高泛化性能。<br />3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br />4）避免神经网络结构选择和局部极小的问题。</td>
<td style="text-align: left;">1）对缺失数据敏感。<br />2）内存消耗大，难以解释。<br />3）运行和调参略烦人。</td>
</tr>
<tr class="even">
<td style="text-align: left;">KNN K近邻</td>
<td style="text-align: left;">1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br />2）可用于非线性分类；<br /> 3）训练时间复杂度为O(n)； <br />4）准确度高，对数据没有假设，对outlier不敏感；</td>
<td style="text-align: left;">1）计算量太大。<br />2）对于样本分类不均衡的问题，会产生误判。<br />3）需要大量的内存。<br />4）输出的可解释性不强。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Logistic Regression逻辑回归</td>
<td style="text-align: left;">1）速度快。<br />2）简单易于理解，直接看到各个特征的权重。<br />3）能容易地更新模型吸收新的数据。<br />4）如果想要一个概率框架，动态调整分类阀值。</td>
<td style="text-align: left;">特征处理复杂。需要归一化和较多的特征工程。</td>
</tr>
<tr class="even">
<td style="text-align: left;">Neural Network 神经网络</td>
<td style="text-align: left;">1）分类准确率高。<br />2）并行处理能力强。<br />3）分布式存储和学习能力强。<br />4）鲁棒性较强，不易受噪声影响。</td>
<td style="text-align: left;">1）需要大量参数（网络拓扑、阀值、阈值）。<br />2）结果难以解释。<br />3）训练时间过长。</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adaboosting</td>
<td style="text-align: left;">1）adaboost是一种有很高精度的分类器。<br />2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br />3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br />4）简单，不用做特征筛选。<br />5）不用担心overfitting。</td>
<td style="text-align: left;">对outlier比较敏感</td>
</tr>
</tbody>
</table>
<h3 id="分类算法的评估方法">2.8.2 分类算法的评估方法</h3>
<p>​ 分类评估方法主要功能是用来评估分类算法的好坏，而评估一个分类器算法的好坏又包括许多项指标。了解各种评估方法，在实际应用中选择正确的评估方法是十分重要的。</p>
<ul>
<li><strong>几个常用术语</strong> ​ 这里首先介绍几个常见的模型评价术语，现在假设我们的分类目标只有两类，计为正例（positive）和负例（negative）分别是：
<ol type="1">
<li>True positives(TP): 被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数；</li>
<li>False positives(FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数；</li>
<li>False negatives(FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数；</li>
<li>True negatives(TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。　</li>
</ol>
​ 表2-2 四个术语的混淆矩阵</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915230926.png" alt="image-20210915230926707" /><figcaption>image-20210915230926707</figcaption>
</figure>
<p>表2-2是这四个术语的混淆矩阵，做以下说明： 1）P=TP+FN表示实际为正例的样本个数。 2）True、False描述的是分类器是否判断正确。 3）Positive、Negative是分类器的分类结果，如果正例计为1、负例计为-1，即positive=1、negative=-1。用1表示True，-1表示False，那么实际的类标=TF*PN，TF为true或false，PN为positive或negative。 4）例如True positives(TP)的实际类标=1*1=1为正例，False positives(FP)的实际类标=(-1)*1=-1为负例，False negatives(FN)的实际类标=(-1)*(-1)=1为正例，True negatives(TN)的实际类标=1*(-1)=-1为负例。</p>
<ul>
<li><strong>评价指标</strong>
<ol type="1">
<li>正确率（accuracy） 正确率是我们最常见的评价指标，accuracy = (TP+TN)/(P+N)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。</li>
<li>错误率（error rate) 错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 - error rate。</li>
<li>灵敏度（sensitivity） sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。</li>
<li>特异性（specificity) specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。</li>
<li>精度（precision） precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。</li>
<li>召回率（recall） 召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitivity，可以看到召回率与灵敏度是一样的。</li>
<li>其他评价指标 计算速度：分类器训练和预测需要的时间； 鲁棒性：处理缺失值和异常值的能力； 可扩展性：处理大数据集的能力； 可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。</li>
<li>精度和召回率反映了分类器分类性能的两个方面。如果综合考虑查准率与查全率，可以得到新的评价指标F1-score，也称为综合分类率：<span class="math inline">\(F1=\frac{2 \times precision \times recall}{precision + recall}\)</span>。</li>
</ol>
<p>为了综合多个类别的分类情况，评测系统整体性能，经常采用的还有微平均F1（micro-averaging）和宏平均F1（macro-averaging ）两种指标。</p>
<p>（1）宏平均F1与微平均F1是以两种不同的平均方式求的全局F1指标。</p>
<p>（2）宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。</p>
<p>（3）微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。</p>
<p>（4）由两种平均F1的计算方式不难看出，宏平均F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均F1平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。</p></li>
<li><p><strong>ROC曲线和PR曲线</strong></p>
<p>如图2-3，ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，是以灵敏度（真阳性率）为纵坐标，以1减去特异性（假阳性率）为横坐标绘制的性能评价曲线。可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠。也可以通过ROC曲线下面的面积（Area Under Curve, AUC）来评价模型，AUC越大，模型越可靠。</p></li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231030.png" /></p>
<p>PR曲线是Precision Recall Curve的简称，描述的是precision和recall之间的关系，以recall为横坐标，precision为纵坐标绘制的曲线。该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。</p>
<h3 id="正确率能很好的评估分类算法吗">2.8.3 正确率能很好的评估分类算法吗</h3>
<p>​ 不同算法有不同特点，在不同数据集上有不同的表现效果，根据特定的任务选择不同的算法。如何评价分类算法的好坏，要做具体任务具体分析。对于决策树，主要用正确率去评估，但是其他算法，只用正确率能很好的评估吗？ ​ 答案是否定的。 ​ 正确率确实是一个很直观很好的评价指标，但是有时候正确率高并不能完全代表一个算法就好。比如对某个地区进行地震预测，地震分类属性分为0：不发生地震、1发生地震。我们都知道，不发生的概率是极大的，对于分类器而言，如果分类器不加思考，对每一个测试样例的类别都划分为0，达到99%的正确率，但是，问题来了，如果真的发生地震时，这个分类器毫无察觉，那带来的后果将是巨大的。很显然，99%正确率的分类器并不是我们想要的。出现这种现象的原因主要是数据分布不均衡，类别为1的数据太少，错分了类别1但达到了很高的正确率缺忽视了研究者本身最为关注的情况。</p>
<h3 id="什么样的分类器是最好的">2.8.4 什么样的分类器是最好的</h3>
<p>​ 对某一个任务，某个具体的分类器不可能同时满足或提高所有上面介绍的指标。 ​ 如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如之前说的地震预测，既然不能百分百预测地震的发生，但实际情况中能容忍一定程度的误报。假设在1000次预测中，共有5次预测发生了地震，真实情况中有一次发生了地震，其他4次则为误报。正确率由原来的999/1000=99.9下降为996/1000=99.6。召回率由0/1=0%上升为1/1=100%。对此解释为，虽然预测失误了4次，但真的地震发生前，分类器能预测对，没有错过，这样的分类器实际意义更为重大，正是我们想要的。在这种情况下，在一定正确率前提下，要求分类器的召回率尽量高。</p>
<h2 id="逻辑回归">2.9 逻辑回归</h2>
<h3 id="回归划分">2.9.1 回归划分</h3>
<p>广义线性模型家族里，依据因变量不同，可以有如下划分：</p>
<p>（1）如果是连续的，就是多重线性回归。</p>
<p>（2）如果是二项分布，就是逻辑回归。</p>
<p>（3）如果是泊松（Poisson）分布，就是泊松回归。</p>
<p>（4）如果是负二项分布，就是负二项回归。</p>
<p>（5）逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的逻辑回归。</p>
<h3 id="逻辑回归适用性">2.9.2 逻辑回归适用性</h3>
<p>逻辑回归可用于以下几个方面：</p>
<p>（1）用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。</p>
<p>（2）用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。</p>
<p>（3）寻找危险因素。寻找某一疾病的危险因素等。</p>
<p>（4）仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。</p>
<p>（5）各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。</p>
<h3 id="生成模型和判别模型的区别">2.9.3 生成模型和判别模型的区别</h3>
<p>生成模型：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)（贝叶斯概率）。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。典型的生成模型有朴素贝叶斯，隐马尔科夫模型等</p>
<p>判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。这些模型的特点都是输入属性X可以直接得到后验概率P(Y|X)，输出条件概率最大的作为最终的类别（对于二分类任务来说，实际得到一个score，当score大于threshold时则为正类，否则为负类）。</p>
<p>举例：</p>
<p>判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。</p>
<p>生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。</p>
<p>联系和区别：</p>
<pre><code>生成方法的特点：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y,X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。

判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</code></pre>
<p>​ 最后，由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p>
<h3 id="逻辑回归与朴素贝叶斯有什么区别">2.9.4 逻辑回归与朴素贝叶斯有什么区别</h3>
<p>逻辑回归与朴素贝叶斯区别有以下几个方面：</p>
<p>（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。</p>
<p>（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。</p>
<p>（3）朴素贝叶斯需要条件独立假设。</p>
<p>（4）逻辑回归需要求特征参数间是线性的。</p>
<h3 id="线性回归与逻辑回归的区别">2.9.5 线性回归与逻辑回归的区别</h3>
<p>线性回归与逻辑回归的区别如下描述：</p>
<p>（1）线性回归的样本的输出，都是连续值，$ y(-,+)<span class="math inline">\(，而逻辑回归中\)</span>y(0,1)$，只能取0和1。</p>
<p>（2）对于拟合函数也有本质上的差别：</p>
<p>​ 线性回归：<span class="math inline">\(f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}\)</span></p>
<p>​ 逻辑回归：<span class="math inline">\(f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)\)</span>，其中，<span class="math inline">\(g(z)=\frac{1}{1+e^{-z}}\)</span></p>
<p>​ 可以看出，线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。</p>
<p>​ 那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？</p>
<p>​ <span class="math inline">\(\theta ^{T}x=0\)</span>就相当于是1类和0类的决策边界：</p>
<p>​ 当<span class="math inline">\(\theta ^{T}x&gt;0\)</span>，则y&gt;0.5；若$^{T}x+<span class="math inline">\(，则\)</span>y 1 $，即y为1类;</p>
<p>​ 当<span class="math inline">\(\theta ^{T}x&lt;0\)</span>，则y&lt;0.5；若$^{T}x-<span class="math inline">\(，则\)</span>y 0 $，即y为0类;</p>
<p>这个时候就能看出区别，在线性回归中<span class="math inline">\(\theta ^{T}x\)</span>为预测值的拟合函数；而在逻辑回归中<span class="math inline">\(\theta ^{T}x\)</span>为决策边界。下表2-3为线性回归和逻辑回归的区别。</p>
<p>​ 表2-3 线性回归和逻辑回归的区别</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">线性回归</th>
<th style="text-align: center;">逻辑回归</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">目的</td>
<td style="text-align: center;">预测</td>
<td style="text-align: center;">分类</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(y^{(i)}\)</span></td>
<td style="text-align: center;">未知</td>
<td style="text-align: center;">（0,1）</td>
</tr>
<tr class="odd">
<td style="text-align: center;">函数</td>
<td style="text-align: center;">拟合函数</td>
<td style="text-align: center;">预测函数</td>
</tr>
<tr class="even">
<td style="text-align: center;">参数计算方式</td>
<td style="text-align: center;">最小二乘法</td>
<td style="text-align: center;">极大似然估计</td>
</tr>
</tbody>
</table>
<p>下面具体解释一下：</p>
<ol type="1">
<li>拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得<span class="math inline">\(y^{(i)} \in (0,1)\)</span>;</li>
<li>最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然是Probability。而最小二乘是计算误差损失。</li>
</ol>
<h2 id="代价函数">2.10 代价函数</h2>
<h3 id="为什么需要代价函数">2.10.1 为什么需要代价函数</h3>
<ol type="1">
<li>为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。</li>
<li>用于找到最优解的目的函数。</li>
</ol>
<h3 id="代价函数作用原理">2.10.2 代价函数作用原理</h3>
<p>​ 在回归问题中，通过代价函数来求解最优解，常用的是平方误差代价函数。假设函数图像如图2-4所示，当参数发生变化时，假设函数状态也会随着变化。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231141.png" alt="image-20210915231141389" /><figcaption>image-20210915231141389</figcaption>
</figure>
<p>​ 图2-4 <span class="math inline">\(h(x) = A + Bx\)</span>函数示意图</p>
<p>​ 想要拟合图中的离散点，我们需要尽可能找到最优的<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>来使这条直线更能代表所有数据。如何找到最优解呢，这就需要使用代价函数来求解，以平方误差代价函数为例，假设函数为<span class="math inline">\(h(x)=\theta_0x\)</span>。 ​ <strong>平方误差代价函数的主要思想</strong>就是将实际数据给出的值与拟合出的线的对应值做差，求出拟合出的直线与实际的差距。在实际应用中，为了避免因个别极端数据产生的影响，采用类似方差再取二分之一的方式来减小个别数据的影响。因此，引出代价函数： <span class="math inline">\(J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2\)</span></p>
<p>​ <strong>最优解即为代价函数的最小值</strong><span class="math inline">\(\min J(\theta_0, \theta_1)\)</span>。如果是1个参数，代价函数一般通过二维曲线便可直观看出。如果是2个参数，代价函数通过三维图像可看出效果，参数越多，越复杂。 当参数为2个时，代价函数是三维图像，如下图2-5所示。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231203.png" alt="image-20210915231203226" /><figcaption>image-20210915231203226</figcaption>
</figure>
<p>图2-5 代价函数三维图像</p>
<h3 id="为什么代价函数要非负">2.10.3 为什么代价函数要非负</h3>
<p>​ 目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。 ​ 只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。</p>
<h3 id="常见代价函数">2.10.4 常见代价函数</h3>
<p>（1）<strong>二次代价函数（quadratic cost）</strong>： <span class="math inline">\(J = \frac{1}{2n}\sum_x\Vert y(x)-a^L(x)\Vert^2\)</span></p>
<p>​ 其中，<span class="math inline">\(J\)</span>表示代价函数，<span class="math inline">\(x\)</span>表示样本，<span class="math inline">\(y\)</span>表示实际值，<span class="math inline">\(a\)</span>表示输出值，<span class="math inline">\(n\)</span>表示样本的总数。使用一个样本为例简单说明，此时二次代价函数为： <span class="math inline">\(J = \frac{(y-a)^2}{2}\)</span> ​ 假如使用梯度下降法（Gradient descent）来调整权值参数的大小，权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>的梯度推导如下： <span class="math inline">\(\frac{\partial J}{\partial w}=(y-a)\sigma&#39;(z)x\;, \frac{\partial J}{\partial b}=(y-a)\sigma&#39;(z)\)</span> 其中，<span class="math inline">\(z\)</span>表示神经元的输入，<span class="math inline">\(\sigma\)</span>表示激活函数。权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>的梯度跟激活函数的梯度成正比，激活函数的梯度越大，权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>的大小调整得越快，训练收敛得就越快。</p>
<p><em>注</em>：神经网络常用的激活函数为sigmoid函数，该函数的曲线如下图2-6所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231232.png" alt="image-20210915231232871" /><figcaption>image-20210915231232871</figcaption>
</figure>
<p>图2-6 sigmoid函数曲线</p>
<p>如上图所示，对0.88和0.98两个点进行比较： ​ 假设目标是收敛到1.0。0.88离目标1.0比较远，梯度比较大，权值调整比较大。0.98离目标1.0比较近，梯度比较小，权值调整比较小。调整方案合理。 ​ 假如目标是收敛到0。0.88离目标0比较近，梯度比较大，权值调整比较大。0.98离目标0比较远，梯度比较小，权值调整比较小。调整方案不合理。 ​ 原因：在使用sigmoid函数的情况下, 初始的代价（误差）越大，导致训练越慢。</p>
<p>（2）<strong>交叉熵代价函数（cross-entropy）</strong>： <span class="math inline">\(J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]\)</span></p>
<p>其中，<span class="math inline">\(J\)</span>表示代价函数，<span class="math inline">\(x\)</span>表示样本，<span class="math inline">\(y\)</span>表示实际值，<span class="math inline">\(a\)</span>表示输出值，<span class="math inline">\(n\)</span>表示样本的总数。 权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>的梯度推导如下： <span class="math inline">\(\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;， \frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)\)</span></p>
<p>当误差越大时，梯度就越大，权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>调整就越快，训练的速度也就越快。 <strong>二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是S型函数的情况。</strong></p>
<p>（3）<strong>对数似然代价函数（log-likelihood cost）</strong>： 对数似然函数常用来作为softmax回归的代价函数。深度学习中普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然代价函数。 对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。 在tensorflow中： 与sigmoid搭配使用的交叉熵函数：<code>tf.nn.sigmoid_cross_entropy_with_logits()</code>。 与softmax搭配使用的交叉熵函数：<code>tf.nn.softmax_cross_entropy_with_logits()</code>。 在pytorch中： 与sigmoid搭配使用的交叉熵函数：<code>torch.nn.BCEWithLogitsLoss()</code>。 与softmax搭配使用的交叉熵函数：<code>torch.nn.CrossEntropyLoss()</code>。</p>
<p>对数似然函数：</p>
<p>​ 我们将似然函数作为机器学习模型的损失函数，并且用在分类问题中。这时似然函数是直接作用于模型的输出的（损失函数就是为了衡量当前参数下model的预测值predict距离真实值label的大小，所以似然函数用作损失函数时当然也是为了完成该任务），所以对于似然函数来说，这里的样本集就成了label集（而不是机器学习意义上的样本集X了），这里的参数也不是机器学习model 的参数，而是predict值。</p>
<p>其实作为损失函数的似然函数并不关心你当前的机器学习model的参数是怎样的，毕竟它此时所接收的输入只有两部分：<strong>1、predict。2、label 。3、分布模型（predict服从的分布）</strong>。</p>
<p>显然这里的label就是似然函数的观测值，即样本集。<strong>而它眼里的模型，当然就是predict这个随机变量所服从的概率分布模型。它的目的，就是衡量predict背后的模型对于当前观测值的解释程度。而每个样本的predict值，恰恰就是它所服从的分布模型的参数。</strong></p>
<p>比如此时我们的机器学习任务是一个4个类别的分类任务，机器学习model的输出就是当前样本X下的每个类别的概率，如predict=[0.1, 0.1, 0.7, 0.1]，而该样本的标签是类别3，表示成向量就是label=[0, 0, 1, 0]。那么label=[0, 0, 1, 0]就是似然函数眼里的样本，然后我们可以假设predict这个随机变量背后的模型是<strong>单次观测下的多项式分布</strong>，（<strong>因为softmax本身是基于多项式分布的</strong>）。</p>
<p>回顾：</p>
<p>伯努利分布，也叫做（0，1）分布，贝努利分布可以看成是将一枚硬币（只有正反两个面，代表两个类别）向上扔出，出现某个面（类别）的概率情况，因此其概率密度函数为：</p>
<p><span class="math inline">\(f(x)=p^x(1-p)^{1-x}= \begin{cases} p,&amp; x=1\\ q,&amp; x=0 \end{cases}\)</span> 这是理解似然函数做损失函数的关键！另外，贝努利分布的模型参数就是其中一个类别的发生概率。</p>
<p>而二项分布呢，就是将贝努利实验重复n次（各次实验之间是相互独立的）。</p>
<p>而多项式分布呢，就是将二项分布推广到多个面（类别）。</p>
<p><strong>所以，单次观测下的多项式分布就是贝努利分布的多类推广！即：</strong> <span class="math inline">\(f_{mulit}(x;p)=\prod_{i=1}^C p_{i}^{xi}\)</span> 其中，C代表类别数。p代表向量形式的模型参数，即各个类别的发生概率，如p=[0.1, 0.1, 0.7, 0.1]，则p1=0.1, p3=0.7等。即，<strong>多项式分布的模型参数就是各个类别的发生概率！</strong>x代表<strong>one-hot形式</strong>的观测值，如x=类别3，则x=[0, 0, 1, 0]。xi代表x的第i个元素，比如x=类别3时，x1=0，x2=0，x3=1，x4=0。</p>
<p>想一下，机器学习model对某个样本的输出，就代表各个类别发生的概率。但是，对于当前<strong>这一个</strong>样本而言，它肯定只能有<strong>一个类别</strong>，所以这一个样本就可以看成是一次实验（观察），而这次实验（观察）的结果要服从上述各个类别发生的概率，那不就是服从多项式分布嘛！而且是单次观察！各个类别发生的概率predict当然就是这个多项式分布的参数。</p>
<p><strong>总结一下，对于多类分类问题，似然函数就是衡量当前这个以predict为参数的单次观测下的多项式分布模型与样本值label之间的似然度。</strong></p>
<p>所以，根据似然函数的定义，单个样本的似然函数即：</p>
<p><span class="math inline">\(L = f_{mulit}(label;predict)\)</span> 所以，整个样本集（或者一个batch）的似然函数即： <span class="math inline">\(L=\prod_{X}f_{multi}(label;predict)= \prod_{X}\prod_{i=1}^{C}predict(i)^{label(i)}\)</span> 所以在累乘号前面加上log函数后，就成了所谓的对数似然函数： <span class="math inline">\(L=\sum_{X}\sum_{i=1}^{C}label(i)log(predict(i))\)</span> 而最大化对数似然函数就等效于最小化负对数似然函数，所以前面加个负号就和交叉熵的形式相同的了。</p>
<p>交叉熵定义：对于某种分布的随机变量X~p(x), 有一个模型q(x)用于近似p(x)的概率分布，则分布X与模型q之间的交叉熵即：</p>
<p><span class="math inline">\(H(X,q)=-\sum_{x}p(x)logq(x)\)</span> 这里X的分布模型即样本集label的真实分布模型，这里模型q(x)即想要模拟真实分布模型的机器学习模型。可以说交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。</p>
<p>tips：</p>
<p>最大似然估计：</p>
<p>给定一堆数据，假如我们知道它是从某一种分布中随机取出来的，可是我们并不知道这个分布具体的参，即“模型已定，参数未知”。例如，我们知道这个分布是正态分布，但是不知道均值和方差；或者是二项分布，但是不知道均值。最大似然估计（MLE，Maximum Likelihood Estimation）就可以用来估计模型的参数。<strong>MLE的目标是找出一组参数，使得模型产生出观测数据的概率最大。</strong></p>
<h3 id="为什么用交叉熵代替二次代价函数">2.10.5 为什么用交叉熵代替二次代价函数</h3>
<p>（1）<strong>为什么不用二次方代价函数</strong> 由上一节可知，权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>的偏导数为<span class="math inline">\(\frac{\partial J}{\partial w}=(a-y)\sigma&#39;(z)x\)</span>，<span class="math inline">\(\frac{\partial J}{\partial b}=(a-y)\sigma&#39;(z)\)</span>， 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近0和1时非常小，会导致一些实例在刚开始训练时学习得非常慢。</p>
<p>（2）<strong>为什么要用交叉熵</strong> 交叉熵函数权值<span class="math inline">\(w\)</span>和偏置<span class="math inline">\(b\)</span>的梯度推导为： <span class="math inline">\(\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;， \frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)\)</span></p>
<p>由以上公式可知，权重学习的速度受到<span class="math inline">\(\sigma{(z)}-y\)</span>影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因<span class="math inline">\(\sigma&#39;{(z)}\)</span>导致的学习缓慢的情况。</p>
<h2 id="损失函数">2.11 损失函数</h2>
<h3 id="什么是损失函数">2.11.1 什么是损失函数</h3>
<p>​ 损失函数（Loss Function）又叫做误差函数，用来衡量算法的运行情况，估量模型的预测值与真实值的不一致程度，是一个非负实值函数，通常使用$ L(Y, f(x))$来表示。损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<h3 id="常见的损失函数">2.11.2 常见的损失函数</h3>
<p>​ 机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。 ​ 损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。 ​ 损失函数可分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数上加上正则项。 ​ 下面介绍常用的损失函数：</p>
<p>（1）<strong>0-1损失函数</strong> 如果预测值和目标值相等，值为0，如果不相等，值为1。 <span class="math inline">\(L(Y, f(x)) = \begin{cases} 1,&amp; Y\ne f(x)\\ 0,&amp; Y = f(x) \end{cases}\)</span></p>
<p>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
<p><span class="math inline">\(L(Y, f(x)) = \begin{cases} 1,&amp; |Y-f(x)|\geqslant T\\ 0,&amp; |Y-f(x)|&lt; T \end{cases}\)</span></p>
<p>（2）<strong>绝对值损失函数</strong> 和0-1损失函数相似，绝对值损失函数表示为： <span class="math inline">\(L(Y, f(x)) = |Y-f(x)|\)</span></p>
<p>（3）<strong>平方损失函数</strong> <span class="math inline">\(L(Y, f(x)) = \sum_N{(Y-f(x))}^2\)</span></p>
<p>这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该使所有点到回归直线的距离和最小。</p>
<p>（4）<strong>对数损失函数</strong> <span class="math inline">\(L(Y, P(Y|X)) = -\log{P(Y|X)}=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M y_{ij}log(p_{ij})\)</span></p>
<p>​ 其中, Y 为输出变量, X为输入变量, L 为损失函数. N为输入样本量, M为可能的类别数, <span class="math inline">\(y_{ij}\)</span> 是一个二值指标, 表示类别 j 是否是输入实例 xi 的真实类别. <span class="math inline">\(p_{ij}\)</span> 为模型或分类器预测输入实例 xi 属于类别 j 的概率.</p>
<p>常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数是平方损失，其实不然。逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是对数损失函数。形式上等价于二分类的交叉熵损失函数。</p>
<p>（6）<strong>指数损失函数</strong> 指数损失函数的标准形式为： <span class="math inline">\(L(Y, f(x)) = \exp(-Yf(x))\)</span></p>
<p>例如AdaBoost就是以指数损失函数为损失函数。</p>
<p>（7）<strong>Hinge损失函数</strong> Hinge损失函数的标准形式如下： <span class="math inline">\(L(y) = \max{(0, 1-ty)}\)</span></p>
<p>统一的形式： <span class="math inline">\(L(Y, f(x)) = \max{(0, Yf(x))}\)</span></p>
<p>其中y是预测值，范围为(-1,1)，t为目标值，其为-1或1。</p>
<p>在线性支持向量机中，最优化问题可等价于</p>
<p><span class="math inline">\(\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2\)</span></p>
<p>上式相似于下式</p>
<p><span class="math inline">\(\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2\)</span></p>
<p>其中<span class="math inline">\(l(wx_i+by_i)\)</span>是Hinge损失函数，<span class="math inline">\(\Vert w\Vert ^2\)</span>可看做为正则化项。</p>
<h3 id="逻辑回归为什么使用对数损失函数">2.11.3 逻辑回归为什么使用对数损失函数</h3>
<p>假设逻辑回归模型 <span class="math inline">\(P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}\)</span> 假设逻辑回归模型的概率分布是伯努利分布，其概率质量函数为： <span class="math inline">\(P(X=n)= \begin{cases} 1-p, n=0\\  p,n=1 \end{cases}\)</span> 其似然函数为： <span class="math inline">\(L(\theta)=\prod_{i=1}^{m} P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}\)</span> 对数似然函数为： <span class="math inline">\(\ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\  =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]\)</span> 对数函数在单个数据点上的定义为： <span class="math inline">\(cost(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}\)</span> 则全局样本损失函数为： <span class="math inline">\(cost(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]\)</span> 由此可看出，对数损失函数与极大似然估计的对数似然函数本质上是相同的。所以逻辑回归直接采用对数损失函数。</p>
<h3 id="对数损失函数是如何度量损失的">2.11.4 对数损失函数是如何度量损失的</h3>
<p>​ 例如，在高斯分布中，我们需要确定均值和标准差。 ​ 如何确定这两个参数？最大似然估计是比较常用的方法。最大似然的目标是找到一些参数值，这些参数值对应的分布可以最大化观测到数据的概率。 ​ 因为需要计算观测到所有数据的全概率，即所有观测到的数据点的联合概率。现考虑如下简化情况：</p>
<p>（1）假设观测到每个数据点的概率和其他数据点的概率是独立的。</p>
<p>（2）取自然对数。 假设观测到单个数据点<span class="math inline">\(x_i(i=1,2,...n)\)</span>的概率为： <span class="math inline">\(P(x_i;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp  \left( - \frac{(x_i-\mu)^2}{2\sigma^2} \right)\)</span></p>
<p>（3）其联合概率为： <span class="math inline">\(P(x_1,x_2,...,x_n;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp  \left( - \frac{(x_1-\mu)^2}{2\sigma^2} \right) \\ \times  \frac{1}{\sigma \sqrt{2\pi}}\exp  \left( - \frac{(x_2-\mu)^2}{2\sigma^2} \right) \times ... \times  \frac{1}{\sigma \sqrt{2\pi}}\exp  \left( - \frac{(x_n-\mu)^2}{2\sigma^2} \right)\)</span> ​ 对上式取自然对数，可得： <span class="math inline">\(\ln(P(x_1,x_2,...x_n;\mu,\sigma))=  \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \right)  - \frac{(x_1-\mu)^2}{2\sigma^2} \\ +  \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right)  - \frac{(x_2-\mu)^2}{2\sigma^2} +...+  \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right)  - \frac{(x_n-\mu)^2}{2\sigma^2}\)</span> 根据对数定律，上式可以化简为： <span class="math inline">\(\ln(P(x_1,x_2,...x_n;\mu,\sigma))=-n\ln(\sigma)-\frac{n}{2} \ln(2\pi)\\  -\frac{1}{2\sigma^2}[(x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2]\)</span> 然后求导为： <span class="math inline">\(\frac{\partial\ln(P(x_1,x_2,...,x_n;\mu,\sigma))}{\partial\mu}=  \frac{n}{\sigma^2}[\mu - (x_1+x_2+...+x_n)]\)</span> ​ 上式左半部分为对数损失函数。损失函数越小越好，因此我们令等式左半的对数损失函数为0，可得： <span class="math inline">\(\mu=\frac{x_1+x_2+...+x_n}{n}\)</span> 同理，可计算$$。</p>
<h2 id="梯度下降">2.12 梯度下降</h2>
<h3 id="机器学习中为什么需要梯度下降">2.12.1 机器学习中为什么需要梯度下降</h3>
<p>梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：</p>
<p>（1）梯度下降是迭代法的一种，可以用于求解最小二乘问题。</p>
<p>（2）在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。</p>
<p>（3）在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</p>
<p>（4）如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。</p>
<p>（5）在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</p>
<h3 id="梯度下降法缺点">2.12.2 梯度下降法缺点</h3>
<p>梯度下降法缺点有以下几点：</p>
<p>（1）靠近极小值时收敛速度减慢。</p>
<p>（2）直线搜索时可能会产生一些问题。</p>
<p>（3）可能会“之字形”地下降。</p>
<p>梯度概念也有需注意的地方：</p>
<p>（1）梯度是一个向量，即有方向有大小。</p>
<p>（2）梯度的方向是最大方向导数的方向。</p>
<p>（3）梯度的值是最大方向导数的值。</p>
<h3 id="梯度下降法直观理解">2.12.3 梯度下降法直观理解</h3>
<p>梯度下降法经典图示如下图2.7所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231308.png" alt="image-20210915231308117" /><figcaption>image-20210915231308117</figcaption>
</figure>
<p>图2.7 梯度下降法经典图示</p>
<p>​ 形象化举例，由上图2.7所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。 ​ 由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><strong>核心思想归纳</strong>：</p>
<p>（1）初始化参数，随机选取取值范围内的任意数；</p>
<p>（2）迭代操作： a）计算当前梯度； b）修改新的变量； c）计算朝最陡的下坡方向走一步； d）判断是否需要终止，如否，返回a）；</p>
<p>（3）得到全局最优解或者接近全局最优解。</p>
<h3 id="梯度下降法算法描述">2.12.4 梯度下降法算法描述</h3>
<p>梯度下降法算法步骤如下：</p>
<p>（1）确定优化模型的假设函数及损失函数。 ​ 举例，对于线性回归，假设函数为： <span class="math inline">\(h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n\)</span> 其中，<span class="math inline">\(\theta_i,x_i(i=0,1,2,...,n)\)</span>分别为模型参数、每个样本的特征值。 对于假设函数，损失函数为： <span class="math inline">\(J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0  ,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2\)</span></p>
<p>（2）相关参数初始化。 ​ 主要初始化<span class="math inline">\({\theta}_i\)</span>、算法迭代步长${} <span class="math inline">\(、终止距离\)</span>{} <span class="math inline">\(。初始化时可以根据经验初始化，即\)</span>{} <span class="math inline">\(初始化为0，步长\)</span>{} <span class="math inline">\(初始化为1。当前步长记为\)</span>{}_i $。当然，也可随机初始化。</p>
<p>（3）迭代计算。</p>
<p>​ 1）计算当前位置时损失函数的梯度，对${}_i $，其梯度表示为： <span class="math inline">\(\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0 ​ ,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2\)</span> ​ 2）计算当前位置下降的距离。 <span class="math inline">\({\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)\)</span> ​ 3）判断是否终止。 ​ 确定是否所有<span class="math inline">\({\theta}_i\)</span>梯度下降的距离<span class="math inline">\({\varphi}_i\)</span>都小于终止距离<span class="math inline">\({\zeta}\)</span>，如果都小于<span class="math inline">\({\zeta}\)</span>，则算法终止，当然的值即为最终结果，否则进入下一步。 ​ 4）更新所有的<span class="math inline">\({\theta}_i\)</span>，更新后的表达式为： <span class="math inline">\({\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)\)</span></p>
<p><span class="math inline">\(\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0  ,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i\)</span></p>
<p>​ 5）令上式<span class="math inline">\(x^{(j)}_0=1\)</span>，更新完毕后转入1)。 ​ 由此，可看出，当前位置的梯度方向由所有样本决定，上式中 <span class="math inline">\(\frac{1}{m}\)</span>、<span class="math inline">\(\alpha \frac{1}{m}\)</span> 的目的是为了便于理解。</p>
<h3 id="如何对梯度下降法进行调优">2.12.5 如何对梯度下降法进行调优</h3>
<p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<p>（1）<strong>算法迭代步长<span class="math inline">\(\alpha\)</span>选择。</strong> 在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</p>
<p>（2）<strong>参数的初始值选择。</strong> 初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>（3）<strong>标准化处理。</strong> 由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</p>
<h3 id="随机梯度和批量梯度区别">2.12.6 随机梯度和批量梯度区别</h3>
<p>​ 随机梯度下降（SGD）和批量梯度下降（BGD）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。 下面通过介绍两种梯度下降法的求解思路，对其进行比较。 假设函数为： <span class="math inline">\(h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n\)</span> 损失函数为： <span class="math inline">\(J(\theta_0, \theta_1, ... , \theta_n) = ​ \frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0 ​ ,x^{j}_1,...,x^{j}_n)-y^j)^2\)</span> 其中，<span class="math inline">\(m\)</span>为样本个数，<span class="math inline">\(j\)</span>为参数个数。</p>
<p>1、 <strong>批量梯度下降的求解思路如下：</strong> a) 得到每个$ $对应的梯度： <span class="math inline">\(\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0  ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i\)</span> b) 由于是求最小化风险函数，所以按每个参数 $ $ 的梯度负方向更新 $ _i $ ： <span class="math inline">\(\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0  ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i\)</span> c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。 相比而言，随机梯度下降可避免这种问题。</p>
<p>2、<strong>随机梯度下降的求解思路如下：</strong> a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。 损失函数可以写成如下这种形式， <span class="math inline">\(J(\theta_0, \theta_1, ... , \theta_n) =  \frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0  ,x^{j}_1,...,x^{j}_n))^2 =  \frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))\)</span> b）对每个参数 $ $ 按梯度方向更新 $ $： <span class="math inline">\(\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))\)</span> c) 随机梯度下降是通过每个样本来迭代更新一次。 随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。</p>
<p><strong>小结：</strong> 随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">方法</th>
<th style="text-align: left;">特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">批量梯度下降</td>
<td style="text-align: left;">a）采用所有数据来梯度下降。<br/>b）批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr class="even">
<td style="text-align: center;">随机梯度下降</td>
<td style="text-align: left;">a）随机梯度下降用一个样本来梯度下降。<br/>b）训练速度很快。<br />c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<p>3、 <strong>小批量（Mini-Batch）梯度下降的求解思路如下</strong> 对于总数为<span class="math inline">\(m\)</span>个样本的数据，根据样本的数据，选取其中的<span class="math inline">\(n(1&lt; n&lt; m)\)</span>个子样本来迭代。其参数<span class="math inline">\(\theta\)</span>按梯度方向更新<span class="math inline">\(\theta_i\)</span>公式如下： <span class="math inline">\(\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}  ( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}\)</span></p>
<h3 id="各种梯度下降法性能比较">2.12.7 各种梯度下降法性能比较</h3>
<p>​ 下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">BGD</th>
<th style="text-align: center;">SGD</th>
<th style="text-align: center;">Mini-batch GD</th>
<th style="text-align: center;">Online GD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">训练集</td>
<td style="text-align: center;">固定</td>
<td style="text-align: center;">固定</td>
<td style="text-align: center;">固定</td>
<td style="text-align: center;">实时更新</td>
</tr>
<tr class="even">
<td style="text-align: center;">单次迭代样本数</td>
<td style="text-align: center;">整个训练集</td>
<td style="text-align: center;">单个样本</td>
<td style="text-align: center;">训练集的子集</td>
<td style="text-align: center;">根据具体算法定</td>
</tr>
<tr class="odd">
<td style="text-align: center;">算法复杂度</td>
<td style="text-align: center;">高</td>
<td style="text-align: center;">低</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">低</td>
</tr>
<tr class="even">
<td style="text-align: center;">时效性</td>
<td style="text-align: center;">低</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">高</td>
</tr>
<tr class="odd">
<td style="text-align: center;">收敛性</td>
<td style="text-align: center;">稳定</td>
<td style="text-align: center;">不稳定</td>
<td style="text-align: center;">较稳定</td>
<td style="text-align: center;">不稳定</td>
</tr>
</tbody>
</table>
<p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。</p>
<p>​ Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>​ Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h2 id="自然梯度法">2.13 自然梯度法</h2>
<p><strong>（贡献者：郜泉凯－华南理工大学）</strong></p>
<h3 id="为什么我们需要自然梯度">2.13.1 为什么我们需要自然梯度</h3>
<p>传统的梯度下降方法是在欧氏空间进行、并与时序过程结合的优化方法，但这样的更新过程无法度量由于参数变化引起的概率属性的变化（这一点也可以认为是传统梯度下降方法的缺点）。在如强化学习等很多应用领域关注模型输出的概率分布，优化过程常常需要在一定概率属性的约束下完成，这就需要自然梯度。</p>
<h3 id="如何定义自然梯度">2.12.2 如何定义自然梯度</h3>
<p>若度量模型参数变化引起的概率分布变化，常用的“距离”度量是KL散度（Kullback-Leibler divergence）。设模型概率分布为<span class="math inline">\(p(x;\theta)\)</span>，其与参数变动后的概率分布间的KL散度为： <span class="math inline">\(D_{KL}(p(x;\theta)||p(x;\theta+\delta\theta))=\int p(x;\theta)log\frac {p(x;\theta)}{p(x;\theta+\delta\theta)}dx\)</span> 我们令<span class="math inline">\(f(\theta+\delta\theta)=log p(x;\theta+\delta\theta)\)</span>，做泰勒展开取二阶近似（忽略高阶余项）得到： <span class="math inline">\(f(\theta+\delta\theta)\approx f(\theta)+\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}+\frac{1}{2}\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}\delta\theta\)</span> 带入到<span class="math inline">\(D_{KL}(p(x;\theta)||p(x;\theta+\delta\theta))\)</span>中可得到： <span class="math inline">\(\begin{eqnarray} D_{KL}(p(x;\theta)||p(x;\theta+\delta\theta))&amp;=&amp;\int p(x;\theta)(f(\theta)-f(\theta+\delta\theta))dx\\ &amp;=&amp;-\int p(x;\theta)(\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}+\frac{1}{2}\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}\delta\theta)dx\\ &amp;=&amp;-\delta\theta^T\int p(x;\theta)\frac{\partial logp(x;\theta)}{\partial\theta}dx\\ &amp;-&amp;\frac{1}{2}\delta\theta^T\int p(x;\theta)\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}dx\delta\theta\\ &amp;=&amp;-\delta\theta^T\int p(x;\theta)\frac{\frac{\partial p(x;\theta)}{\partial\theta}}{p(x;\theta)}dx-\frac{1}{2}\delta\theta^TG\delta\theta\\ &amp;=&amp;-\frac{1}{2}\delta\theta^TG\delta\theta \end{eqnarray}\)</span> 我们记在KL散度意义下的参数增量为<span class="math inline">\(\delta\theta_G\)</span>，接下来我们寻求在<span class="math inline">\(||\delta\theta_G||^2=\epsilon\)</span>约束下<span class="math inline">\(\delta\theta_G\)</span>的方向，使得目标函数<span class="math inline">\(J(\theta)\)</span>下降最快,即<span class="math inline">\(J(\theta+\delta\theta)-J(\theta)\)</span>最大。应用拉格朗日乘子法： <span class="math inline">\(\max_{\delta\theta}J(\theta+\delta\theta)-J(\theta)-\lambda(||\delta\theta_G||^2-\epsilon)\)</span> 应用一阶泰勒展开等价于: <span class="math inline">\(\max_{\delta\theta}\nabla \delta\theta^T J(\theta)-\frac{1}{2}\lambda\delta\theta^TG\delta\theta\)</span> 对<span class="math inline">\(\delta\theta\)</span>求导得<span class="math inline">\(\nabla J(\theta)-\lambda G\delta\theta=0\)</span>，即<span class="math inline">\(\delta\theta=\frac{1}{\lambda}G^{-1}\nabla J(\theta)\)</span>，其中<span class="math inline">\(G^{-1}\nabla J(\theta)\)</span>称为自然梯度，相应的自然梯度下降公式为<span class="math inline">\(\theta_{k+1}=\theta_k-\alpha_kG^{-1}(\theta_k)\nabla J(\theta_K)\)</span>。</p>
<h3 id="fisher信息矩阵的意义">2.12.3 Fisher信息矩阵的意义</h3>
<p>首先我们对一个模型进行建模，成为以<span class="math inline">\(\theta\)</span>为参数的概率分布<span class="math inline">\(p(x;\theta)\)</span>。为求出一个合理的<span class="math inline">\(\theta\)</span>我们需要一个评分函数（score function）：<span class="math inline">\(s(\theta)=\nabla_{\theta}logp(x;\theta)\)</span>，意为对数似然的梯度，当分数为0时（对数似然梯度为0），对数似然达到极值。对评分函数求关于<span class="math inline">\(p(x;\theta)\)</span>数学期望<span class="math inline">\(p_E\)</span>不难发现期望为0。接下来求估计误差的界，我们用评分函数的方差来确定，即<span class="math inline">\(E_{p(x;\theta)}[(s(\theta)-p_E)(s(\theta-p_E)^T)]\)</span>。带入评分函数的数学表达形式则等价于Fisher信息矩阵<span class="math inline">\(G(\theta)=\int p(x;\theta)\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}dx\)</span>。特别地，Fisher信息矩阵与评分函数<span class="math inline">\(\nabla_{\theta}logp(x;\theta)\)</span>的Hessian似然的负数等价。</p>
<p>证明：首先求出评分函数的Hessian矩阵，由梯度的Jacobian决定 <span class="math inline">\(\begin{eqnarray} H_{logp(x;\theta)}&amp;=&amp;J(\frac{\nabla p(x;\theta)}{p(x;\theta)})\\ &amp;=&amp;\frac{\frac{\partial\nabla p(x;\theta)}{\partial\theta}p(x;\theta)-\nabla p(x;\theta)\nabla p(x;\theta)^T}{p(x;\theta)p(x;\theta)}\\ &amp;=&amp;\frac{H_{p(x;\theta)}p(x;\theta)}{p(x;\theta)p(x;\theta)}-\frac{\nabla p(x;\theta)\nabla p(x;\theta)^T}{p(x;\theta)p(x;\theta)}\\ \end{eqnarray}\)</span> 等式两边同时求关于<span class="math inline">\(p(x;\theta)\)</span>的数学期望： <span class="math inline">\(\begin{eqnarray} E_{p(x;\theta)}[H_{logp(x;\theta)}] &amp;=&amp; E_{p(x;\theta)}(\frac{H_{p(x;\theta)}p(x;\theta)}{p(x;\theta)p(x;\theta)})-G\\ &amp;=&amp;\int\frac{H_{p(x;\theta)}}{p(x;\theta)}p(x;\theta)dx-G\\ &amp;=&amp;\nabla^2\int p(x;\theta)dx-G\\ &amp;=&amp;-G \end{eqnarray}\)</span> 而Hessian矩阵刻画着对数似然函数的曲率，所以本质上自然梯度下降法是在一个消除了不同概率分布的曲率后，在同一个“平坦”曲面上进行迭代更新，步长等于原概率分布空间的步长按照曲率折合到新的“平坦曲面”的大小。</p>
<p>值得注意的一点是，一般来说似然函数获取很难，在实际问题中，我们可以用采样的方法从数据集中采样数据，将Fisher信息矩阵原始表达式的积分变为求和来近似估计，这样的方式得到的Fisher信息矩阵称为经验Fisher。</p>
<h2 id="线性判别分析lda">2.14 线性判别分析（LDA）</h2>
<h3 id="lda思想总结">2.14.1 LDA思想总结</h3>
<p>​ 线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</p>
<p>LDA分类思想简单总结如下：</p>
<ol type="1">
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。<br />
</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。<br />
</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p>如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</p>
<h3 id="图解lda核心思想">2.14.2 图解LDA核心思想</h3>
<p>​ 假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231340.png" alt="image-20210915231340517" /><figcaption>image-20210915231340517</figcaption>
</figure>
<p>左图和右图是两种不同的投影方式。</p>
<p>​ 左图思路：让不同类别的平均点距离最远的投影方式。</p>
<p>​ 右图思路：让同类别的数据挨得最近的投影方式。</p>
<p>​ 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。</p>
<p>​ 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3 id="二类lda算法原理">2.14.3 二类LDA算法原理</h3>
<p>​ 输入：数据集 <span class="math inline">\(D=\{(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),...,(\boldsymbol x_m,\boldsymbol y_m)\}\)</span>，其中样本 $x_i $ 是n维向量，<span class="math inline">\(\boldsymbol y_i \epsilon \{0, 1\}\)</span>，降维后的目标维度 <span class="math inline">\(d\)</span>。定义</p>
<p>​ <span class="math inline">\(N_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本个数；</p>
<p>​ <span class="math inline">\(X_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本的集合；</p>
<p>​ <span class="math inline">\(u_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本的均值向量；</p>
<p>​ <span class="math inline">\(\sum_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本的协方差矩阵。</p>
<p>​ 其中 <span class="math inline">\(u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x(j=0,1)， \sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)\)</span> ​ 假设投影直线是向量 <span class="math inline">\(\boldsymbol w\)</span>，对任意样本 <span class="math inline">\(\boldsymbol x_i\)</span>，它在直线 <span class="math inline">\(w\)</span>上的投影为 <span class="math inline">\(\boldsymbol w^Tx_i\)</span>，两个类别的中心点 <span class="math inline">\(u_0\)</span>, $u_1 $在直线 <span class="math inline">\(w\)</span> 的投影分别为 <span class="math inline">\(\boldsymbol w^Tu_0\)</span> 、<span class="math inline">\(\boldsymbol w^Tu_1\)</span>。</p>
<p>​ LDA的目标是让两类别的数据中心间的距离 <span class="math inline">\(\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2\)</span> 尽量大，与此同时，希望同类样本投影点的协方差<span class="math inline">\(\boldsymbol w^T \sum_0 \boldsymbol w\)</span>、<span class="math inline">\(\boldsymbol w^T \sum_1 \boldsymbol w\)</span> 尽量小，最小化 <span class="math inline">\(\boldsymbol w^T \sum_0 \boldsymbol w + \boldsymbol w^T \sum_1 \boldsymbol w\)</span> 。 ​ 定义 ​ 类内散度矩阵 <span class="math inline">\(S_w = \sum_0 + \sum_1 = ​ \sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T + ​ \sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T\)</span> ​ 类间散度矩阵 <span class="math inline">\(S_b = (u_0 - u_1)(u_0 - u_1)^T\)</span></p>
<p>​ 据上分析，优化目标为 <span class="math inline">\(\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} = \frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} = \frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}\)</span> ​ 根据广义瑞利商的性质，矩阵 <span class="math inline">\(S^{-1}_{w} S_b\)</span> 的最大特征值为 <span class="math inline">\(J(\boldsymbol w)\)</span> 的最大值，矩阵 <span class="math inline">\(S^{-1}_{w} S_b\)</span> 的最大特征值对应的特征向量即为 <span class="math inline">\(\boldsymbol w\)</span>。</p>
<h3 id="lda算法流程总结">2.14.4 LDA算法流程总结</h3>
<p>LDA算法降维流程如下：</p>
<p>​ 输入：数据集 <span class="math inline">\(D = \{ (x_1,y_1),(x_2,y_2), ... ,(x_m,y_m) \}\)</span>，其中样本 $x_i $ 是n维向量，<span class="math inline">\(y_i \epsilon \{C_1, C_2, ..., C_k\}\)</span>，降维后的目标维度 <span class="math inline">\(d\)</span> 。</p>
<p>​ 输出：降维后的数据集 $ $ 。</p>
<p>步骤：</p>
<ol type="1">
<li>计算类内散度矩阵 <span class="math inline">\(S_w\)</span>。</li>
<li>计算类间散度矩阵 <span class="math inline">\(S_b\)</span> 。</li>
<li>计算矩阵 <span class="math inline">\(S^{-1}_wS_b\)</span> 。</li>
<li>计算矩阵 <span class="math inline">\(S^{-1}_wS_b\)</span> 的最大的 d 个特征值。</li>
<li>计算 d 个特征值对应的 d 个特征向量，记投影矩阵为 W 。</li>
<li>转化样本集的每个样本，得到新样本 <span class="math inline">\(P_i = W^Tx_i\)</span> 。</li>
<li>输出新样本集 <span class="math inline">\(\overline{D} = \{ (p_1,y_1),(p_2,y_2),...,(p_m,y_m) \}\)</span></li>
</ol>
<h3 id="lda和pca区别">2.14.5 LDA和PCA区别</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">异同点</th>
<th style="text-align: left;">LDA</th>
<th style="text-align: left;">PCA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">相同点</td>
<td style="text-align: left;">1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">不同点</td>
<td style="text-align: left;">有监督的降维方法；</td>
<td style="text-align: left;">无监督的降维方法；</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: left;">降维最多降到k-1维；</td>
<td style="text-align: left;">降维多少没有限制；</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: left;">可以用于降维，还可以用于分类；</td>
<td style="text-align: left;">只用于降维；</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: left;">选择分类性能最好的投影方向；</td>
<td style="text-align: left;">选择样本点投影具有最大方差的方向；</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: left;">更明确，更能反映样本间差异；</td>
<td style="text-align: left;">目的较为模糊；</td>
</tr>
</tbody>
</table>
<h3 id="lda优缺点">2.14.6 LDA优缺点</h3>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 90%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">优缺点</th>
<th style="text-align: left;">简要说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">优点</td>
<td style="text-align: left;">1. 可以使用类别的先验知识；<br />2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr class="even">
<td style="text-align: center;">缺点</td>
<td style="text-align: left;">1. LDA不适合对非高斯分布样本进行降维；<br />2. LDA降维最多降到分类数k-1维；<br />3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br />4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
<h2 id="主成分分析pca">2.15 主成分分析（PCA）</h2>
<h3 id="主成分分析pca思想总结">2.15.1 主成分分析（PCA）思想总结</h3>
<ol type="1">
<li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li>
<li>投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。</li>
<li>去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。</li>
<li>去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。</li>
<li>对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。</li>
<li>完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。</li>
<li>之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。</li>
</ol>
<h3 id="图解pca核心思想">2.15.2 图解PCA核心思想</h3>
<p>​ PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n &lt; m），这n维形成主元，是重构出来最能代表原始数据的正交特征。</p>
<p>​ 假设数据集是m个n维，<span class="math inline">\((\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})\)</span>。如果<span class="math inline">\(n=2\)</span>，需要降维到<span class="math inline">\(n&#39;=1\)</span>，现在想找到某一维度方向代表这两个维度的数据。下图有<span class="math inline">\(u_1, u_2\)</span>两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231401.png" alt="image-20210915231400995" /><figcaption>image-20210915231400995</figcaption>
</figure>
<p>从图可看出，<span class="math inline">\(u_1\)</span>比<span class="math inline">\(u_2\)</span>好，为什么呢？有以下两个主要评价指标：</p>
<ol type="1">
<li>样本点到这个直线的距离足够近。</li>
<li>样本点在这个直线上的投影能尽可能的分开。</li>
</ol>
<p>如果我们需要降维的目标维数是其他任意维，则：</p>
<ol type="1">
<li>样本点到这个超平面的距离足够近。</li>
<li>样本点在这个超平面上的投影能尽可能的分开。</li>
</ol>
<h3 id="pca算法推理">2.15.3 PCA算法推理</h3>
<p>下面以基于最小投影距离为评价指标推理：</p>
<p>​ 假设数据集是m个n维，<span class="math inline">\((x^{(1)}, x^{(2)},...,x^{(m)})\)</span>，且数据进行了中心化。经过投影变换得到新坐标为 <span class="math inline">\({w_1,w_2,...,w_n}\)</span>，其中 <span class="math inline">\(w\)</span> 是标准正交基，即 <span class="math inline">\(\| w \|_2 = 1\)</span>，<span class="math inline">\(w^T_iw_j = 0\)</span>。</p>
<p>​ 经过降维后，新坐标为 <span class="math inline">\(\{ w_1,w_2,...,w_n \}\)</span>，其中 <span class="math inline">\(n&#39;\)</span> 是降维后的目标维数。样本点 <span class="math inline">\(x^{(i)}\)</span> 在新坐标系下的投影为 <span class="math inline">\(z^{(i)} = \left(z^{(i)}_1, z^{(i)}_2, ..., z^{(i)}_{n&#39;} \right)\)</span>，其中 <span class="math inline">\(z^{(i)}_j = w^T_j x^{(i)}\)</span> 是 $x^{(i)} $ 在低维坐标系里第 j 维的坐标。</p>
<p>​ 如果用 $z^{(i)} $ 去恢复 $x^{(i)} $ ，则得到的恢复数据为 <span class="math inline">\(\widehat{x}^{(i)} = \sum^{n&#39;}_{j=1} x^{(i)}_j w_j = Wz^{(i)}\)</span>，其中 <span class="math inline">\(W\)</span>为标准正交基组成的矩阵。</p>
<p>​ 考虑到整个样本集，样本点到这个超平面的距离足够近，目标变为最小化 <span class="math inline">\(\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2\)</span> 。对此式进行推理，可得： <span class="math inline">\(\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2 = ​ \sum^m_{i=1} \| Wz^{(i)} - x^{(i)} \|^2_2 \\ ​ = \sum^m_{i=1} \left( Wz^{(i)} \right)^T \left( Wz^{(i)} \right) ​ - 2\sum^m_{i=1} \left( Wz^{(i)} \right)^T x^{(i)} ​ + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\ ​ = \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right) ​ - 2\sum^m_{i=1} \left( z^{(i)} \right)^T x^{(i)} ​ + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\ ​ = - \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right) ​ + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\ ​ = -tr \left( W^T \left( \sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T \right)W \right) ​ + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\ ​ = -tr \left( W^TXX^TW \right) ​ + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)}\)</span></p>
<p>​ 在推导过程中，分别用到了 <span class="math inline">\(\overline{x}^{(i)} = Wz^{(i)}\)</span> ，矩阵转置公式 <span class="math inline">\((AB)^T = B^TA^T\)</span>，<span class="math inline">\(W^TW = I\)</span>，<span class="math inline">\(z^{(i)} = W^Tx^{(i)}\)</span> 以及矩阵的迹，最后两步是将代数和转为矩阵形式。 ​ 由于 <span class="math inline">\(W\)</span> 的每一个向量 <span class="math inline">\(w_j\)</span> 是标准正交基，<span class="math inline">\(\sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T\)</span> 是数据集的协方差矩阵，$^m_{i=1} ( x^{(i)} )^T x^{(i)} $ 是一个常量。最小化 <span class="math inline">\(\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2\)</span> 又可等价于 <span class="math inline">\(\underbrace{\arg \min}_W - tr \left( W^TXX^TW \right) s.t.W^TW = I\)</span> 利用拉格朗日函数可得到 <span class="math inline">\(J(W) = -tr(W^TXX^TW) + \lambda(W^TW - I)\)</span> ​ 对 <span class="math inline">\(W\)</span> 求导，可得 $-XX^TW + W = 0 $ ，也即 $ XX^TW = W $ 。 $ XX^T $ 是 $ n' $ 个特征向量组成的矩阵，<span class="math inline">\(\lambda\)</span> 为$ XX^T $ 的特征值。<span class="math inline">\(W\)</span> 即为我们想要的矩阵。 ​ 对于原始数据，只需要 <span class="math inline">\(z^{(i)} = W^TX^{(i)}\)</span> ，就可把原始数据集降维到最小投影距离的 <span class="math inline">\(n&#39;\)</span> 维数据集。</p>
<p>​ 基于最大投影方差的推导，这里就不再赘述，有兴趣的同仁可自行查阅资料。</p>
<h3 id="pca算法流程总结">2.15.4 PCA算法流程总结</h3>
<p>输入：<span class="math inline">\(n\)</span> 维样本集 <span class="math inline">\(D = \left( x^{(1)},x^{(2)},...,x^{(m)} \right)\)</span> ，目标降维的维数 <span class="math inline">\(n&#39;\)</span> 。</p>
<p>输出：降维后的新样本集 <span class="math inline">\(D&#39; = \left( z^{(1)},z^{(2)},...,z^{(m)} \right)\)</span> 。</p>
<p>主要步骤如下：</p>
<ol type="1">
<li>对所有的样本进行中心化，$ x^{(i)} = x^{(i)} -  ^m_{j=1} x^{(j)} $ 。</li>
<li>计算样本的协方差矩阵 <span class="math inline">\(XX^T\)</span> 。</li>
<li>对协方差矩阵 <span class="math inline">\(XX^T\)</span> 进行特征值分解。</li>
<li>取出最大的 $n' $ 个特征值对应的特征向量 <span class="math inline">\(\{ w_1,w_2,...,w_{n&#39;} \}\)</span> 。</li>
<li>标准化特征向量，得到特征向量矩阵 <span class="math inline">\(W\)</span> 。</li>
<li>转化样本集中的每个样本 <span class="math inline">\(z^{(i)} = W^T x^{(i)}\)</span> 。</li>
<li>得到输出矩阵 <span class="math inline">\(D&#39; = \left( z^{(1)},z^{(2)},...,z^{(n)} \right)\)</span> 。 <em>注</em>：在降维时，有时不明确目标维数，而是指定降维到的主成分比重阈值 <span class="math inline">\(k(k \epsilon(0,1])\)</span> 。假设 <span class="math inline">\(n\)</span> 个特征值为 <span class="math inline">\(\lambda_1 \geqslant \lambda_2 \geqslant ... \geqslant \lambda_n\)</span> ，则 <span class="math inline">\(n&#39;\)</span> 可从 $^{n'}_{i=1} <em>i k ^n</em>{i=1} _i $ 得到。</li>
</ol>
<h3 id="pca算法主要优缺点">2.15.5 PCA算法主要优缺点</h3>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 90%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">优缺点</th>
<th style="text-align: left;">简要说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">优点</td>
<td style="text-align: left;">1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。</td>
</tr>
<tr class="even">
<td style="text-align: center;">缺点</td>
<td style="text-align: left;">1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</td>
</tr>
</tbody>
</table>
<h3 id="降维的必要性及目的">2.15.6 降维的必要性及目的</h3>
<p><strong>降维的必要性</strong>：</p>
<ol type="1">
<li>多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。</li>
<li>高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。</li>
<li>过多的变量，对查找规律造成冗余麻烦。</li>
<li>仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。</li>
</ol>
<p><strong>降维的目的</strong>：</p>
<ol type="1">
<li>减少预测变量的个数。</li>
<li>确保这些变量是相互独立的。</li>
<li>提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。</li>
<li>数据在低维下更容易处理、更容易使用。</li>
<li>去除数据噪声。</li>
<li>降低算法运算开销。</li>
</ol>
<h3 id="kpca与pca的区别">2.15.7 KPCA与PCA的区别</h3>
<p>​ 应用PCA算法前提是假设存在一个线性超平面，进而投影。那如果数据不是线性的呢？该怎么办？这时候就需要KPCA，数据集从 <span class="math inline">\(n\)</span> 维映射到线性可分的高维 <span class="math inline">\(N &gt;n\)</span>，然后再从 <span class="math inline">\(N\)</span> 维降维到一个低维度 <span class="math inline">\(n&#39;(n&#39;&lt;n&lt;N)\)</span> 。</p>
<p>​ KPCA用到了核函数思想，使用了核函数的主成分分析一般称为核主成分分析(Kernelized PCA, 简称KPCA）。</p>
<p>假设高维空间数据由 <span class="math inline">\(n\)</span> 维空间的数据通过映射 <span class="math inline">\(\phi\)</span> 产生。</p>
<p>​ <span class="math inline">\(n\)</span> 维空间的特征分解为： <span class="math inline">\(\sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T W = \lambda W\)</span></p>
<p>​ 其映射为 <span class="math inline">\(\sum^m_{i=1} \phi \left( x^{(i)} \right) \phi \left( x^{(i)} \right)^T W = \lambda W\)</span></p>
<p>​ 通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。</p>
<h2 id="模型评估">2.16 模型评估</h2>
<h3 id="模型评估常用方法">2.16.1 模型评估常用方法？</h3>
<p>​ 一般情况来说，单一评分标准无法完全评估一个机器学习模型。只用good和bad偏离真实场景去评估某个模型，都是一种欠妥的评估方式。下面介绍常用的分类模型和回归模型评估方法。</p>
<p><strong>分类模型常用评估方法：</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">指标</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">准确率</td>
</tr>
<tr class="even">
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">精准度/查准率</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">召回率/查全率</td>
</tr>
<tr class="even">
<td style="text-align: center;">P-R曲线</td>
<td style="text-align: center;">查准率为纵轴，查全率为横轴，作图</td>
</tr>
<tr class="odd">
<td style="text-align: center;">F1</td>
<td style="text-align: center;">F1值</td>
</tr>
<tr class="even">
<td style="text-align: center;">Confusion Matrix</td>
<td style="text-align: center;">混淆矩阵</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ROC</td>
<td style="text-align: center;">ROC曲线</td>
</tr>
<tr class="even">
<td style="text-align: center;">AUC</td>
<td style="text-align: center;">ROC曲线下的面积</td>
</tr>
</tbody>
</table>
<p><strong>回归模型常用评估方法：</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">指标</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mean Square Error (MSE, RMSE)</td>
<td style="text-align: center;">平均方差</td>
</tr>
<tr class="even">
<td style="text-align: center;">Absolute Error (MAE, RAE)</td>
<td style="text-align: center;">绝对误差</td>
</tr>
<tr class="odd">
<td style="text-align: center;">R-Squared</td>
<td style="text-align: center;">R平方值</td>
</tr>
</tbody>
</table>
<h3 id="误差偏差和方差有什么区别和联系">2.16.2 误差、偏差和方差有什么区别和联系</h3>
<p>在机器学习中，Bias(偏差)，Error(误差)，和Variance(方差)存在以下区别和联系：</p>
<p><strong>对于Error </strong>：</p>
<ul>
<li><p>误差（error）：一般地，我们把学习器的实际预测输出与样本的真是输出之间的差异称为“误差”。</p></li>
<li><p>Error = Bias + Variance + Noise，Error反映的是整个模型的准确度。</p></li>
</ul>
<p><strong>对于Noise:</strong></p>
<p>噪声：描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
<p><strong>对于Bias：</strong></p>
<ul>
<li>Bias衡量模型拟合训练数据的能力（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度。</li>
<li>Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。</li>
<li>偏差越大，越偏离真实数据，如下图第二行所示。</li>
</ul>
<p><strong>对于Variance：</strong></p>
<ul>
<li><p>方差公式：<span class="math inline">\(S_{N}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}\)</span></p></li>
<li>Variance描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。</li>
<li>Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</li>
<li>Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。</li>
<li><p>如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。</p></li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231432.png" alt="image-20210915231432098" /><figcaption>image-20210915231432098</figcaption>
</figure>
<h3 id="经验误差与泛化误差">2.16.3 经验误差与泛化误差</h3>
<p>经验误差（empirical error）：也叫训练误差（training error），模型在训练集上的误差。</p>
<p>泛化误差（generalization error）：模型在新样本集（测试集）上的误差称为“泛化误差”。</p>
<h3 id="图解欠拟合过拟合">2.16.4 图解欠拟合、过拟合</h3>
<p>根据不同的坐标方式，欠拟合与过拟合图解不同。</p>
<ol type="1">
<li><strong>横轴为训练样本数量，纵轴为误差</strong></li>
</ol>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231455.png" alt="image-20210915231455337" /><figcaption>image-20210915231455337</figcaption>
</figure>
<p>如上图所示，我们可以直观看出欠拟合和过拟合的区别：</p>
<p>​ 模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大；</p>
<p>​ 模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</p>
<p>​ 模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。</p>
<ol start="2" type="1">
<li><strong>横轴为模型复杂程度，纵轴为误差</strong></li>
</ol>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231514.png" alt="image-20210915231514461" /><figcaption>image-20210915231514461</figcaption>
</figure>
<p>红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​ 模型欠拟合：模型在点A处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​ 模型过拟合：模型在点C处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</p>
<p>​ 模型正常：模型复杂程度控制在点B处为最优。</p>
<ol start="3" type="1">
<li><strong>横轴为正则项系数，纵轴为误差</strong></li>
</ol>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231530.png" alt="image-20210915231530888" /><figcaption>image-20210915231530888</figcaption>
</figure>
<p>红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​ 模型欠拟合：模型在点C处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​ 模型过拟合：模型在点A处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。 它通常发生在模型过于复杂的情况下，如参数过多等，会使得模型的预测性能变弱，并且增加数据的波动性。虽然模型在训练时的效果可以表现的很完美，基本上记住了数据的全部特点，但这种模型在未知数据的表现能力会大减折扣，因为简单的模型泛化能力通常都是很弱的。</p>
<p>​ 模型正常：模型复杂程度控制在点B处为最优。</p>
<h3 id="如何解决过拟合与欠拟合">2.16.5 如何解决过拟合与欠拟合</h3>
<p><strong>如何解决欠拟合：</strong></p>
<ol type="1">
<li>添加其他特征项。组合、泛化、相关性、上下文特征、平台特征等特征是特征添加的重要手段，有时候特征项不够会导致模型欠拟合。</li>
<li>添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。例如，FM（Factorization Machine）模型、FFM（Field-aware Factorization Machine）模型，其实就是线性模型，增加了二阶多项式，保证了模型一定的拟合程度。</li>
<li>可以增加模型的复杂程度。</li>
<li>减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</li>
</ol>
<p><strong>如何解决过拟合：</strong></p>
<ol type="1">
<li>重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据。</li>
<li>增加训练样本数量。</li>
<li>降低模型复杂程度。</li>
<li>增大正则项系数。</li>
<li>采用dropout方法，dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作。</li>
<li>early stopping。</li>
<li>减少迭代次数。</li>
<li>增大学习率。</li>
<li>添加噪声数据。</li>
<li>树结构中，可以对树进行剪枝。</li>
<li>减少特征项。</li>
</ol>
<p>欠拟合和过拟合这些方法，需要根据实际问题，实际模型，进行选择。</p>
<h3 id="交叉验证的主要作用">2.16.6 交叉验证的主要作用</h3>
<p>​ 为了得到更为稳健可靠的模型，对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。</p>
<p>​ 交叉验证的方法有许多种，但是最常用的是：留一交叉验证、k折交叉验证。</p>
<h3 id="理解k折交叉验证">2.16.7 理解k折交叉验证</h3>
<ol type="1">
<li>将含有N个样本的数据集，分成K份，每份含有N/K个样本。选择其中1份作为测试集，另外K-1份作为训练集，测试集就有K种情况。</li>
<li>在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差。</li>
<li>交叉验证重复K次，每份验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测，得到模型最终的泛化误差。</li>
<li>将K种情况下，模型的泛化误差取均值，得到模型最终的泛化误差。<br />
</li>
<li>一般<span class="math inline">\(2\leqslant K \leqslant10\)</span>。 k折交叉验证的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</li>
<li>训练集中样本数量要足够多，一般至少大于总样本数的50%。</li>
<li>训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。</li>
</ol>
<h3 id="混淆矩阵">2.16.8 混淆矩阵</h3>
<p>第一种混淆矩阵:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">真实情况T or F</th>
<th style="text-align: left;">预测为正例1，P</th>
<th style="text-align: left;">预测为负例0，N</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">本来label标记为1，预测结果真为T、假为F</td>
<td style="text-align: left;">TP(预测为1，实际为1)</td>
<td style="text-align: left;">FN(预测为0，实际为1)</td>
</tr>
<tr class="even">
<td style="text-align: center;">本来label标记为0，预测结果真为T、假为F</td>
<td style="text-align: left;">FP(预测为1，实际为0)</td>
<td style="text-align: left;">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
<p>第二种混淆矩阵:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">预测情况P or N</th>
<th style="text-align: left;">实际label为1,预测对了为T</th>
<th style="text-align: left;">实际label为0,预测对了为T</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">预测为正例1，P</td>
<td style="text-align: left;">TP(预测为1，实际为1)</td>
<td style="text-align: left;">FP(预测为1，实际为0)</td>
</tr>
<tr class="even">
<td style="text-align: center;">预测为负例0，N</td>
<td style="text-align: left;">FN(预测为0，实际为1)</td>
<td style="text-align: left;">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
<h3 id="错误率及精度">2.16.9 错误率及精度</h3>
<ol type="1">
<li>错误率（Error Rate）：分类错误的样本数占样本总数的比例。</li>
<li>精度（accuracy）：分类正确的样本数占样本总数的比例。</li>
</ol>
<h3 id="查准率与查全率">2.16.10 查准率与查全率</h3>
<p>将算法预测的结果分成四种情况：</p>
<ol type="1">
<li>正确肯定（True Positive,TP）：预测为真，实际为真</li>
<li>正确否定（True Negative,TN）：预测为假，实际为假</li>
<li>错误肯定（False Positive,FP）：预测为真，实际为假</li>
<li>错误否定（False Negative,FN）：预测为假，实际为真</li>
</ol>
<p>则：</p>
<p>查准率（Precision）=TP/（TP+FP）</p>
<p><strong>理解</strong>：预测出为阳性的样本中，正确的有多少。区别准确率（正确预测出的样本，包括正确预测为阳性、阴性，占总样本比例）。 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p>
<p>查全率（Recall）=TP/（TP+FN）</p>
<p><strong>理解</strong>：正确预测为阳性的数量占总样本中阳性数量的比例。 例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
<h3 id="roc与auc">2.16.11 ROC与AUC</h3>
<p>​ ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。</p>
<p>​ ROC曲线的面积就是AUC（Area Under Curve）。</p>
<p>​ AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。</p>
<p>​ ROC曲线，通过将连续变量设定出多个不同的临界值，从而计算出一系列真正率和假正率，再以假正率为横坐标、真正率为纵坐标绘制成曲线，曲线下面积越大，推断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为假正率和真正率均较高的临界值。</p>
<p>​ 对于分类器，或者说分类算法，评价指标主要有Precision，Recall，F-score。下图是一个ROC曲线的示例。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231557.png" alt="image-20210915231556957" /><figcaption>image-20210915231556957</figcaption>
</figure>
<p>ROC曲线的横坐标为False Positive Rate（FPR），纵坐标为True Positive Rate（TPR）。其中 <span class="math inline">\(TPR = \frac{TP}{TP+FN} ,FPR = \frac{FP}{FP+TN}\)</span></p>
<p>​ 下面着重介绍ROC曲线图中的四个点和一条线。 ​ 第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False Negative）=0，并且FP（False Positive）=0。意味着这是一个完美的分类器，它将所有的样本都正确分类。 ​ 第二个点(1,0)，即FPR=1，TPR=0，意味着这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。 ​ 第三个点(0,0)，即FPR=TPR=0，即FP（False Positive）=TP（True Positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。 ​ 第四个点(1,1)，即FPR=TPR=1，分类器实际上预测所有的样本都为正样本。 ​ 经过以上分析，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>​ ROC曲线所覆盖的面积称为AUC（Area Under Curve），可以更直观的判断学习器的性能，AUC越大则性能越好。</p>
<h3 id="如何画roc曲线">2.16.12 如何画ROC曲线</h3>
<p>​ 下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。</p>
<p>步骤： 1、假设已经得出一系列样本被划分为正类的概率，按照大小排序。 2、从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。 3、每次选取一个不同的threshold，得到一组FPR和TPR，即ROC曲线上的一点。以此共得到20组FPR和TPR的值。 4、根据3、中的每个坐标点，画图。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231617.png" alt="image-20210915231617926" /><figcaption>image-20210915231617926</figcaption>
</figure>
<h3 id="如何计算tprfpr">2.16.13 如何计算TPR，FPR</h3>
<p>1、分析数据 y_true = [0, 0, 1, 1]；scores = [0.1, 0.4, 0.35, 0.8]； 2、列表</p>
<table>
<thead>
<tr class="header">
<th>样本</th>
<th>预测属于P的概率(score)</th>
<th>真实类别</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>y[0]</td>
<td>0.1</td>
<td>N</td>
</tr>
<tr class="even">
<td>y[1]</td>
<td>0.4</td>
<td>N</td>
</tr>
<tr class="odd">
<td>y[2]</td>
<td>0.35</td>
<td>P</td>
</tr>
<tr class="even">
<td>y[3]</td>
<td>0.8</td>
<td>P</td>
</tr>
</tbody>
</table>
<p>3、将截断点依次取为score值，计算TPR和FPR。 当截断点为0.1时： 说明只要score&gt;=0.1，它的预测类别就是正例。 因为4个样本的score都大于等于0.1，所以，所有样本的预测类别都为P。 scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [1, 1, 1, 1]； 正例与反例信息如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr class="even">
<td><strong>反例</strong></td>
<td>FP=2</td>
<td>TN=0</td>
</tr>
</tbody>
</table>
<p>由此可得： TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 1；</p>
<p>当截断点为0.35时： scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 1, 1]; 正例与反例信息如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr class="even">
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
<p>由此可得： TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.4时： scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 0, 1]； 正例与反例信息如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr class="even">
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
<p>由此可得： TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.8时： scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 0, 0, 1]；</p>
<p>正例与反例信息如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr class="even">
<td><strong>反例</strong></td>
<td>FP=0</td>
<td>TN=2</td>
</tr>
</tbody>
</table>
<p>由此可得： TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0；</p>
<p>4、根据TPR、FPR值，以FPR为横轴，TPR为纵轴画图。</p>
<h3 id="如何计算auc">2.16.14 如何计算AUC</h3>
<ul>
<li>将坐标点按照横坐标FPR排序 。</li>
<li>计算第<span class="math inline">\(i\)</span>个坐标点和第<span class="math inline">\(i+1\)</span>个坐标点的间距<span class="math inline">\(dx\)</span> 。</li>
<li>获取第<span class="math inline">\(i\)</span>或者<span class="math inline">\(i+1\)</span>个坐标点的纵坐标y。</li>
<li>计算面积微元<span class="math inline">\(ds=ydx\)</span>。</li>
<li>对面积微元进行累加，得到AUC。</li>
</ul>
<h3 id="为什么使用roc和auc评价分类器">2.16.15 为什么使用Roc和Auc评价分类器</h3>
<p>​ 模型有很多评估方法，为什么还要使用ROC和AUC呢？ ​ 因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<h3 id="直观理解auc">2.16.16 直观理解AUC</h3>
<p>​ 下图展现了三种AUC的值：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231638.png" alt="image-20210915231638298" /><figcaption>image-20210915231638298</figcaption>
</figure>
<p>AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。其他评价指标有精确度、准确率、召回率，而AUC比这三者更为常用。 一般在分类模型中，预测结果都是以概率的形式表现，如果要计算准确率，通常都会手动设置一个阈值来将对应的概率转化成类别，这个阈值也就很大程度上影响了模型准确率的计算。 举例： 现在假设有一个训练好的二分类器对10个正负样本（正例5个，负例5个）预测，得分按高到低排序得到的最好预测结果为[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]，即5个正例均排在5个负例前面，正例排在负例前面的概率为100%。然后绘制其ROC曲线，由于是10个样本，除去原点我们需要描10个点，如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231657.png" alt="image-20210915231657174" /><figcaption>image-20210915231657174</figcaption>
</figure>
<p>​ 描点方式按照样本预测结果的得分高低从左至右开始遍历。从原点开始，每遇到1便向y轴正方向移动y轴最小步长1个单位，这里是1/5=0.2；每遇到0则向x轴正方向移动x轴最小步长1个单位，这里也是0.2。不难看出，上图的AUC等于1，印证了正例排在负例前面的概率的确为100%。</p>
<p>​ 假设预测结果序列为[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231712.png" alt="image-20210915231712133" /><figcaption>image-20210915231712133</figcaption>
</figure>
<p>计算上图的AUC为0.96与计算正例与排在负例前面的概率0.8 × 1 + 0.2 × 0.8 = 0.96相等，而左上角阴影部分的面积则是负例排在正例前面的概率0.2 × 0.2 = 0.04。</p>
<p>​ 假设预测结果序列为[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231722.png" alt="image-20210915231722194" /><figcaption>image-20210915231722194</figcaption>
</figure>
<p>计算上图的AUC为0.88与计算正例与排在负例前面的概率0.6 × 1 + 0.2 × 0.8 + 0.2 × 0.6 = 0.88相等，左上角阴影部分的面积是负例排在正例前面的概率0.2 × 0.2 × 3 = 0.12。</p>
<h3 id="代价敏感错误率与代价曲线">2.16.17 代价敏感错误率与代价曲线</h3>
<p>不同的错误会产生不同代价。以二分法为例，设置代价矩阵如下：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231736.png" alt="image-20210915231736812" /><figcaption>image-20210915231736812</figcaption>
</figure>
<p>当判断正确的时候，值为0，不正确的时候，分别为<span class="math inline">\(Cost_{01}\)</span>和<span class="math inline">\(Cost_{10}\)</span> 。</p>
<p><span class="math inline">\(Cost_{10}\)</span>:表示实际为反例但预测成正例的代价。</p>
<p><span class="math inline">\(Cost_{01}\)</span>:表示实际为正例但是预测为反例的代价。</p>
<p><strong>代价敏感错误率</strong>=样本中由模型得到的错误值与代价乘积之和 / 总样本。 其数学表达式为： <span class="math inline">\(E(f;D;cost)=\frac{1}{m}\left( \sum_{x_{i} \in D^{+}}({f(x_i)\neq y_i})\times Cost_{01}+ \sum_{x_{i} \in D^{-}}({f(x_i)\neq y_i})\times Cost_{10}\right)\)</span> <span class="math inline">\(D^{+}、D^{-}\)</span>分别代表样例集的正例子集和反例子集，x是预测值，y是真实值。</p>
<p><strong>代价曲线</strong>： 在均等代价时，ROC曲线不能直接反应出模型的期望总体代价，而代价曲线可以。 代价曲线横轴为[0,1]的正例函数代价： <span class="math inline">\(P(+)Cost=\frac{p*Cost_{01}}{p*Cost_{01}+(1-p)*Cost_{10}}\)</span> 其中p是样本为正例的概率。</p>
<p>代价曲线纵轴维[0,1]的归一化代价： <span class="math inline">\(Cost_{norm}=\frac{FNR*p*Cost_{01}+FNR*(1-p)*Cost_{10}}{p*Cost_{01}+(1-p)*Cost_{10}}\)</span></p>
<p>其中FPR为假阳率，FNR=1-TPR为假阴率。</p>
<p>注：ROC每个点，对应代价平面上一条线。</p>
<p>例如，ROC上(TPR,FPR),计算出FNR=1-TPR，在代价平面上绘制一条从(0,FPR)到(1,FNR)的线段，面积则为该条件下期望的总体代价。所有线段下界面积，所有条件下学习器的期望总体代价。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231806.png" alt="image-20210915231806156" /><figcaption>image-20210915231806156</figcaption>
</figure>
<h3 id="模型有哪些比较检验方法">2.16.18 模型有哪些比较检验方法</h3>
<p>正确性分析：模型稳定性分析，稳健性分析，收敛性分析，变化趋势分析，极值分析等。 有效性分析：误差分析，参数敏感性分析，模型对比检验等。 有用性分析：关键数据求解，极值点，拐点，变化趋势分析，用数据验证动态模拟等。 高效性分析：时空复杂度分析与现有进行比较等。</p>
<h3 id="为什么使用标准差">2.16.19 为什么使用标准差</h3>
<p>方差公式为：<span class="math inline">\(S^2_{N}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}\)</span></p>
<p>标准差公式为：<span class="math inline">\(S_{N}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}\)</span></p>
<p>样本标准差公式为：<span class="math inline">\(S_{N}=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}\)</span></p>
<p>与方差相比，使用标准差来表示数据点的离散程度有3个好处： 1、表示离散程度的数字与样本数据点的数量级一致，更适合对数据样本形成感性认知。</p>
<p>2、表示离散程度的数字单位与样本数据的单位一致，更方便做后续的分析运算。</p>
<p>3、在样本数据大致符合正态分布的情况下，标准差具有方便估算的特性：68%的数据点落在平均值前后1个标准差的范围内、95%的数据点落在平均值前后2个标准差的范围内，而99%的数据点将会落在平均值前后3个标准差的范围内。</p>
<h3 id="类别不平衡产生原因">2.16.20 类别不平衡产生原因</h3>
<p>​ 类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。</p>
<p>产生原因：</p>
<p>​ 分类学习算法通常都会假设不同类别的训练样例数目基本相同。如果不同类别的训练样例数目差别很大，则会影响学习结果，测试结果变差。例如二分类问题中有998个反例，正例有2个，那学习方法只需返回一个永远将新样本预测为反例的分类器，就能达到99.8%的精度；然而这样的分类器没有价值。</p>
<h3 id="常见的类别不平衡问题解决方法">2.16.21 常见的类别不平衡问题解决方法</h3>
<p>  防止类别不平衡对学习造成的影响，在构建分类模型之前，需要对分类不平衡性问题进行处理。主要解决方法有：</p>
<p>1、扩大数据集</p>
<p>​ 增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。</p>
<p>2、对大类数据欠采样</p>
<p>​ 减少大类数据样本个数，使与小样本个数接近。 ​ 缺点：欠采样操作时若随机丢弃大类样本，可能会丢失重要信息。 ​ 代表算法：EasyEnsemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。</p>
<p>3、对小类数据过采样</p>
<p>​ 过采样：对小类的数据样本进行采样来增加小类的数据样本个数。</p>
<p>​ 代表算法：SMOTE和ADASYN。</p>
<p>​ SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。</p>
<p>​ 新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。<br />
​ ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。</p>
<p>4、使用新评价指标</p>
<p>​ 如果当前评价指标不适用，则应寻找其他具有说服力的评价指标。比如准确度这个评价指标在类别不均衡的分类任务中并不适用，甚至进行误导。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。</p>
<p>5、选择新算法</p>
<p>​ 不同的算法适用于不同的任务与数据，应该使用不同的算法进行比较。</p>
<p>6、数据代价加权</p>
<p>​ 例如当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。</p>
<p>7、转化问题思考角度</p>
<p>​ 例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。</p>
<p>8、将问题细化分析</p>
<p>​ 对问题进行分析与挖掘，将问题划分成多个更小的问题，看这些小问题是否更容易解决。</p>
<h2 id="决策树">2.17 决策树</h2>
<h3 id="决策树的基本原理">2.17.1 决策树的基本原理</h3>
<p>​ 决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。</p>
<h3 id="决策树的三要素">2.17.2 决策树的三要素？</h3>
<p>​ 一棵决策树的生成过程主要分为下3个部分：</p>
<p>​ 1、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。</p>
<p>​ 2、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。</p>
<p>​ 3、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。</p>
<h3 id="决策树学习基本算法">2.17.3 决策树学习基本算法</h3>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231830.png" alt="image-20210915231830039" /><figcaption>image-20210915231830039</figcaption>
</figure>
<h3 id="决策树算法优缺点">2.17.4 决策树算法优缺点</h3>
<p><strong>决策树算法的优点</strong>：</p>
<p>1、决策树算法易理解，机理解释起来简单。</p>
<p>2、决策树算法可以用于小数据集。</p>
<p>3、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。</p>
<p>4、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。</p>
<p>5、能够处理多输出的问题。</p>
<p>6、对缺失值不敏感。</p>
<p>7、可以处理不相关特征数据。</p>
<p>8、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。</p>
<p><strong>决策树算法的缺点</strong>：</p>
<p>1、对连续性的字段比较难预测。</p>
<p>2、容易出现过拟合。</p>
<p>3、当类别太多时，错误可能就会增加的比较快。</p>
<p>4、在处理特征关联性比较强的数据时表现得不是太好。</p>
<p>5、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。</p>
<h3 id="熵的概念以及理解">2.17.5 熵的概念以及理解</h3>
<p>​ 熵：度量随机变量的不确定性。<br />
​ 定义：假设随机变量X的可能取值有<span class="math inline">\(x_{1},x_{2},...,x_{n}\)</span>，对于每一个可能的取值<span class="math inline">\(x_{i}\)</span>，其概率为<span class="math inline">\(P(X=x_{i})=p_{i},i=1,2...,n\)</span>。随机变量的熵为： <span class="math inline">\(H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}\)</span> ​ 对于样本集合，假设样本有k个类别，每个类别的概率为<span class="math inline">\(\frac{|C_{k}|}{|D|}\)</span>，其中 <span class="math inline">\({|C_{k}|}{|D|}\)</span>为类别为k的样本个数，<span class="math inline">\(|D|\)</span>为样本总数。样本集合D的熵为： <span class="math inline">\(H(D)=-\sum_{k=1}^{k}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}\)</span></p>
<h3 id="信息增益的理解">2.17.6 信息增益的理解</h3>
<p>​ 定义：以某特征划分数据集前后的熵的差值。 ​ 熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。 ​ 假设划分前样本集合D的熵为H(D)。使用某个特征A划分数据集D，计算划分后的数据子集的熵为H(D|A)。<br />
​ 则信息增益为： <span class="math inline">\(g(D,A)=H(D)-H(D|A)\)</span> ​ <em>注：</em>在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。<br />
​ 思想：计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。<br />
​ 另外这里提一下信息增益比相关知识： ​ <span class="math inline">\(信息增益比=惩罚参数\times信息增益\)</span><br />
​ 信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br />
​ 惩罚参数：数据集D以特征A作为随机变量的熵的倒数。</p>
<h3 id="剪枝处理的作用及策略">2.17.7 剪枝处理的作用及策略</h3>
<p>​ 剪枝处理是决策树学习算法用来解决过拟合问题的一种办法。</p>
<p>​ 在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。</p>
<p>​ 剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。</p>
<p>​ 预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。</p>
<p>​ 后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。</p>
<h2 id="支持向量机">2.18 支持向量机</h2>
<h3 id="什么是支持向量机">2.18.1 什么是支持向量机</h3>
<p>​ 支持向量：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。</p>
<p>​ 支持向量机（Support Vector Machine，SVM）：其含义是通过支持向量运算的分类器。</p>
<p>​ 在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，即黑线的具体参数。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231853.png" alt="image-20210915231853475" /><figcaption>image-20210915231853475</figcaption>
</figure>
<p>支持向量机是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是边界最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：</p>
<p>​ 当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；</p>
<p>​ 当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；</p>
<p>​ 当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；</p>
<h3 id="支持向量机能解决哪些问题">2.18.2 支持向量机能解决哪些问题</h3>
<p><strong>线性分类</strong></p>
<p>​ 在训练数据中，每个数据都有n个的属性和一个二分类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面，这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。</p>
<p>​ 这样的超平面有很多，假如我们要找到一个最佳的超平面。此时，增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的，成为最大边距超平面。这个分类器即为最大边距分类器。</p>
<p><strong>非线性分类</strong></p>
<p>​ SVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件，以及核函数可以生成非线性分类器。</p>
<h3 id="核函数特点及其作用">2.18.3 核函数特点及其作用</h3>
<p>​ 引入核函数目的：把原坐标系里线性不可分的数据用核函数Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。<br />
​ 核函数方法的广泛应用，与其特点是分不开的：</p>
<p>1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。</p>
<p>2）无需知道非线性变换函数Φ的形式和参数。</p>
<p>3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。</p>
<p>4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。</p>
<h3 id="svm为什么引入对偶问题">2.18.4 SVM为什么引入对偶问题</h3>
<p>1，对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。</p>
<p>2，可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）。</p>
<p>3，在优化理论中，目标函数 f(x) 会有多种形式：如果目标函数和约束条件都为变量 x 的线性函数，称该问题为线性规划；如果目标函数为二次函数，约束条件为线性函数，称该最优化问题为二次规划；如果目标函数或者约束条件均为非线性函数，称该最优化问题为非线性规划。每个线性规划问题都有一个与之对应的对偶问题，对偶问题有非常良好的性质，以下列举几个：</p>
<p>​ a, 对偶问题的对偶是原问题；</p>
<p>​ b, 无论原始问题是否是凸的，对偶问题都是凸优化问题；</p>
<p>​ c, 对偶问题可以给出原始问题一个下界；</p>
<p>​ d, 当满足一定条件时，原始问题与对偶问题的解是完全等价的。</p>
<h3 id="如何理解svm中的对偶问题">2.18.5 如何理解SVM中的对偶问题</h3>
<p>在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。</p>
<p>​ 假设优化目标为 <span class="math inline">\(\begin{align} &amp;\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\ &amp;s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\ \end{align} \tag{1}\)</span> <strong>step 1</strong>. 转化问题： <span class="math inline">\(\min_{\boldsymbol w, b} \max_{\alpha_i \geqslant 0} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \tag{2}\)</span> 上式等价于原问题，因为若满足(1)中不等式约束，则(2)式求max时,<span class="math inline">\(\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\)</span>必须取0，与(1)等价；若不满足(1)中不等式约束，(2)中求max会得到无穷大。 交换min和max获得其对偶问题: <span class="math inline">\(\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}\)</span> 交换之后的对偶问题和原问题并不相等，上式的解小于等于原问题的解。</p>
<p><strong>step 2</strong>.现在的问题是如何找到问题(1) 的最优值的一个最好的下界? <span class="math inline">\(\frac{1}{2}||\boldsymbol w||^2 &lt; v\\ 1 - y_i(\boldsymbol w^T\boldsymbol x_i+b) \leqslant 0\tag{3}\)</span> 若方程组(3)无解， 则v是问题(1)的一个下界。若(3)有解， 则 <span class="math inline">\(\forall \boldsymbol \alpha &gt; 0 , \ \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} &lt; v\)</span> 由逆否命题得：若 <span class="math inline">\(\exists \boldsymbol \alpha &gt; 0 , \ \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \geqslant v\)</span> 则(3)无解。</p>
<p>那么v是问题</p>
<p>(1)的一个下界。 要求得一个好的下界，取最大值即可 <span class="math inline">\(\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}\)</span> <strong>step 3</strong>. 令 <span class="math inline">\(L(\boldsymbol w, b,\boldsymbol a) = \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\)</span> <span class="math inline">\(p^*\)</span>为原问题的最小值，对应的<span class="math inline">\(w,b\)</span>分别为<span class="math inline">\(w^*,b^*\)</span>,则对于任意的<span class="math inline">\(a&gt;0\)</span>: <span class="math inline">\(p^* = \frac{1}{2}||\boldsymbol w^*||^2 \geqslant L(\boldsymbol w^*, b,\boldsymbol a) \geqslant \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)\)</span> 则 <span class="math inline">\(\min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)\)</span>是问题（1）的一个下界。</p>
<p>此时，取最大值即可求得好的下界，即 <span class="math inline">\(\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)\)</span></p>
<h3 id="常见的核函数有哪些">2.18.7 常见的核函数有哪些</h3>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 49%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="header">
<th>核函数</th>
<th>表达式</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Kernel线性核</td>
<td><span class="math inline">\(k(x,y)=x^{t}y+c\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Polynomial Kernel多项式核</td>
<td><span class="math inline">\(k(x,y)=(ax^{t}y+c)^{d}\)</span></td>
<td><span class="math inline">\(d\geqslant1\)</span>为多项式的次数</td>
</tr>
<tr class="odd">
<td>Exponential Kernel指数核</td>
<td><span class="math inline">\(k(x,y)=exp(-\frac{\left \|x-y \right \|}{2\sigma ^{2}})\)</span></td>
<td><span class="math inline">\(\sigma&gt;0\)</span></td>
</tr>
<tr class="even">
<td>Gaussian Kernel高斯核</td>
<td><span class="math inline">\(k(x,y)=exp(-\frac{\left \|x-y \right \|^{2}}{2\sigma ^{2}})\)</span></td>
<td><span class="math inline">\(\sigma\)</span>为高斯核的带宽，<span class="math inline">\(\sigma&gt;0\)</span>,</td>
</tr>
<tr class="odd">
<td>Laplacian Kernel拉普拉斯核</td>
<td><span class="math inline">\(k(x,y)=exp(-\frac{\left \|x-y \right \|}{\sigma})\)</span></td>
<td><span class="math inline">\(\sigma&gt;0\)</span></td>
</tr>
<tr class="even">
<td>ANOVA Kernel</td>
<td><span class="math inline">\(k(x,y)=exp(-\sigma(x^{k}-y^{k})^{2})^{d}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Sigmoid Kernel</td>
<td><span class="math inline">\(k(x,y)=tanh(ax^{t}y+c)\)</span></td>
<td><span class="math inline">\(tanh\)</span>为双曲正切函数，<span class="math inline">\(a&gt;0,c&lt;0\)</span></td>
</tr>
</tbody>
</table>
<h3 id="svm主要特点">2.18.9 SVM主要特点</h3>
<p>特点：</p>
<ol type="1">
<li>SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。<br />
</li>
<li>SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。<br />
</li>
<li>支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。<br />
</li>
<li>SVM是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。</li>
<li>SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。<br />
</li>
<li>少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在： ​ ①增、删非支持向量样本对模型没有影响;<br />
​ ②支持向量样本集具有一定的鲁棒性;<br />
​ ③有些成功的应用中，SVM方法对核的选取不敏感<br />
</li>
<li>SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。<br />
</li>
<li>SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。</li>
<li>SVM在小样本训练集上能够得到比其它算法好很多的结果。SVM优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。<br />
</li>
<li>它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。</li>
</ol>
<h3 id="svm主要缺点">2.18.10 SVM主要缺点</h3>
<ol type="1">
<li><p>SVM算法对大规模训练样本难以实施<br />
​ SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。<br />
​ 如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。</p></li>
<li><p>用SVM解决多分类问题存在困难</p></li>
</ol>
<p>​ 经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。</p>
<ol start="3" type="1">
<li>对缺失数据敏感，对参数和核函数的选择敏感</li>
</ol>
<p>​ 支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。</p>
<h3 id="逻辑回归与svm的异同">2.18.11 逻辑回归与SVM的异同</h3>
<p>相同点：</p>
<ul>
<li>LR和SVM都是<strong>分类</strong>算法。</li>
<li>LR和SVM都是<strong>监督学习</strong>算法。</li>
<li>LR和SVM都是<strong>判别模型</strong>。</li>
<li>如果不考虑核函数，LR和SVM都是<strong>线性分类</strong>算法，也就是说他们的分类决策面都是线性的。 说明：LR也是可以用核函数的.但LR通常不采用核函数的方法。（<strong>计算量太大</strong>）</li>
</ul>
<p>不同点：</p>
<p><strong>1、LR采用log损失，SVM采用合页(hinge)损失。</strong> 逻辑回归的损失函数： <span class="math inline">\(J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y^{i}logh_{\theta}(x^{i})+ (1-y^{i})log(1-h_{\theta}(x^{i}))\right]\)</span> 支持向量机的目标函数: <span class="math inline">\(L(w,n,a)=\frac{1}{2}||w||^2-\sum^n_{i=1}\alpha_i \left( y_i(w^Tx_i+b)-1\right)\)</span> ​ 逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过<strong>极大似然估计</strong>的方法估计出参数的值。<br />
​ 支持向量机基于几何<strong>边界最大化</strong>原理，认为存在最大几何边界的分类面为最优分类面。</p>
<p>2、<strong>LR对异常值敏感，SVM对异常值不敏感</strong>。</p>
<p>​ 支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。<br />
​ 支持向量机改变非支持向量样本并不会引起决策面的变化。<br />
​ 逻辑回归中改变任何样本都会引起决策面的变化。</p>
<p>3、<strong>计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高</strong></p>
<p>​ 当样本较少，特征维数较低时，SVM和LR的运行时间均比较短，SVM较短一些。准确率的话，LR明显比SVM要高。当样本稍微增加些时，SVM运行时间开始增长，但是准确率赶超了LR。SVM时间虽长，但在可接受范围内。当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。(这其中主要原因是大量非支持向量参与计算，造成SVM的二次规划问题)</p>
<p>4、<strong>对非线性问题的处理方式不同</strong></p>
<p>​ LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM则可以通过对偶求解高效处理。LR则在特征空间维度很高时，表现较差。</p>
<p>5、<strong>SVM的损失函数就自带正则</strong>。<br />
​ 损失函数中的1/2||w||^2项，这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！**</p>
<p>6、SVM自带<strong>结构风险最小化</strong>，LR则是<strong>经验风险最小化</strong>。</p>
<p>7、SVM会用核函数而LR一般不用核函数。</p>
<h2 id="贝叶斯分类器">2.19 贝叶斯分类器</h2>
<h3 id="图解极大似然估计">2.19.1 图解极大似然估计</h3>
<p>极大似然估计的原理，用一张图片来说明，如下图所示：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915231944.png" alt="image-20210915231944027" /><figcaption>image-20210915231944027</figcaption>
</figure>
<p>例：有两个外形完全相同的箱子，1号箱有99只白球，1只黑球；2号箱有1只白球，99只黑球。在一次实验中，取出的是黑球，请问是从哪个箱子中取出的？</p>
<p>​ 一般的根据经验想法，会猜测这只黑球最像是从2号箱取出，此时描述的“最像”就有“最大似然”的意思，这种想法常称为“最大似然原理”。</p>
<h3 id="极大似然估计原理">2.19.2 极大似然估计原理</h3>
<p>​ 总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>​ 极大似然估计是建立在极大似然原理的基础上的一个统计方法。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>​ 由于样本集中的样本都是独立同分布，可以只考虑一类样本集<span class="math inline">\(D\)</span>，来估计参数向量<span class="math inline">\(\vec\theta\)</span>。记已知的样本集为： <span class="math inline">\(D=\vec x_{1},\vec x_{2},...,\vec x_{n}\)</span> 似然函数（likelihood function）：联合概率密度函数<span class="math inline">\(p(D|\vec\theta )\)</span>称为相对于<span class="math inline">\(\vec x_{1},\vec x_{2},...,\vec x_{n}\)</span>的<span class="math inline">\(\vec\theta\)</span>的似然函数。 <span class="math inline">\(l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{n}|\vec\theta )=\prod_{i=1}^{n}p(\vec x_{i}|\vec \theta )\)</span> 如果<span class="math inline">\(\hat{\vec\theta}\)</span>是参数空间中能使似然函数<span class="math inline">\(l(\vec\theta)\)</span>最大的<span class="math inline">\(\vec\theta\)</span>值，则<span class="math inline">\(\hat{\vec\theta}\)</span>应该是“最可能”的参数值，那么<span class="math inline">\(\hat{\vec\theta}\)</span>就是<span class="math inline">\(\theta\)</span>的极大似然估计量。它是样本集的函数，记作： <span class="math inline">\(\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )\)</span> <span class="math inline">\(\hat{\vec\theta}(\vec x_{1},\vec x_{2},...,\vec x_{n})\)</span>称为极大似然函数估计值。</p>
<h3 id="贝叶斯分类器基本原理">2.19.3 贝叶斯分类器基本原理</h3>
<p>​ 贝叶斯决策论通过<strong>相关概率已知</strong>的情况下利用<strong>误判损失</strong>来选择最优的类别分类。<br />
假设有<span class="math inline">\(N\)</span>种可能的分类标记，记为<span class="math inline">\(Y=\{c_1,c_2,...,c_N\}\)</span>，那对于样本<span class="math inline">\(\boldsymbol{x}\)</span>，它属于哪一类呢？</p>
<p>计算步骤如下：</p>
<p>step 1. 算出样本<span class="math inline">\(\boldsymbol{x}\)</span>属于第i个类的概率，即<span class="math inline">\(P(c_i|x)\)</span>；</p>
<p>step 2. 通过比较所有的<span class="math inline">\(P(c_i|\boldsymbol{x})\)</span>，得到样本<span class="math inline">\(\boldsymbol{x}\)</span>所属的最佳类别。</p>
<p>step 3. 将类别<span class="math inline">\(c_i\)</span>和样本<span class="math inline">\(\boldsymbol{x}\)</span>代入到贝叶斯公式中，得到： <span class="math inline">\(P(c_i|\boldsymbol{x})=\frac{P(\boldsymbol{x}|c_i)P(c_i)}{P(\boldsymbol{x})}.\)</span> ​ 一般来说，<span class="math inline">\(P(c_i)\)</span>为先验概率，<span class="math inline">\(P(\boldsymbol{x}|c_i)\)</span>为条件概率，<span class="math inline">\(P(\boldsymbol{x})\)</span>是用于归一化的证据因子。对于<span class="math inline">\(P(c_i)\)</span>可以通过训练样本中类别为<span class="math inline">\(c_i\)</span>的样本所占的比例进行估计；此外，由于只需要找出最大的<span class="math inline">\(P(\boldsymbol{x}|c_i)\)</span>，因此我们并不需要计算<span class="math inline">\(P(\boldsymbol{x})\)</span>。<br />
​ 为了求解条件概率，基于不同假设提出了不同的方法，以下将介绍朴素贝叶斯分类器和半朴素贝叶斯分类器。</p>
<h3 id="朴素贝叶斯分类器">2.19.4 朴素贝叶斯分类器</h3>
<p>​ 假设样本<span class="math inline">\(\boldsymbol{x}\)</span>包含<span class="math inline">\(d\)</span>个属性，即<span class="math inline">\(\boldsymbol{x}=\{ x_1,x_2,...,x_d\}\)</span>。于是有： <span class="math inline">\(P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)\)</span> 这个联合概率难以从有限的训练样本中直接估计得到。于是，朴素贝叶斯（Naive Bayesian，简称NB）采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立。于是有： $P(x_1,x_2,,x_d|c_i)=_{j=1}^d P(x_j|c_i)$2.19.5 举例理解朴素贝叶斯分类器</p>
<p>使用经典的西瓜训练集如下：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">编号</th>
<th style="text-align: center;">色泽</th>
<th style="text-align: center;">根蒂</th>
<th style="text-align: center;">敲声</th>
<th style="text-align: center;">纹理</th>
<th style="text-align: center;">脐部</th>
<th style="text-align: center;">触感</th>
<th style="text-align: center;">密度</th>
<th style="text-align: center;">含糖率</th>
<th style="text-align: center;">好瓜</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">乌黑</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">沉闷</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">乌黑</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">沉闷</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">浅白</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">稍凹</td>
<td style="text-align: center;">软粘</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">乌黑</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">稍糊</td>
<td style="text-align: center;">稍凹</td>
<td style="text-align: center;">软粘</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">乌黑</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">稍凹</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">乌黑</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">沉闷</td>
<td style="text-align: center;">稍糊</td>
<td style="text-align: center;">稍凹</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">硬挺</td>
<td style="text-align: center;">清脆</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">平坦</td>
<td style="text-align: center;">软粘</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">11</td>
<td style="text-align: center;">浅白</td>
<td style="text-align: center;">硬挺</td>
<td style="text-align: center;">清脆</td>
<td style="text-align: center;">模糊</td>
<td style="text-align: center;">平坦</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">12</td>
<td style="text-align: center;">浅白</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">模糊</td>
<td style="text-align: center;">平坦</td>
<td style="text-align: center;">软粘</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">13</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">稍糊</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">14</td>
<td style="text-align: center;">浅白</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">沉闷</td>
<td style="text-align: center;">稍糊</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">15</td>
<td style="text-align: center;">乌黑</td>
<td style="text-align: center;">稍蜷</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">稍凹</td>
<td style="text-align: center;">软粘</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">16</td>
<td style="text-align: center;">浅白</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">模糊</td>
<td style="text-align: center;">平坦</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">17</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">沉闷</td>
<td style="text-align: center;">稍糊</td>
<td style="text-align: center;">稍凹</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">否</td>
</tr>
</tbody>
</table>
<p>对下面的测试例“测1”进行 分类：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">编号</th>
<th style="text-align: center;">色泽</th>
<th style="text-align: center;">根蒂</th>
<th style="text-align: center;">敲声</th>
<th style="text-align: center;">纹理</th>
<th style="text-align: center;">脐部</th>
<th style="text-align: center;">触感</th>
<th style="text-align: center;">密度</th>
<th style="text-align: center;">含糖率</th>
<th style="text-align: center;">好瓜</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">测1</td>
<td style="text-align: center;">青绿</td>
<td style="text-align: center;">蜷缩</td>
<td style="text-align: center;">浊响</td>
<td style="text-align: center;">清晰</td>
<td style="text-align: center;">凹陷</td>
<td style="text-align: center;">硬滑</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">？</td>
</tr>
</tbody>
</table>
<p>首先，估计类先验概率<span class="math inline">\(P(c_j)\)</span>，有 <span class="math inline">\(\begin{align} &amp;P(好瓜=是)=\frac{8}{17}=0.471 \newline &amp;P(好瓜=否)=\frac{9}{17}=0.529 \end{align}\)</span> 然后，为每个属性估计条件概率（这里，对于连续属性，假定它们服从正态分布） <span class="math inline">\(P_{青绿|是}=P（色泽=青绿|好瓜=是）=\frac{3}{8}=0.375\)</span></p>
<p><span class="math inline">\(P_{青绿|否}=P（色泽=青绿|好瓜=否）=\frac{3}{9}\approx0.333\)</span></p>
<p><span class="math inline">\(P_{蜷缩|是}=P（根蒂=蜷缩|好瓜=是）=\frac{5}{8}=0.625\)</span></p>
<p><span class="math inline">\(P_{蜷缩|否}=P（根蒂=蜷缩|好瓜=否）=\frac{3}{9}=0.333\)</span></p>
<p><span class="math inline">\(P_{浊响|是}=P（敲声=浊响|好瓜=是）=\frac{6}{8}=0.750\)</span></p>
<p><span class="math inline">\(P_{浊响|否}=P（敲声=浊响|好瓜=否）=\frac{4}{9}\approx 0.444\)</span></p>
<p><span class="math inline">\(P_{清晰|是}=P（纹理=清晰|好瓜=是）=\frac{7}{8}= 0.875\)</span></p>
<p><span class="math inline">\(P_{清晰|否}=P（纹理=清晰|好瓜=否）=\frac{2}{9}\approx 0.222\)</span></p>
<p><span class="math inline">\(P_{凹陷|是}=P（脐部=凹陷|好瓜=是）=\frac{6}{8}= 0.750\)</span></p>
<p><span class="math inline">\(P_{凹陷|否}=P（脐部=凹陷|好瓜=否）=\frac{2}{9} \approx 0.222\)</span></p>
<p><span class="math inline">\(P_{硬滑|是}=P（触感=硬滑|好瓜=是）=\frac{6}{8}= 0.750\)</span></p>
<p><span class="math inline">\(P_{硬滑|否}=P（触感=硬滑|好瓜=否）=\frac{6}{9} \approx 0.667\)</span></p>
<p><span class="math inline">\(\begin{aligned} \rho_{密度：0.697|是}&amp;=\rho（密度=0.697|好瓜=是）\\&amp;=\frac{1}{\sqrt{2 \pi}\times0.129}exp\left( -\frac{(0.697-0.574)^2}{2\times0.129^2}\right) \approx 1.959 \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned} \rho_{密度：0.697|否}&amp;=\rho（密度=0.697|好瓜=否）\\&amp;=\frac{1}{\sqrt{2 \pi}\times0.195}exp\left( -\frac{(0.697-0.496)^2}{2\times0.195^2}\right) \approx 1.203 \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned} \rho_{含糖：0.460|是}&amp;=\rho（密度=0.460|好瓜=是）\\&amp;=\frac{1}{\sqrt{2 \pi}\times0.101}exp\left( -\frac{(0.460-0.279)^2}{2\times0.101^2}\right) \approx 0.788 \end{aligned}\)</span></p>
<p><span class="math inline">\(\begin{aligned} \rho_{含糖：0.460|否}&amp;=\rho（密度=0.460|好瓜=是）\\&amp;=\frac{1}{\sqrt{2 \pi}\times0.108}exp\left( -\frac{(0.460-0.154)^2}{2\times0.108^2}\right) \approx 0.066 \end{aligned}\)</span></p>
<p>于是有 <span class="math inline">\(\begin{align} P(&amp;好瓜=是)\times P_{青绿|是} \times P_{蜷缩|是} \times P_{浊响|是} \times P_{清晰|是} \times P_{凹陷|是}\newline &amp;\times P_{硬滑|是} \times p_{密度：0.697|是} \times p_{含糖：0.460|是} \approx 0.063 \newline\newline P(&amp;好瓜=否)\times P_{青绿|否} \times P_{蜷缩|否} \times P_{浊响|否} \times P_{清晰|否} \times P_{凹陷|否}\newline &amp;\times P_{硬滑|否} \times p_{密度：0.697|否} \times p_{含糖：0.460|否} \approx 6.80\times 10^{-5} \end{align}\)</span></p>
<p>由于<span class="math inline">\(0.063&gt;6.80\times 10^{-5}\)</span>，因此，朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。</p>
<h3 id="半朴素贝叶斯分类器">2.19.6 半朴素贝叶斯分类器</h3>
<p>​ 朴素贝叶斯采用了“属性条件独立性假设”，半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息。<strong>独依赖估计</strong>（One-Dependence Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，独依赖是假设每个属性在类别之外最多依赖一个其他属性，即： <span class="math inline">\(P(\boldsymbol{x}|c_i)=\prod_{j=1}^d P(x_j|c_i,{\rm pa}_j)\)</span> 其中<span class="math inline">\(pa_j\)</span>为属性<span class="math inline">\(x_i\)</span>所依赖的属性，成为<span class="math inline">\(x_i\)</span>的父属性。假设父属性<span class="math inline">\(pa_j\)</span>已知，那么可以使用下面的公式估计<span class="math inline">\(P(x_j|c_i,{\rm pa}_j)\)</span> <span class="math inline">\(P(x_j|c_i,{\rm pa}_j)=\frac{P(x_j,c_i,{\rm pa}_j)}{P(c_i,{\rm pa}_j)}\)</span></p>
<h2 id="em算法">2.20 EM算法</h2>
<h3 id="em算法基本思想">2.20.1 EM算法基本思想</h3>
<p>​ 最大期望算法（Expectation-Maximization algorithm, EM），是一类通过迭代进行极大似然估计的优化算法，通常作为牛顿迭代法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计。</p>
<p>​ 最大期望算法基本思想是经过两个步骤交替进行计算：</p>
<p>​ 第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值<strong>；</strong></p>
<p>​ 第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。</p>
<p>​ M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p>
<h3 id="em算法推导">2.20.2 EM算法推导</h3>
<p>​ 对于<span class="math inline">\(m\)</span>个样本观察数据<span class="math inline">\(x=(x^{1},x^{2},...,x^{m})\)</span>，现在想找出样本的模型参数<span class="math inline">\(\theta\)</span>，其极大化模型分布的对数似然函数为： <span class="math inline">\(\theta = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta)\)</span> 如果得到的观察数据有未观察到的隐含数据<span class="math inline">\(z=(z^{(1)},z^{(2)},...z^{(m)})\)</span>，极大化模型分布的对数似然函数则为： <span class="math inline">\(\theta =\mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta) = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta) \tag{a}\)</span> 由于上式不能直接求出<span class="math inline">\(\theta\)</span>，采用缩放技巧： <span class="math inline">\(\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta) &amp; = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \\ &amp; \geqslant \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align} \tag{1}\)</span> 上式用到了Jensen不等式： <span class="math inline">\(log\sum\limits_j\lambda_jy_j \geqslant \sum\limits_j\lambda_jlogy_j\;\;, \lambda_j \geqslant 0, \sum\limits_j\lambda_j =1\)</span> 并且引入了一个未知的新分布<span class="math inline">\(Q_i(z^{(i)})\)</span>。</p>
<p>此时，如果需要满足Jensen不等式中的等号，所以有： <span class="math inline">\(\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} =c, c为常数\)</span> 由于<span class="math inline">\(Q_i(z^{(i)})\)</span>是一个分布，所以满足 <span class="math inline">\(\sum\limits_{z}Q_i(z^{(i)}) =1\)</span> 综上，可得： <span class="math inline">\(Q_i(z^{(i)}) = \frac{P(x^{(i)}， z^{(i)};\theta)}{\sum\limits_{z}P(x^{(i)}, z^{(i)};\theta)} = \frac{P(x^{(i)}, z^{(i)};\theta)}{P(x^{(i)};\theta)} = P( z^{(i)}|x^{(i)};\theta)\)</span> 如果<span class="math inline">\(Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)};\theta)\)</span> ，则第(1)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式： <span class="math inline">\(\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)};\theta)}{Q_i(z^{(i)})}\)</span> 简化得： <span class="math inline">\(\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}\)</span> 以上即为EM算法的M步，<span class="math inline">\(\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}\)</span>可理解为$logP(x^{(i)}, z^{(i)};) <span class="math inline">\(基于条件概率分布\)</span>Q_i(z^{(i)}) $的期望。以上即为EM算法中E步和M步的具体数学含义。</p>
<h3 id="图解em算法">2.20.3 图解EM算法</h3>
<p>​ 考虑上一节中的（a）式，表达式中存在隐变量，直接找到参数估计比较困难，通过EM算法迭代求解下界的最大值到收敛为止。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232024.png" alt="image-20210915232024277" /><figcaption>image-20210915232024277</figcaption>
</figure>
<p>​ 图片中的紫色部分是我们的目标模型<span class="math inline">\(p(x|\theta)\)</span>，该模型复杂，难以求解析解，为了消除隐变量<span class="math inline">\(z^{(i)}\)</span>的影响，我们可以选择一个不包含<span class="math inline">\(z^{(i)}\)</span>的模型<span class="math inline">\(r(x|\theta)\)</span>，使其满足条件$r(x|) p(x|) $。</p>
<p>求解步骤如下：</p>
<p>（1）选取<span class="math inline">\(\theta_1\)</span>，使得<span class="math inline">\(r(x|\theta_1) = p(x|\theta_1)\)</span>，然后对此时的<span class="math inline">\(r\)</span>求取最大值，得到极值点<span class="math inline">\(\theta_2\)</span>，实现参数的更新。</p>
<p>（2）重复以上过程到收敛为止，在更新过程中始终满足$r p $.</p>
<h3 id="em算法流程">2.20.4 EM算法流程</h3>
<p>输入：观察数据<span class="math inline">\(x=(x^{(1)},x^{(2)},...x^{(m)})\)</span>，联合分布<span class="math inline">\(p(x,z ;\theta)\)</span>，条件分布<span class="math inline">\(p(z|x; \theta)\)</span>，最大迭代次数<span class="math inline">\(J\)</span></p>
<p>1）随机初始化模型参数<span class="math inline">\(\theta\)</span>的初值<span class="math inline">\(\theta^0\)</span>。</p>
<p>2）<span class="math inline">\(for \ j \ from \ 1 \ to \ j\)</span>：</p>
<p>​ a） E步。计算联合分布的条件概率期望： <span class="math inline">\(Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}, \theta^{j})\)</span></p>
<p><span class="math inline">\(L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}, \theta^{j})log{P(x^{(i)}, z^{(i)};\theta)}\)</span></p>
<p>​ b） M步。极大化<span class="math inline">\(L(\theta, \theta^{j})\)</span>，得到<span class="math inline">\(\theta^{j+1}\)</span>: <span class="math inline">\(\theta^{j+1} = \mathop{\arg\max}_\theta L(\theta, \theta^{j})\)</span> ​ c） 如果<span class="math inline">\(\theta^{j+1}\)</span>收敛，则算法结束。否则继续回到步骤a）进行E步迭代。</p>
<p>输出：模型参数<span class="math inline">\(\theta\)</span>。</p>
<h2 id="降维和聚类">2.21 降维和聚类</h2>
<h3 id="图解为什么会产生维数灾难">2.21.1 图解为什么会产生维数灾难</h3>
<p>​ 假如数据集包含10张照片，照片中包含三角形和圆两种形状。现在来设计一个分类器进行训练，让这个分类器对其他的照片进行正确分类（假设三角形和圆的总数是无限大），简单的，我们用一个特征进行分类：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232055.png" alt="image-20210915232055188" /><figcaption>image-20210915232055188</figcaption>
</figure>
<p>​ 图2.21.1.a</p>
<p>​ 从上图可看到，如果仅仅只有一个特征进行分类，三角形和圆几乎是均匀分布在这条线段上，很难将10张照片线性分类。那么，增加一个特征后的情况会怎么样：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232114.png" alt="image-20210915232114427" /><figcaption>image-20210915232114427</figcaption>
</figure>
<p>图2.21.1.b</p>
<p>增加一个特征后，我们发现仍然无法找到一条直线将猫和狗分开。所以，考虑需要再增加一个特征：</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232142.png" alt="image-20210915232142531" /><figcaption>image-20210915232142531</figcaption>
</figure>
<p>图2.21.1.c</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232205.png" alt="image-20210915232205451" /><figcaption>image-20210915232205451</figcaption>
</figure>
<p>​ 图2.21.1.d</p>
<p>​ 此时，可以找到一个平面将三角形和圆分开。</p>
<p>​ 现在计算一下不同特征数是样本的密度：</p>
<p>​ （1）一个特征时，假设特征空间时长度为5的线段，则样本密度为<span class="math inline">\(10 \div 5 = 2\)</span>。</p>
<p>​ （2）两个特征时，特征空间大小为$ 55 = 25$，样本密度为<span class="math inline">\(10 \div 25 = 0.4\)</span>。</p>
<p>​ （3）三个特征时，特征空间大小是$ 555 = 125$，样本密度为<span class="math inline">\(10 \div 125 = 0.08\)</span>。</p>
<p>​ 以此类推，如果继续增加特征数量，样本密度会越来越稀疏，此时，更容易找到一个超平面将训练样本分开。当特征数量增长至无限大时，样本密度就变得非常稀疏。</p>
<p>​ 下面看一下将高维空间的分类结果映射到低维空间时，会出现什么情况？</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232223.png" alt="image-20210915232223279" /><figcaption>image-20210915232223279</figcaption>
</figure>
<p>​ 图2.21.1.e</p>
<p>​ 上图是将三维特征空间映射到二维特征空间后的结果。尽管在高维特征空间时训练样本线性可分，但是映射到低维空间后，结果正好相反。事实上，增加特征数量使得高维空间线性可分，相当于在低维空间内训练一个复杂的非线性分类器。不过，这个非线性分类器太过“聪明”，仅仅学到了一些特例。如果将其用来辨别那些未曾出现在训练样本中的测试样本时，通常结果不太理想，会造成过拟合问题。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232235.png" alt="image-20210915232235712" /><figcaption>image-20210915232235712</figcaption>
</figure>
<p>​ 图2.21.1.f</p>
<p>​ 上图所示的只采用2个特征的线性分类器分错了一些训练样本，准确率似乎没有图2.21.1.e的高，但是，采用2个特征的线性分类器的泛化能力比采用3个特征的线性分类器要强。因为，采用2个特征的线性分类器学习到的不只是特例，而是一个整体趋势，对于那些未曾出现过的样本也可以比较好地辨别开来。换句话说，通过减少特征数量，可以避免出现过拟合问题，从而避免“维数灾难”。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232256.png" alt="image-20210915232256580" /><figcaption>image-20210915232256580</figcaption>
</figure>
<p>​ 上图从另一个角度诠释了“维数灾难”。假设只有一个特征时，特征的值域是0到1，每一个三角形和圆的特征值都是唯一的。如果我们希望训练样本覆盖特征值值域的20%，那么就需要三角形和圆总数的20%。我们增加一个特征后，为了继续覆盖特征值值域的20%就需要三角形和圆总数的45%(<span class="math inline">\(0.452^2\approx0.2\)</span>)。继续增加一个特征后，需要三角形和圆总数的58%(<span class="math inline">\(0.583^3\approx0.2\)</span>)。随着特征数量的增加，为了覆盖特征值值域的20%，就需要更多的训练样本。如果没有足够的训练样本，就可能会出现过拟合问题。</p>
<p>​ 通过上述例子，我们可以看到特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232313.png" alt="image-20210915232313163" /><figcaption>image-20210915232313163</figcaption>
</figure>
<p>假设有一个二维特征空间，如上图所示的矩形，在矩形内部有一个内切的圆形。由于越接近圆心的样本越稀疏，因此，相比于圆形内的样本，那些位于矩形四角的样本更加难以分类。当维数变大时，特征超空间的容量不变，但单位圆的容量会趋于0，在高维空间中，大多数训练数据驻留在特征超空间的角落。散落在角落的数据要比处于中心的数据难于分类。</p>
<h3 id="怎样避免维数灾难">2.21.2 怎样避免维数灾难</h3>
<p><strong>有待完善！！！</strong></p>
<p>解决维度灾难问题：</p>
<p>主成分分析法PCA，线性判别法LDA</p>
<p>奇异值分解简化数据、拉普拉斯特征映射</p>
<p>Lassio缩减系数法、小波分析法、</p>
<h3 id="聚类和降维有什么区别与联系">2.21.3 聚类和降维有什么区别与联系</h3>
<p>​ 聚类用于找寻数据内在的分布结构，既可以作为一个单独的过程，比如异常检测等等。也可作为分类等其他学习任务的前驱过程。聚类是标准的无监督学习。</p>
<p>​ 1）在一些推荐系统中需确定新用户的类型，但定义“用户类型”却可能不太容易，此时往往可先对原有的用户数据进行聚类，根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232335.png" alt="image-20210915232335469" /><figcaption>image-20210915232335469</figcaption>
</figure>
<p>​ 2）而降维则是为了缓解维数灾难的一个重要方法，就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”。其基于的假设就是，虽然人们平时观测到的数据样本虽然是高维的，但是实际上真正与学习任务相关的是个低维度的分布。从而通过最主要的几个特征维度就可以实现对数据的描述，对于后续的分类很有帮助。比如对于Kaggle（数据分析竞赛平台之一）上的泰坦尼克号生还问题。通过给定一个乘客的许多特征如年龄、姓名、性别、票价等，来判断其是否能在海难中生还。这就需要首先进行特征筛选，从而能够找出主要的特征，让学习到的模型有更好的泛化性。</p>
<p>​ 聚类和降维都可以作为分类等问题的预处理步骤。</p>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/img/20210915232355.png" alt="image-20210915232355064" /><figcaption>image-20210915232355064</figcaption>
</figure>
<p>但是他们虽然都能实现对数据的约减。但是二者适用的对象不同，聚类针对的是数据点，而降维则是对于数据的特征。另外它们有着很多种实现方法。聚类中常用的有K-means、层次聚类、基于密度的聚类等；降维中常用的则PCA、Isomap、LLE等。</p>
<h3 id="有哪些聚类算法优劣衡量标准">2.21.4 有哪些聚类算法优劣衡量标准</h3>
<p>不同聚类算法有不同的优劣和不同的适用条件。可从以下方面进行衡量判断： 1、算法的处理能力：处理大的数据集的能力，即算法复杂度；处理数据噪声的能力；处理任意形状，包括有间隙的嵌套的数据的能力； 2、算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识；</p>
<p>​ 3、算法的数据输入属性：算法处理的结果与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理有很多属性数据的能力，也就是对数据维数是否敏感，对数据的类型有无要求。</p>
<h3 id="聚类和分类有什么区别">2.21.5 聚类和分类有什么区别</h3>
<p><strong>聚类（Clustering） </strong> 聚类，简单地说就是把相似的东西分到一组，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此聚类通常并不需要使用训练数据进行学习，在机器学习中属于无监督学习。</p>
<p><strong>分类（Classification） </strong></p>
<p>​ 分类，对于一个分类器，通常需要你告诉它“这个东西被分为某某类”。一般情况下，一个分类器会从它得到的训练集中进行学习，从而具备对未知数据进行分类的能力，在机器学习中属于监督学习。</p>
<h3 id="不同聚类算法特点性能比较">2.21.6 不同聚类算法特点性能比较</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">算法名称</th>
<th style="text-align: center;">可伸缩性</th>
<th style="text-align: center;">适合的数据类型</th>
<th style="text-align: center;">高维性</th>
<th style="text-align: center;">异常数据抗干扰性</th>
<th style="text-align: center;">聚类形状</th>
<th style="text-align: center;">算法效率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">WAVECLUSTER</td>
<td style="text-align: center;">很高</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">很高</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">很高</td>
</tr>
<tr class="even">
<td style="text-align: center;">ROCK</td>
<td style="text-align: center;">很高</td>
<td style="text-align: center;">混合型</td>
<td style="text-align: center;">很高</td>
<td style="text-align: center;">很高</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">一般</td>
</tr>
<tr class="odd">
<td style="text-align: center;">BIRCH</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">球形</td>
<td style="text-align: center;">很高</td>
</tr>
<tr class="even">
<td style="text-align: center;">CURE</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">很高</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">较高</td>
</tr>
<tr class="odd">
<td style="text-align: center;">K-PROTOTYPES</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">混合型</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">一般</td>
</tr>
<tr class="even">
<td style="text-align: center;">DENCLUE</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">较高</td>
</tr>
<tr class="odd">
<td style="text-align: center;">OPTIGRID</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">一般</td>
</tr>
<tr class="even">
<td style="text-align: center;">CLIQUE</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">较低</td>
</tr>
<tr class="odd">
<td style="text-align: center;">DBSCAN</td>
<td style="text-align: center;">一般</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">任意形状</td>
<td style="text-align: center;">一般</td>
</tr>
<tr class="even">
<td style="text-align: center;">CLARANS</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">数值型</td>
<td style="text-align: center;">较低</td>
<td style="text-align: center;">较高</td>
<td style="text-align: center;">球形</td>
<td style="text-align: center;">较低</td>
</tr>
</tbody>
</table>
<h3 id="四种常用聚类方法之比较">2.21.7 四种常用聚类方法之比较</h3>
<p>​ 聚类就是按照某个特定标准把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。 ​ 主要的聚类算法可以划分为如下几类：划分方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法。下面主要对k-means聚类算法、凝聚型层次聚类算法、神经网络聚类算法之SOM,以及模糊聚类的FCM算法通过通用测试数据集进行聚类效果的比较和分析。</p>
<h3 id="k-means聚类算法">2.21.8 k-means聚类算法</h3>
<p>k-means是划分方法中较经典的聚类算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。 k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。k-means算法的处理过程如下：首先，随机地 选择k个对象，每个对象初始地代表了一个簇的平均值或中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值。 这个过程不断重复，直到准则函数收敛。通常，采用平方误差准则，其定义如下： <span class="math inline">\(E=\sum_{i=1}^{k}\sum_{p\in C_i}\left\|p-m_i\right\|^2\)</span> 　这里E是数据中所有对象的平方误差的总和，p是空间中的点，<span class="math inline">\(m_i\)</span>是簇<span class="math inline">\(C_i\)</span>的平均值[9]。该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离，当然也可以用其他距离度量。</p>
<p><strong>算法流程</strong>： ​ 输入：包含n个对象的数据和簇的数目k； ​ 输出：n个对象到k个簇，使平方误差准则最小。 ​ 步骤： 　　(1) 任意选择k个对象作为初始的簇中心； 　　(2) 根据簇中对象的平均值，将每个对象(重新)赋予最类似的簇； 　　(3) 更新簇的平均值，即计算每个簇中对象的平均值； 　　(4) 重复步骤(2)、(3)直到簇中心不再变化；</p>
<h3 id="层次聚类算法">2.21.9 层次聚类算法</h3>
<p>​ 根据层次分解的顺序是自底向上的还是自上向下的，层次聚类算法分为凝聚的层次聚类算法和分裂的层次聚类算法。 　凝聚型层次聚类的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。</p>
<p><strong>算法流程</strong>：</p>
<p>注：以采用最小距离的凝聚层次聚类算法为例：</p>
<p>　(1) 将每个对象看作一类，计算两两之间的最小距离； 　(2) 将距离最小的两个类合并成一个新类； 　(3) 重新计算新类与所有类之间的距离； 　(4) 重复(2)、(3)，直到所有类最后合并成一类。</p>
<h3 id="som聚类算法">2.21.10 SOM聚类算法</h3>
<p>​ SOM神经网络[11]是由芬兰神经网络专家Kohonen教授提出的，该算法假设在输入对象中存在一些拓扑结构或顺序，可以实现从输入空间(n维)到输出平面(2维)的降维映射，其映射具有拓扑特征保持性质,与实际的大脑处理有很强的理论联系。</p>
<p>​ SOM网络包含输入层和输出层。输入层对应一个高维的输入向量，输出层由一系列组织在2维网格上的有序节点构成，输入节点与输出节点通过权重向量连接。 学习过程中，找到与之距离最短的输出层单元，即获胜单元，对其更新。同时，将邻近区域的权值更新，使输出节点保持输入向量的拓扑特征。</p>
<p><strong>算法流程</strong>：</p>
<p>​ (1) 网络初始化，对输出层每个节点权重赋初值； ​ (2) 从输入样本中随机选取输入向量并且归一化，找到与输入向量距离最小的权重向量； ​ (3) 定义获胜单元，在获胜单元的邻近区域调整权重使其向输入向量靠拢； ​ (4) 提供新样本、进行训练； ​ (5) 收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类结果。</p>
<h3 id="fcm聚类算法">2.21.11 FCM聚类算法</h3>
<p>​ 1965年美国加州大学柏克莱分校的扎德教授第一次提出了‘集合’的概念。经过十多年的发展，模糊集合理论渐渐被应用到各个实际应用方面。为克服非此即彼的分类缺点，出现了以模糊集合论为数学基础的聚类分析。用模糊数学的方法进行聚类分析，就是模糊聚类分析[12]。<br />
​ FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。<br />
​ 设数据集<span class="math inline">\(X={x_1,x_2,...,x_n}\)</span>,它的模糊<span class="math inline">\(c\)</span>划分可用模糊矩阵<span class="math inline">\(U=[u_{ij}]\)</span>表示，矩阵<span class="math inline">\(U\)</span>的元素<span class="math inline">\(u_{ij}\)</span>表示第<span class="math inline">\(j(j=1,2,...,n)\)</span>个数据点属于第<span class="math inline">\(i(i=1,2,...,c)\)</span>类的隶属度，<span class="math inline">\(u_{ij}\)</span>满足如下条件：<br />
<span class="math inline">\(\begin{equation} \left\{ \begin{array}{lr} \sum_{i=1}^c u_{ij}=1 \quad\forall~j \\u_{ij}\in[0,1] \quad\forall ~i,j \\\sum_{j=1}^c u_{ij}&gt;0 \quad\forall ~i \end{array} \right. \end{equation}\)</span> 目前被广泛使用的聚类准则是取类内加权误差平方和的极小值。即： <span class="math inline">\((min)J_m(U,V)=\sum^n_{j=1}\sum^c_{i=1}u^m_{ij}d^2_{ij}(x_j,v_i)\)</span> 其中<span class="math inline">\(V\)</span>为聚类中心，<span class="math inline">\(m\)</span>为加权指数，<span class="math inline">\(d_{ij}(x_j,v_i)=||v_i-x_j||\)</span>。</p>
<p><strong>算法流程</strong>：</p>
<p>　(1) 标准化数据矩阵； 　(2) 建立模糊相似矩阵，初始化隶属矩阵； 　(3) 算法开始迭代，直到目标函数收敛到极小值； 　(4) 根据迭代结果，由最后的隶属矩阵确定数据所属的类，显示最后的聚类结果。</p>
<h3 id="四种聚类算法试验">2.21.12 四种聚类算法试验</h3>
<p>​ 选取专门用于测试分类、聚类算法的国际通用的UCI数据库中的IRIS数据集，IRIS数据集包含150个样本数据，分别取自三种不同 的莺尾属植物setosa、versicolor和virginica的花朵样本,每个数据含有4个属性，即萼片长度、萼片宽度、花瓣长度、花瓣宽度，单位为cm。 在数据集上执行不同的聚类算法，可以得到不同精度的聚类结果。基于前面描述的各算法原理及流程，可初步得如下聚类结果。</p>
<table>
<thead>
<tr class="header">
<th>聚类方法</th>
<th>聚错样本数</th>
<th>运行时间/s</th>
<th>平均准确率/（%）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>K-means</td>
<td>17</td>
<td>0.146001</td>
<td>89</td>
</tr>
<tr class="even">
<td>层次聚类</td>
<td>51</td>
<td>0.128744</td>
<td>66</td>
</tr>
<tr class="odd">
<td>SOM</td>
<td>22</td>
<td>5.267283</td>
<td>86</td>
</tr>
<tr class="even">
<td>FCM</td>
<td>12</td>
<td>0.470417</td>
<td>92</td>
</tr>
</tbody>
</table>
<p><strong>注</strong>：</p>
<ol type="1">
<li>聚错样本数：总的聚错的样本数，即各类中聚错的样本数的和；<br />
</li>
<li>运行时间：即聚类整个过程所耗费的时间，单位为s；<br />
</li>
<li>平均准确度：设原数据集有k个类,用<span class="math inline">\(c_i\)</span>表示第i类，<span class="math inline">\(n_i\)</span>为<span class="math inline">\(c_i\)</span>中样本的个数，<span class="math inline">\(m_i\)</span>为聚类正确的个数,则<span class="math inline">\(m_i/n_i\)</span>为 第i类中的精度，则平均精度为：<span class="math inline">\(avg=\frac{1}{k}\sum_{i=1}^{k}\frac{m_{i}}{n_{i}}\)</span>。</li>
</ol>
<h2 id="参考文献">参考文献</h2>
<p>[0] github.com/scutan90/DeepLearning-500-questions</p>
<p>[1] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.<br />
[2] 周志华. 机器学习[M].清华大学出版社, 2016.<br />
[3] Michael A. Nielsen. &quot;Neural Networks and Deep Learning&quot;, Determination Press, 2015.<br />
[4] Suryansh S. Gradient Descent: All You Need to Know, 2018.<br />
[5] 刘建平. 梯度下降小结,EM算法的推导, 2018<br />
[6] 杨小兵．聚类分析中若干关键技术的研究[D]． 杭州：浙江大学, 2005.<br />
[7] XU Rui, Donald Wunsch 1 1． survey of clustering algorithm[J]．IEEE．Transactions on Neural Networks, 2005, 16(3)：645-67 8.<br />
[8] YI Hong, SAM K． Learning assignment order of instances for the constrained k-means clustering algorithm[J]．IEEE Transactions on Systems, Man, and Cybernetics, Part B：Cybernetics,2009,39 (2)：568-574.<br />
[9] 贺玲, 吴玲达, 蔡益朝．数据挖掘中的聚类算法综述[J]．计算机应用研究, 2007, 24(1):10-13．<br />
[10] 孙吉贵, 刘杰, 赵连宇．聚类算法研究[J]．软件学报, 2008, 19(1)：48-61．<br />
[11] 孔英会, 苑津莎, 张铁峰等．基于数据流管理技术的配变负荷分类方法研究．中国国际供电会议, CICED2006．<br />
[12] 马晓艳, 唐雁．层次聚类算法研究[J]．计算机科学, 2008, 34(7)：34-36．<br />
[13] FISHER R A． Iris Plants Database https://www.ics.uci.edu/vmlearn/MLRepository.html, Authorized license．<br />
[14] Quinlan J R. Induction of decision trees[J]. Machine learning, 1986, 1(1): 81-106.<br />
[15] Breiman L. Random forests[J]. Machine learning, 2001, 45(1): 5-32.</p>
]]></content>
      <categories>
        <category>笔记</category>
        <category>DL基础</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>图解自注意力机制</title>
    <url>/2022/05/27/%E5%9B%BE%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<ul>
<li>单头自注意力机制过程</li>
</ul>
<p><img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/pictures/2022/05/27_14_14_39_20220527selfattention1.png" /></p>
<ul>
<li>数据划分，分别做单头self-attention，再将结果拼接，便是多头自注意力</li>
</ul>
<figure>
<img src="https://gitlab.com/XiubenWu/xiubenwu-images/-/raw/master/pictures/2022/05/27_14_15_46_20220527selfattention2.png" alt="mutilhead" /><figcaption>mutilhead</figcaption>
</figure>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>DL</tag>
      </tags>
  </entry>
</search>
